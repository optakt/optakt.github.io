{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview Introduction This documentation represents the main knowledge base of the Optakt team outside the project repositories themselves. It serves as a single place to introduce new engineers to Optakt, to share knowledge between team members and to improve our processes on a continuous basis. Structure The documentation website is divided into three major sections: Code talks about everything related to writing code, from design principles, over style guidelines, to useful guides on various topics such as testing. Process is all about how the team organizes its work and how tools from Git, to CI/CD pipelines, should be used. Knowledge puts together the knowledge about various projects that we have collected over time to provide the necessary background to work on them.","title":"Overview"},{"location":"#overview","text":"","title":"Overview"},{"location":"#introduction","text":"This documentation represents the main knowledge base of the Optakt team outside the project repositories themselves. It serves as a single place to introduce new engineers to Optakt, to share knowledge between team members and to improve our processes on a continuous basis.","title":"Introduction"},{"location":"#structure","text":"The documentation website is divided into three major sections: Code talks about everything related to writing code, from design principles, over style guidelines, to useful guides on various topics such as testing. Process is all about how the team organizes its work and how tools from Git, to CI/CD pipelines, should be used. Knowledge puts together the knowledge about various projects that we have collected over time to provide the necessary background to work on them.","title":"Structure"},{"location":"code/design/","text":"Design Principles The design principles mentioned in this document are at the core of the software we create at Optakt, as they provide helpful guidelines for engineers to build software in well-thought-out ways. They provide means to build complex software effectively so that it is testable, maintainable, expandable and clear. SOLID The SOLID acronym represents five design principles: Single-responsibility principle Open-closed principle Liskov Substitution principle Interface segregation principle Dependency inversion principle Go is a perfect language for integrating all of those principles within your software design, and this is best explained by Dave Cheney in his article on the matter .","title":"Design"},{"location":"code/design/#design-principles","text":"The design principles mentioned in this document are at the core of the software we create at Optakt, as they provide helpful guidelines for engineers to build software in well-thought-out ways. They provide means to build complex software effectively so that it is testable, maintainable, expandable and clear.","title":"Design Principles"},{"location":"code/design/#solid","text":"The SOLID acronym represents five design principles: Single-responsibility principle Open-closed principle Liskov Substitution principle Interface segregation principle Dependency inversion principle Go is a perfect language for integrating all of those principles within your software design, and this is best explained by Dave Cheney in his article on the matter .","title":"SOLID"},{"location":"code/go/","text":"Go Code Style Guidelines Guidelines Copy Slices and Maps at Boundaries Slices and maps contain pointers to the underlying data so be wary of scenarios when they need to be copied. Receiving Slices and Maps Keep in mind that users can modify a map or slice you received as an argument if you store a reference to it. Bad Good func ( d * Driver ) SetTrips ( trips [] Trip ) { d . trips = trips } trips := ... d1 . SetTrips ( trips ) // Did you mean to modify d1.trips? trips [ 0 ] = ... func ( d * Driver ) SetTrips ( trips [] Trip ) { d . trips = make ([] Trip , len ( trips )) copy ( d . trips , trips ) } trips := ... d1 . SetTrips ( trips ) // We can now modify trips[0] without affecting d1.trips. trips [ 0 ] = ... Returning Slices and Maps Similarly, be wary of user modifications to maps or slices exposing internal state. Bad Good type Stats struct { mu sync . Mutex counters map [ string ] int } // Snapshot returns the current stats. func ( s * Stats ) Snapshot () map [ string ] int { s . mu . Lock () defer s . mu . Unlock () return s . counters } // snapshot is no longer protected by the mutex, so any // access to the snapshot is subject to data races. snapshot := stats . Snapshot () type Stats struct { mu sync . Mutex counters map [ string ] int } func ( s * Stats ) Snapshot () map [ string ] int { s . mu . Lock () defer s . mu . Unlock () result := make ( map [ string ] int , len ( s . counters )) for k , v := range s . counters { result [ k ] = v } return result } // Snapshot is now a copy. snapshot := stats . Snapshot () Start Enums at One The standard way of introducing enumerations in Go is to declare a custom type and a const group with iota . Since variables have a 0 default value, you should usually start your enums on a non-zero value. Bad Good type Operation int const ( Add Operation = iota Subtract Multiply ) // Add=0, Subtract=1, Multiply=2 type Operation int const ( Add Operation = iota + 1 Subtract Multiply ) // Add=1, Subtract=2, Multiply=3 There are cases where using the zero value makes sense, for example when the zero value case is the desirable default behavior. type LogOutput int const ( LogToStdout LogOutput = iota LogToFile LogToRemote ) // LogToStdout=0, LogToFile=1, LogToRemote=2 Error Types There are various options for declaring errors: errors.New for errors with simple static strings fmt.Errorf for formatted and wrapped error strings Custom types that implement the error interface When returning errors, consider the following to determine the best choice: Is this a simple error that needs no extra information? If so, errors.New should suffice. Do the clients need to detect and handle this error? If so, you should use a custom type, and implement the Error() method. Are you propagating an error returned by a downstream function? If so, check the section on error wrapping . Otherwise, fmt.Errorf is okay. If the client needs to detect a specific error case, a sentinel error should be created using errors.New . Bad Good // package foo func Open () error { return errors . New ( \"could not open\" ) } // package bar func use () { err := foo . Open () if err != nil { if err . Error () == \"could not open\" { // handle } else { panic ( \"unknown error\" ) } } } // package foo var ErrCouldNotOpen = errors . New ( \"could not open\" ) func Open () error { return ErrCouldNotOpen } // package bar err := foo . Open () if err != nil { if errors . Is ( err , foo . ErrCouldNotOpen ) { // handle } else { panic ( \"unknown error\" ) } } If you have an error that clients may need to detect, and you would like to add more information to it (e.g., it is not a static string), then you should use a custom type. Bad Good func open ( file string ) error { return fmt . Errorf ( \"file %q not found\" , file ) } func use () { err := open ( \"testfile.txt\" ) if err != nil { if strings . Contains ( err . Error (), \"not found\" ) { // handle } else { panic ( \"unknown error\" ) } } } type errNotFound struct { file string } func ( e errNotFound ) Error () string { return fmt . Sprintf ( \"file %q not found\" , e . file ) } func open ( file string ) error { return errNotFound { file : file } } func use () { err := open ( \"testfile.txt\" ) if err != nil { var errNF * errNotFound if errors . As ( err , & errNF ) { // handle the error. errNF contains the error's value. } else { panic ( \"unknown error\" ) } } } Error Wrapping There are three main options for propagating errors if a call fails: Return the original error if there is no additional context to add and you want to maintain the original error type. Add context using fmt.Errorf with the %w flag, so that the error message provides more context. It is recommended to add context where possible so that instead of a vague error such as \"connection refused\", you get more useful errors such as \"call service foo: connection refused\". See also Don't just check errors, handle them gracefully . Handle Type Assertion Failures The single return value form of a type assertion will panic on an incorrect type. Therefore, always use the \"comma ok\" idiom. Bad Good t := i .( string ) t , ok := i .( string ) if ! ok { // handle the error gracefully } Avoid Mutable Globals Avoid mutating global variables, instead opting for dependency injection. This applies to function pointers as well as other kinds of values. Bad Good // sign.go var _timeNow = time . Now func sign ( msg string ) string { now := _timeNow () return signWithTime ( msg , now ) } // sign.go type signer struct { now func () time . Time } func newSigner () * signer { return & signer { now : time . Now , } } func ( s * signer ) Sign ( msg string ) string { now := s . now () return signWithTime ( msg , now ) } // sign_test.go func TestSign ( t * testing . T ) { oldTimeNow := _timeNow _timeNow = func () time . Time { return someFixedTime } defer func () { _timeNow = oldTimeNow }() assert . Equal ( t , want , sign ( give )) } // sign_test.go func TestSigner ( t * testing . T ) { s := newSigner () s . now = func () time . Time { return someFixedTime } assert . Equal ( t , want , s . Sign ( give )) } Avoid init() Avoid init() where possible. When init() is unavoidable or desirable, code should attempt to: Be completely deterministic, regardless of program environment or invocation. Avoid depending on the ordering or side effects of other init() functions. While init() ordering is well-known, code can change, and thus relationships between init() functions can make code brittle and error-prone. Avoid accessing or manipulating global or environment state, such as machine information, environment variables, working directory, program arguments/inputs, etc. Avoid I/O, including both filesystem, network, and system calls. Code that cannot satisfy these requirements likely belongs as a helper to be called as part of main() (or elsewhere in a program's lifecycle), or be written as part of main() itself. In particular, libraries that are intended to be used by other programs should take special care to be completely deterministic and not perform \"init magic\". Bad Good type Foo struct { // ... } var _defaultFoo Foo func init () { _defaultFoo = Foo { // ... } } var _defaultFoo = Foo { // ... } // or, better, for testability: var _defaultFoo = defaultFoo () func defaultFoo () Foo { return Foo { // ... } } type Config struct { // ... } var _config Config func init () { // Bad: based on current directory cwd , _ := os . Getwd () // Bad: I/O raw , _ := ioutil . ReadFile ( path . Join ( cwd , \"config\" , \"config.yaml\" ), ) yaml . Unmarshal ( raw , & _config ) } type Config struct { // ... } func loadConfig () Config { cwd , err := os . Getwd () // handle err raw , err := ioutil . ReadFile ( path . Join ( cwd , \"config\" , \"config.yaml\" ), ) // handle err var config Config yaml . Unmarshal ( raw , & config ) return config } Considering the above, some situations in which init() may be preferable or necessary might include: Complex expressions that cannot be represented as single assignments. Pluggable hooks, such as database/sql dialects, encoding type registries, etc. Exit in Main Go programs use os.Exit or log.Fatal* to exit immediately. (Panicking is not a good way to exit programs, please don't panic.) Call one of os.Exit or log.Fatal* only in main() . All other functions should return errors to signal failure. Bad Good func main () { body := readFile ( path ) fmt . Println ( body ) } func readFile ( path string ) string { file , err := os . Open ( path ) if err != nil { log . Fatal ( err ) } payload , err := ioutil . ReadAll ( file ) if err != nil { log . Fatal ( err ) } return string ( payload ) } func main () { body , err := readFile ( path ) if err != nil { log . Fatal ( err ) } fmt . Println ( body ) } func readFile ( path string ) ( string , error ) { file , err := os . Open ( path ) if err != nil { return \"\" , err } payload , err := ioutil . ReadAll ( file ) if err != nil { return \"\" , err } return string ( payload ), nil } Rationale: Programs with multiple functions that exit present a few issues: Non-obvious control flow: Any function can exit the program, so it becomes difficult to reason about the control flow. Difficult to test: A function that exits the program will also exit the test calling it. This makes the function difficult to test and introduces risk of skipping other tests that have not yet been run by go test . Skipped cleanup: When a function exits the program, it skips function calls enqueued with defer statements. This adds risk of skipping important cleanup tasks. Exit Once If possible, prefer to call os.Exit or log.Fatal at most once in your main() . If there are multiple error scenarios that halt program execution, put that logic under a separate function and return errors from it. This has the effect of shortening your main() function and putting all key business logic into a separate, testable function. Bad Good package main func main () { args := os . Args [ 1 :] if len ( args ) != 1 { log . Fatal ( \"missing file\" ) } name := args [ 0 ] file , err := os . Open ( name ) if err != nil { log . Fatal ( err ) } defer file . Close () // If we call log.Fatal after this line, // file.Close will not be called. payload , err := ioutil . ReadAll ( file ) if err != nil { log . Fatal ( err ) } // ... } package main func main () { err := run () if err != nil { log . Fatal ( err ) } } func run () error { args := os . Args [ 1 :] if len ( args ) != 1 { return errors . New ( \"missing file\" ) } name := args [ 0 ] file , err := os . Open ( name ) if err != nil { return err } defer file . Close () payload , err := ioutil . ReadAll ( file ) if err != nil { return err } // ... } Performance Performance-specific guidelines apply only to the hot path(s) of the application. Prefer strconv over fmt When converting primitives to/from strings, strconv is faster than fmt . Bad Good for i := 0 ; i < b . N ; i ++ { s := fmt . Sprint ( rand . Int ()) } for i := 0 ; i < b . N ; i ++ { s := strconv . Itoa ( rand . Int ()) } BenchmarkFmtSprint-4 143 ns/op 2 allocs/op BenchmarkStrconv-4 64.2 ns/op 1 allocs/op Avoid string-to-byte conversion Do not create byte slices from a fixed string repeatedly. Instead, perform the conversion once and capture the result. Bad Good for i := 0 ; i < b . N ; i ++ { w . Write ([] byte ( \"Hello world\" )) } data := [] byte ( \"Hello world\" ) for i := 0 ; i < b . N ; i ++ { w . Write ( data ) } BenchmarkBad-4 50000000 22.2 ns/op BenchmarkGood-4 500000000 3.25 ns/op Prefer Specifying Container Capacity Specify container capacity where possible in order to allocate memory for the container up front. This minimizes subsequent allocations (by copying and resizing of the container) as elements are added. Specifying Map Capacity Hints Where possible, provide capacity hints when initializing maps with make() . make ( map [ T1 ] T2 , hint ) Providing a capacity hint to make() tries to right-size the map at initialization time, which reduces the need for growing the map and allocations as elements are added to the map. Note that, unlike slices, map capacity hints do not guarantee complete, preemptive allocation, but are used to approximate the number of hashmap buckets required. Consequently, allocations may still occur when adding elements to the map, even up to the specified capacity. Bad Good m := make ( map [ string ] os . FileInfo ) files , _ := ioutil . ReadDir ( \"./files\" ) for _ , f := range files { m [ f . Name ()] = f } files , _ := ioutil . ReadDir ( \"./files\" ) m := make ( map [ string ] os . FileInfo , len ( files )) for _ , f := range files { m [ f . Name ()] = f } `m` is created without a size hint; there may be more allocations at assignment time. `m` is created with a size hint; there may be fewer allocations at assignment time. Specifying Slice Capacity Where possible, provide capacity hints when initializing slices with make() , particularly when appending. make ([] T , length , capacity ) Unlike maps, slice capacity is not a hint: the compiler will allocate enough memory for the capacity of the slice as provided to make() , which means that subsequent append() operations will incur zero allocations (until the length of the slice matches the capacity, after which any appends will require a resize to hold additional elements). Bad Good for n := 0 ; n < b . N ; n ++ { data := make ([] int , 0 ) for k := 0 ; k < size ; k ++ { data = append ( data , k ) } } for n := 0 ; n < b . N ; n ++ { data := make ([] int , 0 , size ) for k := 0 ; k < size ; k ++ { data = append ( data , k ) } } BenchmarkBad-4 100000000 2.48s BenchmarkGood-4 100000000 0.21s Style Import Grouping In order to make imports orderly and clear, imported packages should be grouped in the following order from top to bottom: Standard library External dependencies External dependencies from our org Internal dependencies Within each import group, imports should be sorted alphabetically. Bad Good import ( \"errors\" \"fmt\" \"github.com/rs/zerolog\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" \"sync\" \"time\" ) import ( \"errors\" \"fmt\" \"sync\" \"time\" \"github.com/rs/zerolog\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" ) Package Names When naming packages, choose a name that is: All lower-case. No capitals or underscores. Does not need to be renamed using named imports at most call sites. Short and succinct. Remember that the name is identified in full at every call site. Not plural. For example, net/url , not net/urls . Not \"common\", \"util\", \"shared\", or \"lib\". These are bad, uninformative names. See also Package Names and Style guideline for Go packages . Function Names We follow the Go community's convention of using MixedCaps for function names . An exception is made for test functions, which may contain underscores for the purpose of grouping related test cases, e.g., TestMyFunction_WhatIsBeingTested . Function Grouping and Ordering Functions should be sorted in rough call order. Functions in a file should be grouped by receiver. Therefore, exported functions should appear first in a file, after struct , const , var definitions. A newXYZ() / NewXYZ() may appear after the type is defined, but before the rest of the methods on the receiver. Since functions are grouped by receiver, plain utility functions should appear towards the end of the file. Bad Good func ( s * something ) Cost () { return calcCost ( s . weights ) } type something struct { ... } func calcCost ( n [] int ) int { ... } func ( s * something ) Stop () { ... } func newSomething () * something { return & something {} } type something struct { ... } func newSomething () * something { return & something {} } func ( s * something ) Cost () { return calcCost ( s . weights ) } func ( s * something ) Stop () { ... } func calcCost ( n [] int ) int { ... } Reduce Nesting Code should reduce nesting where possible by handling error cases/special conditions first and returning early or continuing the loop. Reduce the amount of code that is nested multiple levels. Bad Good for _ , v := range data { if v . F1 == 1 { v = process ( v ) err := v . Call () if err == nil { v . Send () } else { return err } } else { log . Printf ( \"Invalid v: %v\" , v ) } } for _ , v := range data { if v . F1 != 1 { log . Printf ( \"Invalid v: %v\" , v ) continue } v = process ( v ) err := v . Call () if err != nil { return err } v . Send () } Unnecessary Else If a variable is set in both branches of an if, it can be replaced with a single if. Bad Good var a int if b { a = 100 } else { a = 10 } a := 10 if b { a = 100 } Top-level Variable Declarations At the top level, use the standard var keyword. Do not specify the type, unless it is not the same type as the expression. Bad Good var _s string = F () func F () string { return \"A\" } var _s = F () // Since F already states that it returns a string, we don't need to specify // the type again. func F () string { return \"A\" } Specify the type if the type of the expression does not match the desired type exactly. type myError struct {} func ( myError ) Error () string { return \"error\" } func F () myError { return myError {} } var _e error = F () // F returns an object of type myError, but we want error. Local Variable Declarations Short variable declarations ( := ) should be used if a variable is being set to some value explicitly. Bad Good var s = \"foo\" s := \"foo\" However, there are cases where the default value is clearer when the var keyword is used. Declaring Empty Slices , for example. Bad Good func f ( list [] int ) { filtered := [] int {} for _ , v := range list { if v > 10 { filtered = append ( filtered , v ) } } } func f ( list [] int ) { var filtered [] int for _ , v := range list { if v > 10 { filtered = append ( filtered , v ) } } } Reduce Scope of Variables Where possible, reduce scope of variables. Do not reduce the scope if it conflicts with Reduce Nesting . Bad Good data , err := ioutil . ReadFile ( name ) if err == nil { err = cfg . Decode ( data ) if err != nil { return err } fmt . Println ( cfg ) return nil } else { return err } data , err := ioutil . ReadFile ( name ) if err != nil { return err } err = cfg . Decode ( data ) if err != nil { return err } fmt . Println ( cfg ) return nil Avoid Naked Parameters Naked parameters in function calls can hurt readability. Add C-style comments( /* ... */ ) for parameter names when their meaning is not obvious. Bad Good // func printInfo(name string, isLocal, done bool) printInfo ( \"foo\" , true , true ) // func printInfo(name string, isLocal, done bool) printInfo ( \"foo\" , true /* isLocal */ , true /* done */ ) Better yet, replace naked bool types with custom types for more readable and type-safe code. This allows more than just two states (true/false) for that parameter in the future. type Region int const ( UnknownRegion Region = iota Local ) type Status int const ( StatusReady Status = iota + 1 StatusDone // Maybe we will have a StatusInProgress in the future. ) func printInfo ( name string , region Region , status Status ) Initializing Structs Use Field Names to Initialize Structs You should almost always specify field names when initializing structs. This is enforced by go vet . Bad Good k := User { \"John\" , \"Doe\" , true } k := User { FirstName : \"John\" , LastName : \"Doe\" , Admin : true , } Exception: Field names may be omitted in test tables when there are 3 or fewer fields. tests := [] struct { op Operation want string }{ { Add , \"add\" }, { Subtract , \"subtract\" }, } Omit Zero Value Fields in Structs When initializing structs with field names, omit fields that have zero values unless they provide meaningful context. Otherwise, let Go set these to zero values automatically. Bad Good user := User { FirstName : \"John\" , LastName : \"Doe\" , MiddleName : \"\" , Admin : false , } user := User { FirstName : \"John\" , LastName : \"Doe\" , } This helps reduce noise for readers by omitting values that are default in that context. Only meaningful values are specified. Include zero values where field names provide meaningful context. For example, test cases in test tables can benefit from names of fields even when they are zero-valued. tests := [] struct { give string want int }{ { give : \"0\" , want : 0 }, // ... } Use var for Zero Value Structs When all the fields of a struct are omitted in a declaration, use the var form to declare the struct. Bad Good user := User {} var user User This differentiates zero valued structs from those with non-zero fields similar to the distinction created for map initialization , and matches how we prefer to declare empty slices . Initializing Struct References Use &T{} instead of new(T) when initializing struct references so that it is consistent with the struct initialization. Bad Good sval := T { Name : \"foo\" } // inconsistent sptr := new ( T ) sptr . Name = \"bar\" sval := T { Name : \"foo\" } sptr := & T { Name : \"bar\" } Initializing Maps Prefer make(..) for empty maps, and maps populated programmatically. This makes map initialization visually distinct from declaration, and it makes it easy to add size hints later if available. Bad Good var ( // m1 is safe to read and write; // m2 will panic on writes. m1 = map [ T1 ] T2 {} m2 map [ T1 ] T2 ) var ( // m1 is safe to read and write; // m2 will panic on writes. m1 = make ( map [ T1 ] T2 ) m2 map [ T1 ] T2 ) Declaration and initialization are visually similar. Declaration and initialization are visually distinct. Where possible, provide capacity hints when initializing maps with make() . See Specifying Map Capacity Hints for more information. On the other hand, if the map holds a fixed list of elements, use map literals to initialize the map. Bad Good m := make ( map [ T1 ] T2 , 3 ) m [ k1 ] = v1 m [ k2 ] = v2 m [ k3 ] = v3 m := map [ T1 ] T2 { k1 : v1 , k2 : v2 , k3 : v3 , } The basic rule of thumb is to use map literals when adding a fixed set of elements at initialization time, otherwise use make (and specify a size hint if available). Patterns Functional Options Functional options is a pattern in which you declare an opaque Option type that records information in some internal struct. You accept a variadic number of these options and act upon the full information recorded by the options on the internal struct. Use this pattern for optional arguments in constructors and other public APIs that you foresee needing to expand, especially if you already have three or more arguments on those functions. Bad Good // package db func Open ( addr string , cache bool , logger * zap . Logger ) ( * Connection , error ) { // ... } // package db type Option interface { // ... } func WithCache ( c bool ) Option { // ... } func WithLogger ( log * zap . Logger ) Option { // ... } // Open creates a connection. func Open ( addr string , opts ... Option , ) ( * Connection , error ) { // ... } The cache and logger parameters must always be provided, even if the user wants to use the default. db . Open ( addr , db . DefaultCache , zap . NewNop ()) db . Open ( addr , db . DefaultCache , log ) db . Open ( addr , false /* cache */ , zap . NewNop ()) db . Open ( addr , false /* cache */ , log ) Options are provided only if needed. db . Open ( addr ) db . Open ( addr , db . WithLogger ( log )) db . Open ( addr , db . WithCache ( false )) db . Open ( addr , db . WithCache ( false ), db . WithLogger ( log ), ) Our suggested way of implementing this pattern is with an Option interface that holds an unexported method, recording options on an unexported options struct. type options struct { cache bool logger * zap . Logger } type Option interface { apply ( * options ) } type cacheOption bool func ( c cacheOption ) apply ( opts * options ) { opts . cache = bool ( c ) } func WithCache ( c bool ) Option { return cacheOption ( c ) } type loggerOption struct { Log * zap . Logger } func ( l loggerOption ) apply ( opts * options ) { opts . logger = l . Log } func WithLogger ( log * zap . Logger ) Option { return loggerOption { Log : log } } // Open creates a connection. func Open ( addr string , opts ... Option , ) ( * Connection , error ) { options := options { cache : defaultCache , logger : zap . NewNop (), } for _ , o := range opts { o . apply ( & options ) } // ... } Note that there's a method of implementing this pattern with closures, but we believe that the pattern above provides more flexibility for authors and is easier to debug and test for users. In particular, it allows options to be compared against each other in tests and mocks, versus closures where this is impossible. Further, it lets options implement other interfaces, including fmt.Stringer which allows for user-readable string representations of the options. See also, Self-referential functions and the design of options Functional options for friendly APIs","title":"Go"},{"location":"code/go/#go-code-style-guidelines","text":"","title":"Go Code Style Guidelines"},{"location":"code/go/#guidelines","text":"","title":"Guidelines"},{"location":"code/go/#copy-slices-and-maps-at-boundaries","text":"Slices and maps contain pointers to the underlying data so be wary of scenarios when they need to be copied.","title":"Copy Slices and Maps at Boundaries"},{"location":"code/go/#receiving-slices-and-maps","text":"Keep in mind that users can modify a map or slice you received as an argument if you store a reference to it. Bad Good func ( d * Driver ) SetTrips ( trips [] Trip ) { d . trips = trips } trips := ... d1 . SetTrips ( trips ) // Did you mean to modify d1.trips? trips [ 0 ] = ... func ( d * Driver ) SetTrips ( trips [] Trip ) { d . trips = make ([] Trip , len ( trips )) copy ( d . trips , trips ) } trips := ... d1 . SetTrips ( trips ) // We can now modify trips[0] without affecting d1.trips. trips [ 0 ] = ...","title":"Receiving Slices and Maps"},{"location":"code/go/#returning-slices-and-maps","text":"Similarly, be wary of user modifications to maps or slices exposing internal state. Bad Good type Stats struct { mu sync . Mutex counters map [ string ] int } // Snapshot returns the current stats. func ( s * Stats ) Snapshot () map [ string ] int { s . mu . Lock () defer s . mu . Unlock () return s . counters } // snapshot is no longer protected by the mutex, so any // access to the snapshot is subject to data races. snapshot := stats . Snapshot () type Stats struct { mu sync . Mutex counters map [ string ] int } func ( s * Stats ) Snapshot () map [ string ] int { s . mu . Lock () defer s . mu . Unlock () result := make ( map [ string ] int , len ( s . counters )) for k , v := range s . counters { result [ k ] = v } return result } // Snapshot is now a copy. snapshot := stats . Snapshot ()","title":"Returning Slices and Maps"},{"location":"code/go/#start-enums-at-one","text":"The standard way of introducing enumerations in Go is to declare a custom type and a const group with iota . Since variables have a 0 default value, you should usually start your enums on a non-zero value. Bad Good type Operation int const ( Add Operation = iota Subtract Multiply ) // Add=0, Subtract=1, Multiply=2 type Operation int const ( Add Operation = iota + 1 Subtract Multiply ) // Add=1, Subtract=2, Multiply=3 There are cases where using the zero value makes sense, for example when the zero value case is the desirable default behavior. type LogOutput int const ( LogToStdout LogOutput = iota LogToFile LogToRemote ) // LogToStdout=0, LogToFile=1, LogToRemote=2","title":"Start Enums at One"},{"location":"code/go/#error-types","text":"There are various options for declaring errors: errors.New for errors with simple static strings fmt.Errorf for formatted and wrapped error strings Custom types that implement the error interface When returning errors, consider the following to determine the best choice: Is this a simple error that needs no extra information? If so, errors.New should suffice. Do the clients need to detect and handle this error? If so, you should use a custom type, and implement the Error() method. Are you propagating an error returned by a downstream function? If so, check the section on error wrapping . Otherwise, fmt.Errorf is okay. If the client needs to detect a specific error case, a sentinel error should be created using errors.New . Bad Good // package foo func Open () error { return errors . New ( \"could not open\" ) } // package bar func use () { err := foo . Open () if err != nil { if err . Error () == \"could not open\" { // handle } else { panic ( \"unknown error\" ) } } } // package foo var ErrCouldNotOpen = errors . New ( \"could not open\" ) func Open () error { return ErrCouldNotOpen } // package bar err := foo . Open () if err != nil { if errors . Is ( err , foo . ErrCouldNotOpen ) { // handle } else { panic ( \"unknown error\" ) } } If you have an error that clients may need to detect, and you would like to add more information to it (e.g., it is not a static string), then you should use a custom type. Bad Good func open ( file string ) error { return fmt . Errorf ( \"file %q not found\" , file ) } func use () { err := open ( \"testfile.txt\" ) if err != nil { if strings . Contains ( err . Error (), \"not found\" ) { // handle } else { panic ( \"unknown error\" ) } } } type errNotFound struct { file string } func ( e errNotFound ) Error () string { return fmt . Sprintf ( \"file %q not found\" , e . file ) } func open ( file string ) error { return errNotFound { file : file } } func use () { err := open ( \"testfile.txt\" ) if err != nil { var errNF * errNotFound if errors . As ( err , & errNF ) { // handle the error. errNF contains the error's value. } else { panic ( \"unknown error\" ) } } }","title":"Error Types"},{"location":"code/go/#error-wrapping","text":"There are three main options for propagating errors if a call fails: Return the original error if there is no additional context to add and you want to maintain the original error type. Add context using fmt.Errorf with the %w flag, so that the error message provides more context. It is recommended to add context where possible so that instead of a vague error such as \"connection refused\", you get more useful errors such as \"call service foo: connection refused\". See also Don't just check errors, handle them gracefully .","title":"Error Wrapping"},{"location":"code/go/#handle-type-assertion-failures","text":"The single return value form of a type assertion will panic on an incorrect type. Therefore, always use the \"comma ok\" idiom. Bad Good t := i .( string ) t , ok := i .( string ) if ! ok { // handle the error gracefully }","title":"Handle Type Assertion Failures"},{"location":"code/go/#avoid-mutable-globals","text":"Avoid mutating global variables, instead opting for dependency injection. This applies to function pointers as well as other kinds of values. Bad Good // sign.go var _timeNow = time . Now func sign ( msg string ) string { now := _timeNow () return signWithTime ( msg , now ) } // sign.go type signer struct { now func () time . Time } func newSigner () * signer { return & signer { now : time . Now , } } func ( s * signer ) Sign ( msg string ) string { now := s . now () return signWithTime ( msg , now ) } // sign_test.go func TestSign ( t * testing . T ) { oldTimeNow := _timeNow _timeNow = func () time . Time { return someFixedTime } defer func () { _timeNow = oldTimeNow }() assert . Equal ( t , want , sign ( give )) } // sign_test.go func TestSigner ( t * testing . T ) { s := newSigner () s . now = func () time . Time { return someFixedTime } assert . Equal ( t , want , s . Sign ( give )) }","title":"Avoid Mutable Globals"},{"location":"code/go/#avoid-init","text":"Avoid init() where possible. When init() is unavoidable or desirable, code should attempt to: Be completely deterministic, regardless of program environment or invocation. Avoid depending on the ordering or side effects of other init() functions. While init() ordering is well-known, code can change, and thus relationships between init() functions can make code brittle and error-prone. Avoid accessing or manipulating global or environment state, such as machine information, environment variables, working directory, program arguments/inputs, etc. Avoid I/O, including both filesystem, network, and system calls. Code that cannot satisfy these requirements likely belongs as a helper to be called as part of main() (or elsewhere in a program's lifecycle), or be written as part of main() itself. In particular, libraries that are intended to be used by other programs should take special care to be completely deterministic and not perform \"init magic\". Bad Good type Foo struct { // ... } var _defaultFoo Foo func init () { _defaultFoo = Foo { // ... } } var _defaultFoo = Foo { // ... } // or, better, for testability: var _defaultFoo = defaultFoo () func defaultFoo () Foo { return Foo { // ... } } type Config struct { // ... } var _config Config func init () { // Bad: based on current directory cwd , _ := os . Getwd () // Bad: I/O raw , _ := ioutil . ReadFile ( path . Join ( cwd , \"config\" , \"config.yaml\" ), ) yaml . Unmarshal ( raw , & _config ) } type Config struct { // ... } func loadConfig () Config { cwd , err := os . Getwd () // handle err raw , err := ioutil . ReadFile ( path . Join ( cwd , \"config\" , \"config.yaml\" ), ) // handle err var config Config yaml . Unmarshal ( raw , & config ) return config } Considering the above, some situations in which init() may be preferable or necessary might include: Complex expressions that cannot be represented as single assignments. Pluggable hooks, such as database/sql dialects, encoding type registries, etc.","title":"Avoid init()"},{"location":"code/go/#exit-in-main","text":"Go programs use os.Exit or log.Fatal* to exit immediately. (Panicking is not a good way to exit programs, please don't panic.) Call one of os.Exit or log.Fatal* only in main() . All other functions should return errors to signal failure. Bad Good func main () { body := readFile ( path ) fmt . Println ( body ) } func readFile ( path string ) string { file , err := os . Open ( path ) if err != nil { log . Fatal ( err ) } payload , err := ioutil . ReadAll ( file ) if err != nil { log . Fatal ( err ) } return string ( payload ) } func main () { body , err := readFile ( path ) if err != nil { log . Fatal ( err ) } fmt . Println ( body ) } func readFile ( path string ) ( string , error ) { file , err := os . Open ( path ) if err != nil { return \"\" , err } payload , err := ioutil . ReadAll ( file ) if err != nil { return \"\" , err } return string ( payload ), nil } Rationale: Programs with multiple functions that exit present a few issues: Non-obvious control flow: Any function can exit the program, so it becomes difficult to reason about the control flow. Difficult to test: A function that exits the program will also exit the test calling it. This makes the function difficult to test and introduces risk of skipping other tests that have not yet been run by go test . Skipped cleanup: When a function exits the program, it skips function calls enqueued with defer statements. This adds risk of skipping important cleanup tasks.","title":"Exit in Main"},{"location":"code/go/#exit-once","text":"If possible, prefer to call os.Exit or log.Fatal at most once in your main() . If there are multiple error scenarios that halt program execution, put that logic under a separate function and return errors from it. This has the effect of shortening your main() function and putting all key business logic into a separate, testable function. Bad Good package main func main () { args := os . Args [ 1 :] if len ( args ) != 1 { log . Fatal ( \"missing file\" ) } name := args [ 0 ] file , err := os . Open ( name ) if err != nil { log . Fatal ( err ) } defer file . Close () // If we call log.Fatal after this line, // file.Close will not be called. payload , err := ioutil . ReadAll ( file ) if err != nil { log . Fatal ( err ) } // ... } package main func main () { err := run () if err != nil { log . Fatal ( err ) } } func run () error { args := os . Args [ 1 :] if len ( args ) != 1 { return errors . New ( \"missing file\" ) } name := args [ 0 ] file , err := os . Open ( name ) if err != nil { return err } defer file . Close () payload , err := ioutil . ReadAll ( file ) if err != nil { return err } // ... }","title":"Exit Once"},{"location":"code/go/#performance","text":"Performance-specific guidelines apply only to the hot path(s) of the application.","title":"Performance"},{"location":"code/go/#prefer-strconv-over-fmt","text":"When converting primitives to/from strings, strconv is faster than fmt . Bad Good for i := 0 ; i < b . N ; i ++ { s := fmt . Sprint ( rand . Int ()) } for i := 0 ; i < b . N ; i ++ { s := strconv . Itoa ( rand . Int ()) } BenchmarkFmtSprint-4 143 ns/op 2 allocs/op BenchmarkStrconv-4 64.2 ns/op 1 allocs/op","title":"Prefer strconv over fmt"},{"location":"code/go/#avoid-string-to-byte-conversion","text":"Do not create byte slices from a fixed string repeatedly. Instead, perform the conversion once and capture the result. Bad Good for i := 0 ; i < b . N ; i ++ { w . Write ([] byte ( \"Hello world\" )) } data := [] byte ( \"Hello world\" ) for i := 0 ; i < b . N ; i ++ { w . Write ( data ) } BenchmarkBad-4 50000000 22.2 ns/op BenchmarkGood-4 500000000 3.25 ns/op","title":"Avoid string-to-byte conversion"},{"location":"code/go/#prefer-specifying-container-capacity","text":"Specify container capacity where possible in order to allocate memory for the container up front. This minimizes subsequent allocations (by copying and resizing of the container) as elements are added.","title":"Prefer Specifying Container Capacity"},{"location":"code/go/#specifying-map-capacity-hints","text":"Where possible, provide capacity hints when initializing maps with make() . make ( map [ T1 ] T2 , hint ) Providing a capacity hint to make() tries to right-size the map at initialization time, which reduces the need for growing the map and allocations as elements are added to the map. Note that, unlike slices, map capacity hints do not guarantee complete, preemptive allocation, but are used to approximate the number of hashmap buckets required. Consequently, allocations may still occur when adding elements to the map, even up to the specified capacity. Bad Good m := make ( map [ string ] os . FileInfo ) files , _ := ioutil . ReadDir ( \"./files\" ) for _ , f := range files { m [ f . Name ()] = f } files , _ := ioutil . ReadDir ( \"./files\" ) m := make ( map [ string ] os . FileInfo , len ( files )) for _ , f := range files { m [ f . Name ()] = f } `m` is created without a size hint; there may be more allocations at assignment time. `m` is created with a size hint; there may be fewer allocations at assignment time.","title":"Specifying Map Capacity Hints"},{"location":"code/go/#specifying-slice-capacity","text":"Where possible, provide capacity hints when initializing slices with make() , particularly when appending. make ([] T , length , capacity ) Unlike maps, slice capacity is not a hint: the compiler will allocate enough memory for the capacity of the slice as provided to make() , which means that subsequent append() operations will incur zero allocations (until the length of the slice matches the capacity, after which any appends will require a resize to hold additional elements). Bad Good for n := 0 ; n < b . N ; n ++ { data := make ([] int , 0 ) for k := 0 ; k < size ; k ++ { data = append ( data , k ) } } for n := 0 ; n < b . N ; n ++ { data := make ([] int , 0 , size ) for k := 0 ; k < size ; k ++ { data = append ( data , k ) } } BenchmarkBad-4 100000000 2.48s BenchmarkGood-4 100000000 0.21s","title":"Specifying Slice Capacity"},{"location":"code/go/#style","text":"","title":"Style"},{"location":"code/go/#import-grouping","text":"In order to make imports orderly and clear, imported packages should be grouped in the following order from top to bottom: Standard library External dependencies External dependencies from our org Internal dependencies Within each import group, imports should be sorted alphabetically. Bad Good import ( \"errors\" \"fmt\" \"github.com/rs/zerolog\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" \"sync\" \"time\" ) import ( \"errors\" \"fmt\" \"sync\" \"time\" \"github.com/rs/zerolog\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" )","title":"Import Grouping"},{"location":"code/go/#package-names","text":"When naming packages, choose a name that is: All lower-case. No capitals or underscores. Does not need to be renamed using named imports at most call sites. Short and succinct. Remember that the name is identified in full at every call site. Not plural. For example, net/url , not net/urls . Not \"common\", \"util\", \"shared\", or \"lib\". These are bad, uninformative names. See also Package Names and Style guideline for Go packages .","title":"Package Names"},{"location":"code/go/#function-names","text":"We follow the Go community's convention of using MixedCaps for function names . An exception is made for test functions, which may contain underscores for the purpose of grouping related test cases, e.g., TestMyFunction_WhatIsBeingTested .","title":"Function Names"},{"location":"code/go/#function-grouping-and-ordering","text":"Functions should be sorted in rough call order. Functions in a file should be grouped by receiver. Therefore, exported functions should appear first in a file, after struct , const , var definitions. A newXYZ() / NewXYZ() may appear after the type is defined, but before the rest of the methods on the receiver. Since functions are grouped by receiver, plain utility functions should appear towards the end of the file. Bad Good func ( s * something ) Cost () { return calcCost ( s . weights ) } type something struct { ... } func calcCost ( n [] int ) int { ... } func ( s * something ) Stop () { ... } func newSomething () * something { return & something {} } type something struct { ... } func newSomething () * something { return & something {} } func ( s * something ) Cost () { return calcCost ( s . weights ) } func ( s * something ) Stop () { ... } func calcCost ( n [] int ) int { ... }","title":"Function Grouping and Ordering"},{"location":"code/go/#reduce-nesting","text":"Code should reduce nesting where possible by handling error cases/special conditions first and returning early or continuing the loop. Reduce the amount of code that is nested multiple levels. Bad Good for _ , v := range data { if v . F1 == 1 { v = process ( v ) err := v . Call () if err == nil { v . Send () } else { return err } } else { log . Printf ( \"Invalid v: %v\" , v ) } } for _ , v := range data { if v . F1 != 1 { log . Printf ( \"Invalid v: %v\" , v ) continue } v = process ( v ) err := v . Call () if err != nil { return err } v . Send () }","title":"Reduce Nesting"},{"location":"code/go/#unnecessary-else","text":"If a variable is set in both branches of an if, it can be replaced with a single if. Bad Good var a int if b { a = 100 } else { a = 10 } a := 10 if b { a = 100 }","title":"Unnecessary Else"},{"location":"code/go/#top-level-variable-declarations","text":"At the top level, use the standard var keyword. Do not specify the type, unless it is not the same type as the expression. Bad Good var _s string = F () func F () string { return \"A\" } var _s = F () // Since F already states that it returns a string, we don't need to specify // the type again. func F () string { return \"A\" } Specify the type if the type of the expression does not match the desired type exactly. type myError struct {} func ( myError ) Error () string { return \"error\" } func F () myError { return myError {} } var _e error = F () // F returns an object of type myError, but we want error.","title":"Top-level Variable Declarations"},{"location":"code/go/#local-variable-declarations","text":"Short variable declarations ( := ) should be used if a variable is being set to some value explicitly. Bad Good var s = \"foo\" s := \"foo\" However, there are cases where the default value is clearer when the var keyword is used. Declaring Empty Slices , for example. Bad Good func f ( list [] int ) { filtered := [] int {} for _ , v := range list { if v > 10 { filtered = append ( filtered , v ) } } } func f ( list [] int ) { var filtered [] int for _ , v := range list { if v > 10 { filtered = append ( filtered , v ) } } }","title":"Local Variable Declarations"},{"location":"code/go/#reduce-scope-of-variables","text":"Where possible, reduce scope of variables. Do not reduce the scope if it conflicts with Reduce Nesting . Bad Good data , err := ioutil . ReadFile ( name ) if err == nil { err = cfg . Decode ( data ) if err != nil { return err } fmt . Println ( cfg ) return nil } else { return err } data , err := ioutil . ReadFile ( name ) if err != nil { return err } err = cfg . Decode ( data ) if err != nil { return err } fmt . Println ( cfg ) return nil","title":"Reduce Scope of Variables"},{"location":"code/go/#avoid-naked-parameters","text":"Naked parameters in function calls can hurt readability. Add C-style comments( /* ... */ ) for parameter names when their meaning is not obvious. Bad Good // func printInfo(name string, isLocal, done bool) printInfo ( \"foo\" , true , true ) // func printInfo(name string, isLocal, done bool) printInfo ( \"foo\" , true /* isLocal */ , true /* done */ ) Better yet, replace naked bool types with custom types for more readable and type-safe code. This allows more than just two states (true/false) for that parameter in the future. type Region int const ( UnknownRegion Region = iota Local ) type Status int const ( StatusReady Status = iota + 1 StatusDone // Maybe we will have a StatusInProgress in the future. ) func printInfo ( name string , region Region , status Status )","title":"Avoid Naked Parameters"},{"location":"code/go/#initializing-structs","text":"","title":"Initializing Structs"},{"location":"code/go/#use-field-names-to-initialize-structs","text":"You should almost always specify field names when initializing structs. This is enforced by go vet . Bad Good k := User { \"John\" , \"Doe\" , true } k := User { FirstName : \"John\" , LastName : \"Doe\" , Admin : true , } Exception: Field names may be omitted in test tables when there are 3 or fewer fields. tests := [] struct { op Operation want string }{ { Add , \"add\" }, { Subtract , \"subtract\" }, }","title":"Use Field Names to Initialize Structs"},{"location":"code/go/#omit-zero-value-fields-in-structs","text":"When initializing structs with field names, omit fields that have zero values unless they provide meaningful context. Otherwise, let Go set these to zero values automatically. Bad Good user := User { FirstName : \"John\" , LastName : \"Doe\" , MiddleName : \"\" , Admin : false , } user := User { FirstName : \"John\" , LastName : \"Doe\" , } This helps reduce noise for readers by omitting values that are default in that context. Only meaningful values are specified. Include zero values where field names provide meaningful context. For example, test cases in test tables can benefit from names of fields even when they are zero-valued. tests := [] struct { give string want int }{ { give : \"0\" , want : 0 }, // ... }","title":"Omit Zero Value Fields in Structs"},{"location":"code/go/#use-var-for-zero-value-structs","text":"When all the fields of a struct are omitted in a declaration, use the var form to declare the struct. Bad Good user := User {} var user User This differentiates zero valued structs from those with non-zero fields similar to the distinction created for map initialization , and matches how we prefer to declare empty slices .","title":"Use var for Zero Value Structs"},{"location":"code/go/#initializing-struct-references","text":"Use &T{} instead of new(T) when initializing struct references so that it is consistent with the struct initialization. Bad Good sval := T { Name : \"foo\" } // inconsistent sptr := new ( T ) sptr . Name = \"bar\" sval := T { Name : \"foo\" } sptr := & T { Name : \"bar\" }","title":"Initializing Struct References"},{"location":"code/go/#initializing-maps","text":"Prefer make(..) for empty maps, and maps populated programmatically. This makes map initialization visually distinct from declaration, and it makes it easy to add size hints later if available. Bad Good var ( // m1 is safe to read and write; // m2 will panic on writes. m1 = map [ T1 ] T2 {} m2 map [ T1 ] T2 ) var ( // m1 is safe to read and write; // m2 will panic on writes. m1 = make ( map [ T1 ] T2 ) m2 map [ T1 ] T2 ) Declaration and initialization are visually similar. Declaration and initialization are visually distinct. Where possible, provide capacity hints when initializing maps with make() . See Specifying Map Capacity Hints for more information. On the other hand, if the map holds a fixed list of elements, use map literals to initialize the map. Bad Good m := make ( map [ T1 ] T2 , 3 ) m [ k1 ] = v1 m [ k2 ] = v2 m [ k3 ] = v3 m := map [ T1 ] T2 { k1 : v1 , k2 : v2 , k3 : v3 , } The basic rule of thumb is to use map literals when adding a fixed set of elements at initialization time, otherwise use make (and specify a size hint if available).","title":"Initializing Maps"},{"location":"code/go/#patterns","text":"","title":"Patterns"},{"location":"code/go/#functional-options","text":"Functional options is a pattern in which you declare an opaque Option type that records information in some internal struct. You accept a variadic number of these options and act upon the full information recorded by the options on the internal struct. Use this pattern for optional arguments in constructors and other public APIs that you foresee needing to expand, especially if you already have three or more arguments on those functions. Bad Good // package db func Open ( addr string , cache bool , logger * zap . Logger ) ( * Connection , error ) { // ... } // package db type Option interface { // ... } func WithCache ( c bool ) Option { // ... } func WithLogger ( log * zap . Logger ) Option { // ... } // Open creates a connection. func Open ( addr string , opts ... Option , ) ( * Connection , error ) { // ... } The cache and logger parameters must always be provided, even if the user wants to use the default. db . Open ( addr , db . DefaultCache , zap . NewNop ()) db . Open ( addr , db . DefaultCache , log ) db . Open ( addr , false /* cache */ , zap . NewNop ()) db . Open ( addr , false /* cache */ , log ) Options are provided only if needed. db . Open ( addr ) db . Open ( addr , db . WithLogger ( log )) db . Open ( addr , db . WithCache ( false )) db . Open ( addr , db . WithCache ( false ), db . WithLogger ( log ), ) Our suggested way of implementing this pattern is with an Option interface that holds an unexported method, recording options on an unexported options struct. type options struct { cache bool logger * zap . Logger } type Option interface { apply ( * options ) } type cacheOption bool func ( c cacheOption ) apply ( opts * options ) { opts . cache = bool ( c ) } func WithCache ( c bool ) Option { return cacheOption ( c ) } type loggerOption struct { Log * zap . Logger } func ( l loggerOption ) apply ( opts * options ) { opts . logger = l . Log } func WithLogger ( log * zap . Logger ) Option { return loggerOption { Log : log } } // Open creates a connection. func Open ( addr string , opts ... Option , ) ( * Connection , error ) { options := options { cache : defaultCache , logger : zap . NewNop (), } for _ , o := range opts { o . apply ( & options ) } // ... } Note that there's a method of implementing this pattern with closures, but we believe that the pattern above provides more flexibility for authors and is easier to debug and test for users. In particular, it allows options to be compared against each other in tests and mocks, versus closures where this is impossible. Further, it lets options implement other interfaces, including fmt.Stringer which allows for user-readable string representations of the options. See also, Self-referential functions and the design of options Functional options for friendly APIs","title":"Functional Options"},{"location":"code/markdown/","text":"Markdown Style Guidelines General Always keep the scope of the document in mind. A document should precisely fulfill its purpose, nothing more, nothing less. It is a common pitfall to end up going into rabbit holes and spending half a document explaining irrelevant details. Try to write short sentences whenever possible to avoid complex grammar, complex use of tenses, ambiguous pronouns and so on. A good guideline when it comes to technical writing is to aim for 20-30 words per sentence. Keeping sentences short should however never come at the expense of clarity, syntactic cues and important information. Just like in code, consistency is key. Sections Section titles should follow the Chicago Title Capitalization standard. Documents should start with a level one heading and should ideally be the same as the file name. Sections should be ordered hierarchically. Each document starts with a level one heading ( # ), which can contain one or more level two headings ( ## ), which can contain one or more level threes ( ### ) and so on. Formatting Lists should use the * character rather than the - character. Keep extra newlines between paragraphs and sections to make the source easier to read. Paragraphs that include multiple sentences should have the sentences on separate lines, so that updating one sentence results in a clear diff where one line changes. For long files that are not served within this repository, it is best to have a table of contents at the end of the introduction of the level one heading section. Always specify the language for code blocks so that neither the syntax highlighter nor the text editor must guess. If no specific type makes sense, just use text . Technical Writing Use American English ( organize instead of organise , behavior instead of behaviour , etc.) Do not use gendered pronouns when talking about users/consumers/whatever but always they/their instead. Do not use the future tense but use present simple for expressing general truths instead. Use active voice when there is no specific need to use passive. Abbreviations and acronyms should be spelled out the first time they appear in any technical document with the shortened form appearing in parentheses immediately after the term. The abbreviation or acronym can then be used throughout the document. Avoid ambiguous and abstract language (i.e. really, quite, very), imprecise or subjective terms (i.e. fast, slow, tall, small) and words that have no precise meaning (i.e. bit, thing, stuff). Avoid contractions (i.e. don't, you'll, etc.) as they are meant for informal contexts. Avoid generalized statements, because they are difficult to substantiate and too broad to be supported. Avoid story-telling, remain factual and concise. Avoid jargon and humor. Avoid em-dashes. Putting non-restrictive relative clauses into separate sentences leads to simpler, clearer writing. If em-dashes are needed, make sure to use the right character: \u2014 (alt code: ALT+0151 ). When referring to something in a certain way (i.e. FBAS for Federated Byzantine Agreement System ) make sure to consistently use only FBAS consistently after the term is introduced. Links Use informative link titles. For example, instead of naming your links \"link\" or \"here\", wrap part of the sentence that is meant to be linked as a title. Links to external sources should be: Clear, concise, factual (not tips & tricks-type articles, or blog posts) Reliable to stand the test of time (will not start to 404 because it's a personal blog and the person decided to get rid of it for example) From reliable sources (this is where Wikipedia isn't always perfect, but fine for technical subjects) Whenever possible, use internal links instead of external ones (if something has been described in our documents somewhere, link to it instead of externally) Specific to this Repository In this repository, Markdown is extended by some mkdocs plugins which allow you to include external files within a markdown files, and to insert admonitions. Admonitions are a way to include side content into a page without significantly interrupting the document flow. There are different types of admonitions, and they allow for the inclusion and nesting of arbitrary content. Including external files can be done by writing the name of a file present in the include directory surrounded by { / } and exclamation marks on an empty line. External Resources Technical Writing Standards Google Markdown Style Guide Mastering GitHub Markdown","title":"Markdown"},{"location":"code/markdown/#markdown-style-guidelines","text":"","title":"Markdown Style Guidelines"},{"location":"code/markdown/#general","text":"Always keep the scope of the document in mind. A document should precisely fulfill its purpose, nothing more, nothing less. It is a common pitfall to end up going into rabbit holes and spending half a document explaining irrelevant details. Try to write short sentences whenever possible to avoid complex grammar, complex use of tenses, ambiguous pronouns and so on. A good guideline when it comes to technical writing is to aim for 20-30 words per sentence. Keeping sentences short should however never come at the expense of clarity, syntactic cues and important information. Just like in code, consistency is key.","title":"General"},{"location":"code/markdown/#sections","text":"Section titles should follow the Chicago Title Capitalization standard. Documents should start with a level one heading and should ideally be the same as the file name. Sections should be ordered hierarchically. Each document starts with a level one heading ( # ), which can contain one or more level two headings ( ## ), which can contain one or more level threes ( ### ) and so on.","title":"Sections"},{"location":"code/markdown/#formatting","text":"Lists should use the * character rather than the - character. Keep extra newlines between paragraphs and sections to make the source easier to read. Paragraphs that include multiple sentences should have the sentences on separate lines, so that updating one sentence results in a clear diff where one line changes. For long files that are not served within this repository, it is best to have a table of contents at the end of the introduction of the level one heading section. Always specify the language for code blocks so that neither the syntax highlighter nor the text editor must guess. If no specific type makes sense, just use text .","title":"Formatting"},{"location":"code/markdown/#technical-writing","text":"Use American English ( organize instead of organise , behavior instead of behaviour , etc.) Do not use gendered pronouns when talking about users/consumers/whatever but always they/their instead. Do not use the future tense but use present simple for expressing general truths instead. Use active voice when there is no specific need to use passive. Abbreviations and acronyms should be spelled out the first time they appear in any technical document with the shortened form appearing in parentheses immediately after the term. The abbreviation or acronym can then be used throughout the document. Avoid ambiguous and abstract language (i.e. really, quite, very), imprecise or subjective terms (i.e. fast, slow, tall, small) and words that have no precise meaning (i.e. bit, thing, stuff). Avoid contractions (i.e. don't, you'll, etc.) as they are meant for informal contexts. Avoid generalized statements, because they are difficult to substantiate and too broad to be supported. Avoid story-telling, remain factual and concise. Avoid jargon and humor. Avoid em-dashes. Putting non-restrictive relative clauses into separate sentences leads to simpler, clearer writing. If em-dashes are needed, make sure to use the right character: \u2014 (alt code: ALT+0151 ). When referring to something in a certain way (i.e. FBAS for Federated Byzantine Agreement System ) make sure to consistently use only FBAS consistently after the term is introduced.","title":"Technical Writing"},{"location":"code/markdown/#links","text":"Use informative link titles. For example, instead of naming your links \"link\" or \"here\", wrap part of the sentence that is meant to be linked as a title. Links to external sources should be: Clear, concise, factual (not tips & tricks-type articles, or blog posts) Reliable to stand the test of time (will not start to 404 because it's a personal blog and the person decided to get rid of it for example) From reliable sources (this is where Wikipedia isn't always perfect, but fine for technical subjects) Whenever possible, use internal links instead of external ones (if something has been described in our documents somewhere, link to it instead of externally)","title":"Links"},{"location":"code/markdown/#specific-to-this-repository","text":"In this repository, Markdown is extended by some mkdocs plugins which allow you to include external files within a markdown files, and to insert admonitions. Admonitions are a way to include side content into a page without significantly interrupting the document flow. There are different types of admonitions, and they allow for the inclusion and nesting of arbitrary content. Including external files can be done by writing the name of a file present in the include directory surrounded by { / } and exclamation marks on an empty line.","title":"Specific to this Repository"},{"location":"code/markdown/#external-resources","text":"Technical Writing Standards Google Markdown Style Guide Mastering GitHub Markdown","title":"External Resources"},{"location":"code/testing/","text":"Testing Guide At Optakt, we consistently write tests to ensure a reliable engineering environment where quality is paramount. Over the course of the product development life cycle, testing saves time and money, and helps developers write better code, more efficiently. Untested code is fragile, difficult to maintain and becomes questionable as soon as changes are made. This guide assumes that you are already familiar with Go testing. Unit Tests This section outlines a few of the rules we try to follow when it comes to Go unit tests. Naming Conventions Unit tests should have consistent names. The best way to go about it is to follow the official guidelines of the Go testing package , which states that: The naming convention to declare tests for the package, a function F , a type T and method M on type T are: func Test () { ... } func TestF () { ... } func TestT () { ... } func TestT_M () { ... } When it comes to subtests, the names of individual subtests should be lowercased and concise. The tests usually start with a subtest called nominal case which verifies that the tested component behaves as expected in a baseline situation, where no failures occur and no edge cases are handled. Internal Unit Tests In most cases, packages can be tested using external tests only. When writing tests for a package called xyz , the external tests should be in the same folder, but in a package called xyz_test . This case is handled by Go natively and will therefore not result in complaints about there being two different packages within the same directory. Of course, there are exceptions. If you need to test some internal logic, those tests must be in a file suffixed with _internal_test.go . Mocks When it comes to mocking dependencies for tests, we prefer to use simple hand-made mocks rather than to use testing frameworks to generate them. The Go language makes it easy to do so elegantly by creating structures that implement the interfaces for dependencies of the tested code and exposing functions that match the interface's signature as attributes that can be overridden externally. package mocks import ( \"testing\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" ) type Loader struct { TrieFunc func () ( * trie . MTrie , error ) } func BaselineLoader ( t * testing . T ) * Loader { t . Helper () l := Loader { TrieFunc : func () ( * trie . MTrie , error ) { return GenericTrie , nil }, } return & l } func ( l * Loader ) Trie () ( * trie . MTrie , error ) { return l . TrieFunc () } Using those mocks is as simple as instantiating a baseline version of the mock and setting its attributes to the desired functions: // ... t . Run ( \"handles failure to load checkpoint\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusEmpty ) load := mocks . BaselineLoader ( t ) load . CheckpointFunc = func () ( * trie . MTrie , error ) { return nil , mocks . GenericError } tr . load = load err := tr . BootstrapState ( st ) assert . Error ( t , err ) }) // ... Pseudorandom Generic Values When using test data for unit tests, it is always a good idea to use random generated data as the inputs. This avoids the bias where a test passes because it is given a valid set of inputs while some other inputs might have highlighted a flaw in the logic, by using an unconstrained data set. In order for the tests to be repeatable and for results to be consistent though, the given inputs should not be completely random, but instead they should be pseudorandom, with the same initial seed, to ensure the same sequence of \"random\" tests. Here is an example of such a value being generated. func GenericAddresses ( number int ) [] flow . Address { // Ensure consistent deterministic results. random := rand . New ( rand . NewSource ( 5 )) var addresses [] flow . Address for i := 0 ; i < number ; i ++ { var address flow . Address binary . BigEndian . PutUint64 ( address [ 0 :], random . Uint64 ()) addresses = append ( addresses , address ) } return addresses } func GenericAddress ( index int ) flow . Address { return GenericAddresses ( index + 1 )[ index ] } Warning While randomly generating valid inputs makes sense, randomly generating invalid inputs does not. In the case of invalid inputs, it is much better to have an exhaustive list of all types of cases that are expected to be invalid and always test each one of them. Parallelization Since the version 1.7 of Go, tests can be run in parallel. This can be done by calling t.Parallel in each subtest. Calling this function signals that the test is to be run in parallel with (and only with) other parallel tests, and the amount of tests running in parallel is limited by the value of runtime.GOMAXPROCS . There are multiple advantages to parallelizing tests: It ensures that regardless of the order in which inputs are given, components behave as expected. It maximizes performance, which in turns results in a faster CI and a faster workflow for everyone, which allows us to write more tests and therefore produce more actionable data to find bugs as well as improve tests cases and coverage. It makes it possible to ensure that the components you test are concurrency-safe Parallelizing table-driven tests When it comes to table-driven tests, a common pitfall developers fall into is to call t.Parallel in their subtest without capturing the loop variable with their test case. Here is an example of how it should be done: func TestGroupedParallel ( t * testing . T ) { for _ , tc := range tests { tc := tc // capture range variable t . Run ( tc . Name , func ( t * testing . T ) { t . Parallel () ... }) } } Standard testing Package The standard testing package is very powerful, and does not require additional frameworks to be used efficiently. The only exception we make to that are the stretchr/testify/assert and stretchr/testify/require packages which we use only for convenience, as they expose assertion functions that produce consistent outputs and make tests easy to understand. Subtests and Sub-benchmarks The testing package exposes a Run method on the T type which makes it possible to nest tests within tests. This can be very useful, as it enables creating a hierarchical structure within a test. index_internal_test.go func TestIndex ( t * testing . T ) { // ... t . Run ( \"collections\" , func ( t * testing . T ) { t . Parallel () collections := mocks . GenericCollections ( 4 ) reader , writer , db := setupIndex ( t ) defer db . Close () assert . NoError ( t , writer . Collections ( mocks . GenericHeight , collections )) // Close the writer to make it commit its transactions. require . NoError ( t , writer . Close ()) // NOTE: The following subtests should NOT be run in parallel, because of the deferral // to close the database above. t . Run ( \"retrieve collection by ID\" , func ( t * testing . T ) { got , err := reader . Collection ( collections [ 0 ]. ID ()) require . NoError ( t , err ) assert . Equal ( t , collections [ 0 ], got ) }) t . Run ( \"retrieve collections by height\" , func ( t * testing . T ) { got , err := reader . CollectionsByHeight ( mocks . GenericHeight ) require . NoError ( t , err ) assert . ElementsMatch ( t , mocks . GenericCollectionIDs ( 4 ), got ) }) t . Run ( \"retrieve transactions from collection\" , func ( t * testing . T ) { // For now this index is not used. }) }) // ... } Table-Driven Tests It makes a lot of sense to use subtests when testing the behavior of complex components, but it is better to use table-driven tests when testing a simple function with an expected output for a given input, and that there are many cases to cover. For such cases, table-driven tests massively improve clarity and readability. They should not be used blindly for all tests, however. In cases where the tested component is complex and that testing its methods cannot be simplified to a common setup, call to a function and assertion of the output, trying to use table-driven tests at all costs might lead to messy code, where the subtest which runs the test case is full of conditions to try to handle each separate setup. This is usually a sign that using simple tests and subtests would be a better approach. Case Study: The Flow DPS Mapper Sometimes, a core piece of software might seem impossible to test. That was the case for the mapper component in Flow DPS at some point, where its main function consisted of a 453-lines-long loop which orchestrated the use of all the other components of the application. mapper_old.go package mapper import ( \"bytes\" \"context\" \"errors\" \"fmt\" \"os\" \"sort\" \"sync\" \"github.com/gammazero/deque\" \"github.com/rs/zerolog\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/flattener\" \"github.com/onflow/flow-go/ledger/complete/mtrie/node\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/ledger/complete/wal\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" \"github.com/optakt/flow-dps/models/index\" ) type Mapper struct { log zerolog . Logger cfg Config chain Chain feed Feeder index index . Writer wg * sync . WaitGroup stop chan struct {} } // New creates a new mapper that uses chain data to map trie updates to blocks // and then passes on the details to the indexer for indexing. func New ( log zerolog . Logger , chain Chain , feed Feeder , index index . Writer , options ... func ( * Config )) ( * Mapper , error ) { // We don't use a checkpoint by default. The options can set one, in which // case we will add the checkpoint as a finalized state commitment in our // trie registry. cfg := Config { CheckpointFile : \"\" , PostProcessing : PostNoop , } for _ , option := range options { option ( & cfg ) } // Check if the checkpoint file exists. if cfg . CheckpointFile != \"\" { stat , err := os . Stat ( cfg . CheckpointFile ) if err != nil { return nil , fmt . Errorf ( \"invalid checkpoint file: %w\" , err ) } if stat . IsDir () { return nil , fmt . Errorf ( \"invalid checkpoint file: directory\" ) } } i := Mapper { log : log , chain : chain , feed : feed , index : index , cfg : cfg , wg : & sync . WaitGroup {}, stop : make ( chan struct {}), } return & i , nil } func ( m * Mapper ) Stop ( ctx context . Context ) error { close ( m . stop ) done := make ( chan struct {}) go func () { m . wg . Wait () close ( done ) }() select { case <- ctx . Done (): return ctx . Err () case <- done : return nil } } // NOTE: We might want to move height and tree (checkpoint) to parameters of the // run function; that would make it quite easy to resume from an arbitrary // point in the LedgerWAL and get rid of the related struct fields. func ( m * Mapper ) Run () error { m . wg . Add ( 1 ) defer m . wg . Done () // We start trying to map at the root height. height , err := m . chain . Root () if err != nil { return fmt . Errorf ( \"could not get root height: %w\" , err ) } // We always initialize an empty state trie to refer to the first step // before the checkpoint. If there is no checkpoint, then the step after the // checkpoint will also just be the empty trie. Otherwise, the second trie // will load the checkpoint trie. empty := trie . NewEmptyMTrie () var tree * trie . MTrie if m . cfg . CheckpointFile == \"\" { tree = empty } else { m . log . Info (). Msg ( \"checkpoint rebuild started\" ) file , err := os . Open ( m . cfg . CheckpointFile ) if err != nil { return fmt . Errorf ( \"could not open checkpoint file: %w\" , err ) } checkpoint , err := wal . ReadCheckpoint ( file ) if err != nil { return fmt . Errorf ( \"could not read checkpoint: %w\" , err ) } trees , err := flattener . RebuildTries ( checkpoint ) if err != nil { return fmt . Errorf ( \"could not rebuild tries: %w\" , err ) } if len ( trees ) != 1 { return fmt . Errorf ( \"should only have one trie in root checkpoint (tries: %d)\" , len ( trees )) } tree = trees [ 0 ] m . log . Info (). Msg ( \"checkpoint rebuild finished\" ) } m . log . Info (). Msg ( \"path collection started\" ) // We have to index all of the paths from the checkpoint; otherwise, we will // miss every single one of the bootstrapped registers. paths := make ([] ledger . Path , 0 , len ( tree . AllPayloads ())) queue := deque . New () root := tree . RootNode () if root != nil { queue . PushBack ( root ) } for queue . Len () > 0 { node := queue . PopBack ().( * node . Node ) if node . IsLeaf () { path := node . Path () paths = append ( paths , * path ) continue } if node . LeftChild () != nil { queue . PushBack ( node . LeftChild ()) } if node . RightChild () != nil { queue . PushBack ( node . RightChild ()) } } m . log . Info (). Int ( \"paths\" , len ( paths )). Msg ( \"path collection finished\" ) m . log . Info (). Msg ( \"path sorting started\" ) sort . Slice ( paths , func ( i int , j int ) bool { return bytes . Compare ( paths [ i ][:], paths [ j ][:]) < 0 }) m . log . Info (). Msg ( \"path sorting finished\" ) // When trying to go from one finalized block to the next, we keep a list // of intermediary tries until the full set of transitions have been // identified. We keep track of these transitions as steps in this map. steps := make ( map [ flow . StateCommitment ] * Step ) // We start at an \"imaginary\" step that refers to an empty trie, has no // paths and no previous commit. We consider this step already done, so it // will never be indexed; it's merely used as the sentinel value for // stopping when we index the first block. It also makes sure that we don't // return a `nil` trie if we abort indexing before the first block is done. emptyCommit := flow . DummyStateCommitment steps [ emptyCommit ] = & Step { Commit : flow . StateCommitment {}, Paths : nil , Tree : empty , } // We then add a second step that refers to the first step that is already // done, which uses the commit of the initial state trie after the // checkpoint has been loaded, and contains all of the paths found in the // initial checkpoint state trie. This will make sure that we index all the // data from the checkpoint as part of the first block. rootCommit := flow . StateCommitment ( tree . RootHash ()) steps [ rootCommit ] = & Step { Commit : emptyCommit , Paths : paths , Tree : tree , } // This is how we let the indexing loop know that the first \"imaginary\" step // was already indexed. The `commitPrev` value is used as a sentinel value // for when to stop going backwards through the steps when indexing a block. // This means the value is always set to the last already indexed step. commitPrev := emptyCommit m . log . Info (). Msg ( \"state indexing started\" ) // Next, we launch into the loop that is responsible for mapping all // incoming trie updates to a block. The loop itself has no concept of what // the next state commitment is that we should look at. It will simply try // to find a previous step for _any_ trie update that comes in. This means // that the first trie update needs to either apply to the empty trie or to // the trie after the checkpoint in order to be processed. once := & sync . Once {} Outer : for { // We want to check in this tight loop if we want to quit, just in case // we get stuck on a timed out network connection. select { case <- m . stop : break Outer default : // keep going } log := m . log . With (). Uint64 ( \"height\" , height ). Hex ( \"commit_prev\" , commitPrev [:]). Logger () // As a first step, we retrieve the state commitment of the finalized // block at the current height; we start at the root height and then // increase it each time we are done indexing a block. Once an applied // trie update gives us a state trie with the same root hash as // `commitNext`, we have reached the end state of the next finalized // block and can index all steps in-between for that block height. commitNext , err := m . chain . Commit ( height ) // If the retrieval times out, it's possible that we are on a live chain // and the next block has not been finalized yet. We should thus simply // retry until we have a new block. if errors . Is ( err , dps . ErrTimeout ) { log . Warn (). Msg ( \"commit retrieval timed out, retrying\" ) continue Outer } // If we have reached the end of the finalized blocks, we are probably // on a historical chain and there are no more finalized blocks for the // related spork. We can exit without error. if errors . Is ( err , dps . ErrFinished ) { log . Debug (). Msg ( \"reached end of finalized chain\" ) break Outer } // Any other error should not happen and should crash explicitly. if err != nil { return fmt . Errorf ( \"could not retrieve next commit (height: %d): %w\" , height , err ) } log = log . With (). Hex ( \"commit_next\" , commitNext [:]). Logger () Inner : for { // We want to check in this tight loop if we want to quit, just in case // we get stuck on a timed out network connection. select { case <- m . stop : break Outer default : // keep going } // When we have the state commitment of the next finalized block, we // check to see if we find a trie for it in our steps. If we do, it // means that we have steps from the last finalized block to the // finalized block at the current height. This condition will // trigger immediately for every empty block. _ , ok := steps [ commitNext ] if ok { break Inner } // If we don't find a trie for the current state commitment, we need // to keep applying trie updates to state tries until one of them // does have the correct commit. We simply feed the next trie update // here. update , err := m . feed . Update () // Once more, we might be on a live spork and the next delta might not // be available yet. In that case, keep trying. if errors . Is ( err , dps . ErrTimeout ) { log . Warn (). Msg ( \"delta retrieval timed out, retrying\" ) continue Inner } // Similarly, if no more deltas are available, we reached the end of // the WAL and we are done reconstructing the execution state. if errors . Is ( err , dps . ErrFinished ) { log . Debug (). Msg ( \"reached end of delta log\" ) break Outer } // Other errors should fail execution as they should not happen. if err != nil { return fmt . Errorf ( \"could not retrieve next delta: %w\" , err ) } // NOTE: We used to require a copy of the `RootHash` here, when it // was still a byte slice, as the underlying slice was being reused. // It was changed to a value type that is always copied now. commitBefore := flow . StateCommitment ( update . RootHash ) log := log . With (). Hex ( \"commit_before\" , commitBefore [:]). Logger () // Once we have our new update and know which trie it should be // applied to, we check to see if we have such a trie in our current // steps. If not, we can simply skip it; this can happen, for // example, when there is an execution fork and the trie update // applies to an obsolete part of the blockchain history. step , ok := steps [ commitBefore ] if ! ok { log . Debug (). Msg ( \"skipping trie update without matching trie\" ) continue Inner } // We de-duplicate the paths and payloads here. This replicates some // code that is part of the execution node and has moved between // different layers of the architecture. We keep it to be safe for // all versions of the Flow dependencies. // NOTE: Past versions of this code required paths to be copied, // because the underlying slice was being re-used. In contrary, // deep-copying payloads was a bad idea, because they were already // being copied by the trie insertion code, and it would have led to // twice the memory usage. paths = make ([] ledger . Path , 0 , len ( update . Paths )) lookup := make ( map [ ledger . Path ] * ledger . Payload ) for i , path := range update . Paths { _ , ok := lookup [ path ] if ! ok { paths = append ( paths , path ) } lookup [ path ] = update . Payloads [ i ] } sort . Slice ( paths , func ( i , j int ) bool { return bytes . Compare ( paths [ i ][:], paths [ j ][:]) < 0 }) payloads := make ([] ledger . Payload , 0 , len ( paths )) for _ , path := range paths { payloads = append ( payloads , * lookup [ path ]) } // We can now apply the trie update to the state trie as it was at // the previous step. This is where the trie code will deep-copy the // payloads. // NOTE: It's important that we don't shadow the variable here, // otherwise the root trie will never go out of scope and we will // never garbage collect any of the root trie payloads that have // been replaced by subsequent trie updates. tree , err = trie . NewTrieWithUpdatedRegisters ( step . Tree , paths , payloads ) if err != nil { return fmt . Errorf ( \"could not update trie: %w\" , err ) } // We then store the new trie along with the state commitment of its // parent and the paths that were changed. This will make it // available for subsequent trie updates to be applied to it, and it // will also allow us to reconstruct the payloads changed in this // step by retrieving them directly from the trie with the given // paths. commitAfter := flow . StateCommitment ( tree . RootHash ()) step = & Step { Commit : commitBefore , Paths : paths , Tree : tree , } steps [ commitAfter ] = step log . Debug (). Hex ( \"commit_after\" , commitAfter [:]). Msg ( \"trie update applied\" ) } // At this point we have identified a step that has lead to the state // commitment of the finalized block at the current height. We can // retrieve some additional indexing data, such as the block header and // the events that resulted from transactions in the block. header , err := m . chain . Header ( height ) if err != nil { return fmt . Errorf ( \"could not retrieve header: %w (height: %d)\" , err , height ) } events , err := m . chain . Events ( height ) if err != nil { return fmt . Errorf ( \"could not retrieve events: %w (height: %d)\" , err , height ) } transactions , err := m . chain . Transactions ( height ) if err != nil { return fmt . Errorf ( \"could not retrieve transactions: %w (height: %d)\" , err , height ) } collections , err := m . chain . Collections ( height ) if err != nil { return fmt . Errorf ( \"could not retrieve collections: %w (height: %d)\" , err , height ) } blockID := header . ID () // TODO: Refactor the mapper in https://github.com/optakt/flow-dps/issues/128 // and replace naive if statements around indexing. // We then index the data for the finalized block at the current height. if m . cfg . IndexHeaders { err = m . index . Header ( height , header ) if err != nil { return fmt . Errorf ( \"could not index header: %w\" , err ) } } if m . cfg . IndexCommit { err = m . index . Commit ( height , commitNext ) if err != nil { return fmt . Errorf ( \"could not index commit: %w\" , err ) } } if m . cfg . IndexEvents { err = m . index . Events ( height , events ) if err != nil { return fmt . Errorf ( \"could not index events: %w\" , err ) } } if m . cfg . IndexBlocks { err = m . index . Height ( blockID , height ) if err != nil { return fmt . Errorf ( \"could not index block heights: %w\" , err ) } } if m . cfg . IndexTransactions { err = m . index . Transactions ( blockID , collections , transactions ) if err != nil { return fmt . Errorf ( \"could not index transactions: %w\" , err ) } } // In order to index the payloads, we step back from the state // commitment of the finalized block at the current height to the state // commitment of the last finalized block that was indexed. For each // step, we collect all the payloads by using the paths for the step and // index them as we go. // NOTE: We keep track of the paths for which we already indexed // payloads, so we can skip them in earlier steps. One inherent benefit // of stepping from the last step to the first step is that this will // automatically use only the latest update of a register, which is // exactly what we want. commit := commitNext updated := make ( map [ ledger . Path ] struct {}) for commit != commitPrev { // In the first part, we get the step we are currently at and filter // out any paths that have already been updated. step := steps [ commit ] paths := make ([] ledger . Path , 0 , len ( step . Paths )) for _ , path := range step . Paths { _ , ok := updated [ path ] if ok { continue } paths = append ( paths , path ) updated [ path ] = struct {}{} } if ! m . cfg . IndexPayloads { commit = step . Commit continue } // We then divide the remaining paths into chunks of 1000. For each // batch, we retrieve the payloads from the state trie as it was at // the end of this block and index them. count := 0 n := 1000 total := ( len ( paths ) + n - 1 ) / n log . Debug (). Int ( \"num_paths\" , len ( paths )). Int ( \"num_batches\" , total ). Msg ( \"path batching executed\" ) for start := 0 ; start < len ( paths ); start += n { // This loop may take a while, especially for the root checkpoint // updates, so check if we should quit. select { case <- m . stop : break Outer default : // keep going } end := start + n if end > len ( paths ) { end = len ( paths ) } batch := paths [ start : end ] payloads := step . Tree . UnsafeRead ( batch ) err = m . index . Payloads ( height , batch , payloads ) if err != nil { return fmt . Errorf ( \"could not index payloads: %w\" , err ) } count ++ log . Debug (). Int ( \"batch\" , count ). Int ( \"start\" , start ). Int ( \"end\" , end ). Msg ( \"path batch indexed\" ) } // Finally, we forward the commit to the previous trie update and // repeat until we have stepped all the way back to the last indexed // commit. commit = step . Commit } // At this point, we can delete any trie that does not correspond to // the state that we have just reached. This will allow the garbage // collector to free up any payload that has been changed and which is // no longer part of the state trie at the newly indexed finalized // block. for key := range steps { if key != commitNext { delete ( steps , key ) } } // Last but not least, we take care of properly indexing the height of // the first indexed block and the height of the last indexed block. once . Do ( func () { err = m . index . First ( height ) }) if err != nil { return fmt . Errorf ( \"could not index first height: %w\" , err ) } err = m . index . Last ( height ) if err != nil { return fmt . Errorf ( \"could not index last height: %w\" , err ) } // We have now successfully indexed all state trie changes and other // data at the current height. We set the last indexed step to the last // step from our current height, and then increase the height to start // the indexing of the next block. commitPrev = commitNext height ++ log . Info (). Hex ( \"block\" , blockID [:]). Int ( \"num_changes\" , len ( updated )). Int ( \"num_events\" , len ( events )). Msg ( \"block data indexed\" ) } m . log . Info (). Msg ( \"state indexing finished\" ) step := steps [ commitPrev ] m . cfg . PostProcessing ( step . Tree ) return nil } As it was, this code was untestable. Covering each possible case from this huge piece of logic would have required immense, complex, unreadable tests, that would break whenever a piece of this logic would change, and this would require a huge amount of maintenance effort. To solve that massive problem, we refactored our original mapper into a finite-state machine which replicates the same computation logic by applying transitions to a state. mapper_new.go package mapper import ( \"errors\" \"fmt\" \"sync\" \"time\" \"github.com/rs/zerolog\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" ) // TransitionFunc is a function that is applied onto the state machine's // state. type TransitionFunc func ( * State ) error // Transitions is what applies transitions to the state of an FSM. type Transitions struct { cfg Config log zerolog . Logger load Loader chain dps . Chain feed Feeder read dps . Reader write dps . Writer once * sync . Once } // NewTransitions returns a Transitions component using the given dependencies and using the given options func NewTransitions ( log zerolog . Logger , load Loader , chain dps . Chain , feed Feeder , read dps . Reader , write dps . Writer , options ... Option ) * Transitions { cfg := DefaultConfig for _ , option := range options { option ( & cfg ) } t := Transitions { log : log . With (). Str ( \"component\" , \"mapper_transitions\" ). Logger (), cfg : cfg , load : load , chain : chain , feed : feed , read : read , write : write , once : & sync . Once {}, } return & t } // InitializeMapper initializes the mapper by either going into bootstrapping or // into resuming, depending on the configuration. func ( t * Transitions ) InitializeMapper ( s * State ) error { if s . status != StatusInitialize { return fmt . Errorf ( \"invalid status for initializing mapper (%s)\" , s . status ) } if t . cfg . BootstrapState { s . status = StatusBootstrap return nil } s . status = StatusResume return nil } // BootstrapState bootstraps the state by loading the checkpoint if there is one // and initializing the elements subsequently used by the FSM. func ( t * Transitions ) BootstrapState ( s * State ) error { if s . status != StatusBootstrap { return fmt . Errorf ( \"invalid status for bootstrapping state (%s)\" , s . status ) } // We always need at least one step in our forest, which is used as the // stopping point when indexing the payloads since the last finalized // block. We thus introduce an empty tree, with no paths and an // irrelevant previous commit. empty := trie . NewEmptyMTrie () s . forest . Save ( empty , nil , flow . DummyStateCommitment ) // The chain indexing will forward last to next and next to current height, // which will be the one for the checkpoint. first := flow . StateCommitment ( empty . RootHash ()) s . last = flow . DummyStateCommitment s . next = first t . log . Info (). Hex ( \"commit\" , first [:]). Msg ( \"added empty tree to forest\" ) // Then, we can load the root height and apply it to the state. That // will allow us to load the root blockchain data in the next step. height , err := t . chain . Root () if err != nil { return fmt . Errorf ( \"could not get root height: %w\" , err ) } s . height = height // When bootstrapping, the loader injected into the mapper loads the root // checkpoint. tree , err := t . load . Trie () if err != nil { return fmt . Errorf ( \"could not load root trie: %w\" , err ) } paths := allPaths ( tree ) s . forest . Save ( tree , paths , first ) second := tree . RootHash () t . log . Info (). Uint64 ( \"height\" , s . height ). Hex ( \"commit\" , second [:]). Int ( \"registers\" , len ( paths )). Msg ( \"added checkpoint tree to forest\" ) // We have successfully bootstrapped. However, no chain data for the root // block has been indexed yet. This is why we \"pretend\" that we just // forwarded the state to this height, so we go straight to the chain data // indexing. s . status = StatusIndex return nil } // ResumeIndexing resumes indexing the data from a previous run. func ( t * Transitions ) ResumeIndexing ( s * State ) error { if s . status != StatusResume { return fmt . Errorf ( \"invalid status for resuming indexing (%s)\" , s . status ) } // When resuming, we want to avoid overwriting the `first` height in the // index with the height we are resuming from. Theoretically, all that would // be needed would be to execute a no-op on `once`, which would subsequently // be skipped in the height forwarding code. However, this bug was already // released, so we have databases where `first` was incorrectly set to the // height we resume from. In order to fix them, we explicitly write the // correct `first` height here again, while at the same time using `once` to // disable any subsequent attempts to write it. first , err := t . chain . Root () if err != nil { return fmt . Errorf ( \"could not get root height: %w\" , err ) } t . once . Do ( func () { err = t . write . First ( first ) }) if err != nil { return fmt . Errorf ( \"could not write first: %w\" , err ) } // We need to know what the last indexed height was at the point we stopped // indexing. last , err := t . read . Last () if err != nil { return fmt . Errorf ( \"could not get last height: %w\" , err ) } // When resuming, the loader injected into the mapper rebuilds the trie from // the paths and payloads stored in the index database. tree , err := t . load . Trie () if err != nil { return fmt . Errorf ( \"could not restore index trie: %w\" , err ) } // After loading the trie, we should do a sanity check on its hash against // the commit we indexed for it. hash := flow . StateCommitment ( tree . RootHash ()) commit , err := t . read . Commit ( last ) if err != nil { return fmt . Errorf ( \"could not get last commit: %w\" , err ) } if hash != commit { return fmt . Errorf ( \"restored trie hash does not match last commit (hash: %x, commit: %x)\" , hash , commit ) } // At this point, we can store the restored trie in our forest, as the trie // for the last finalized block. We do not need to care about the parent // state commitment or the paths, as they should not be used. s . last = flow . DummyStateCommitment s . next = commit s . forest . Save ( tree , nil , flow . DummyStateCommitment ) // Lastly, we just need to point to the next height. The chain indexing will // then proceed with the first non-indexed block and forward the state // commitments accordingly. s . height = last + 1 // At this point, we should be able to start indexing the chain data for // the next height. s . status = StatusIndex return nil } // IndexChain indexes chain data for the current height. func ( t * Transitions ) IndexChain ( s * State ) error { if s . status != StatusIndex { return fmt . Errorf ( \"invalid status for indexing chain (%s)\" , s . status ) } log := t . log . With (). Uint64 ( \"height\" , s . height ). Logger () // We try to retrieve the next header until it becomes available, which // means all data coming from the protocol state is available after this // point. header , err := t . chain . Header ( s . height ) if errors . Is ( err , dps . ErrUnavailable ) { log . Debug (). Msg ( \"waiting for next header\" ) time . Sleep ( t . cfg . WaitInterval ) return nil } if err != nil { return fmt . Errorf ( \"could not get header: %w\" , err ) } // At this point, we can retrieve the data from the consensus state. This is // a slight optimization for the live indexer, as it allows us to process // some data before the full execution data becomes available. guarantees , err := t . chain . Guarantees ( s . height ) if err != nil { return fmt . Errorf ( \"could not get guarantees: %w\" , err ) } seals , err := t . chain . Seals ( s . height ) if err != nil { return fmt . Errorf ( \"could not get seals: %w\" , err ) } // We can also proceed to already indexing the data related to the consensus // state, before dealing with anything related to execution data, which // might go into the wait state. blockID := header . ID () err = t . write . Height ( blockID , s . height ) if err != nil { return fmt . Errorf ( \"could not index height: %w\" , err ) } err = t . write . Header ( s . height , header ) if err != nil { return fmt . Errorf ( \"could not index header: %w\" , err ) } err = t . write . Guarantees ( s . height , guarantees ) if err != nil { return fmt . Errorf ( \"could not index guarantees: %w\" , err ) } err = t . write . Seals ( s . height , seals ) if err != nil { return fmt . Errorf ( \"could not index seals: %w\" , err ) } // Next, we try to retrieve the next commit until it becomes available, // at which point all the data coming from the execution data should be // available. commit , err := t . chain . Commit ( s . height ) if errors . Is ( err , dps . ErrUnavailable ) { log . Debug (). Msg ( \"waiting for next state commitment\" ) time . Sleep ( t . cfg . WaitInterval ) return nil } if err != nil { return fmt . Errorf ( \"could not get commit: %w\" , err ) } collections , err := t . chain . Collections ( s . height ) if err != nil { return fmt . Errorf ( \"could not get collections: %w\" , err ) } transactions , err := t . chain . Transactions ( s . height ) if err != nil { return fmt . Errorf ( \"could not get transactions: %w\" , err ) } results , err := t . chain . Results ( s . height ) if err != nil { return fmt . Errorf ( \"could not get transaction results: %w\" , err ) } events , err := t . chain . Events ( s . height ) if err != nil { return fmt . Errorf ( \"could not get events: %w\" , err ) } // Next, all we need to do is index the remaining data and we have fully // processed indexing for this block height. err = t . write . Commit ( s . height , commit ) if err != nil { return fmt . Errorf ( \"could not index commit: %w\" , err ) } err = t . write . Collections ( s . height , collections ) if err != nil { return fmt . Errorf ( \"could not index collections: %w\" , err ) } err = t . write . Transactions ( s . height , transactions ) if err != nil { return fmt . Errorf ( \"could not index transactions: %w\" , err ) } err = t . write . Results ( results ) if err != nil { return fmt . Errorf ( \"could not index transaction results: %w\" , err ) } err = t . write . Events ( s . height , events ) if err != nil { return fmt . Errorf ( \"could not index events: %w\" , err ) } // At this point, we need to forward the `last` state commitment to // `next`, so we know what the state commitment was at the last finalized // block we processed. This will allow us to know when to stop when // walking back through the forest to collect trie updates. s . last = s . next // Last but not least, we need to update `next` to point to the commit we // have just retrieved for the new block height. This is the sentinel that // tells us when we have collected enough trie updates for the forest to // have reached the next finalized block. s . next = commit log . Info (). Msg ( \"indexed blockchain data for finalized block\" ) // After indexing the blockchain data, we can go back to updating the state // tree until we find the commit of the finalized block. This will allow us // to index the payloads then. s . status = StatusUpdate return nil } // UpdateTree updates the state's tree. If the state's forest already matches with the next block's state commitment, // it immediately returns and sets the state's status to StatusMatched. func ( t * Transitions ) UpdateTree ( s * State ) error { if s . status != StatusUpdate { return fmt . Errorf ( \"invalid status for updating tree (%s)\" , s . status ) } log := t . log . With (). Uint64 ( \"height\" , s . height ). Hex ( \"last\" , s . last [:]). Hex ( \"next\" , s . next [:]). Logger () // If the forest contains a tree for the commit of the next finalized block, // we have reached our goal, and we can go to the next step in order to // collect the register payloads we want to index for that block. ok := s . forest . Has ( s . next ) if ok { log . Info (). Hex ( \"commit\" , s . next [:]). Msg ( \"matched commit of finalized block\" ) s . status = StatusCollect return nil } // First, we get the next tree update from the feeder. We can skip it if // it doesn't have any updated paths, or if we can't find the tree to apply // it to in the forest. This usually means that it was meant for a pruned // branch of the execution forest. update , err := t . feed . Update () if errors . Is ( err , dps . ErrUnavailable ) { time . Sleep ( t . cfg . WaitInterval ) log . Debug (). Msg ( \"waiting for next trie update\" ) return nil } if err != nil { return fmt . Errorf ( \"could not feed update: %w\" , err ) } parent := flow . StateCommitment ( update . RootHash ) tree , ok := s . forest . Tree ( parent ) if ! ok { log . Warn (). Msg ( \"state commitment mismatch, retrieving next trie update\" ) return nil } // We then apply the update to the relevant tree, as retrieved from the // forest, and save the updated tree in the forest. If the tree is not new, // we should error, as that should not happen. paths , payloads := pathsPayloads ( update ) tree , err = trie . NewTrieWithUpdatedRegisters ( tree , paths , payloads ) if err != nil { return fmt . Errorf ( \"could not update tree: %w\" , err ) } s . forest . Save ( tree , paths , parent ) hash := tree . RootHash () log . Info (). Hex ( \"commit\" , hash [:]). Int ( \"registers\" , len ( paths )). Msg ( \"updated tree with register payloads\" ) return nil } // CollectRegisters reads the payloads for the next block to be indexed from the state's forest, unless payload // indexing is disabled. func ( t * Transitions ) CollectRegisters ( s * State ) error { log := t . log . With (). Uint64 ( \"height\" , s . height ). Hex ( \"commit\" , s . next [:]). Logger () if s . status != StatusCollect { return fmt . Errorf ( \"invalid status for collecting registers (%s)\" , s . status ) } // If indexing payloads is disabled, we can bypass collection and indexing // of payloads and just go straight to forwarding the height to the next // finalized block. if t . cfg . SkipRegisters { s . status = StatusForward return nil } // If we index payloads, we are basically stepping back from (and including) // the tree that corresponds to the next finalized block all the way up to // (and excluding) the tree for the last finalized block we indexed. To do // so, we will use the parent state commit to retrieve the parent trees from // the forest, and we use the paths we recorded changes on to retrieve the // changed payloads at each step. commit := s . next for commit != s . last { // We do this check only once, so that we don't need to do it for // each item we retrieve. The tree should always be there, but we // should check just to not fail silently. ok := s . forest . Has ( commit ) if ! ok { return fmt . Errorf ( \"could not load tree (commit: %x)\" , commit ) } // For each path, we retrieve the payload and add it to the registers we // will index later. If we already have a payload for the path, it is // more recent as we iterate backwards in time, so we can skip the // outdated payload. // NOTE: We read from the tree one by one here, as the performance // overhead is minimal compared to the disk i/o for badger, and it // allows us to ignore sorting of paths. tree , _ := s . forest . Tree ( commit ) paths , _ := s . forest . Paths ( commit ) for _ , path := range paths { _ , ok := s . registers [ path ] if ok { continue } payloads := tree . UnsafeRead ([] ledger . Path { path }) s . registers [ path ] = payloads [ 0 ] } log . Debug (). Int ( \"batch\" , len ( paths )). Msg ( \"collected register batch for finalized block\" ) // We now step back to the parent of the current state trie. parent , _ := s . forest . Parent ( commit ) commit = parent } log . Info (). Int ( \"registers\" , len ( s . registers )). Msg ( \"collected all registers for finalized block\" ) // At this point, we have collected all the payloads, so we go to the next // step, where we will index them. s . status = StatusMap return nil } // MapRegisters maps the collected registers to the current block. func ( t * Transitions ) MapRegisters ( s * State ) error { if s . status != StatusMap { return fmt . Errorf ( \"invalid status for indexing registers (%s)\" , s . status ) } log := t . log . With (). Uint64 ( \"height\" , s . height ). Hex ( \"commit\" , s . next [:]). Logger () // If there are no registers left to be indexed, we can go to the next step, // which is about forwarding the height to the next finalized block. if len ( s . registers ) == 0 { log . Info (). Msg ( \"indexed all registers for finalized block\" ) s . status = StatusForward return nil } // We will now collect and index 1000 registers at a time. This gives the // FSM the chance to exit the loop between every 1000 payloads we index. It // doesn't really matter for badger if they are in random order, so this // way of iterating should be fine. n := 1000 paths := make ([] ledger . Path , 0 , n ) payloads := make ([] * ledger . Payload , 0 , n ) for path , payload := range s . registers { paths = append ( paths , path ) payloads = append ( payloads , payload ) delete ( s . registers , path ) if len ( paths ) >= n { break } } // Then we store the (maximum) 1000 paths and payloads. err := t . write . Payloads ( s . height , paths , payloads ) if err != nil { return fmt . Errorf ( \"could not index registers: %w\" , err ) } log . Debug (). Int ( \"batch\" , len ( paths )). Int ( \"remaining\" , len ( s . registers )). Msg ( \"indexed register batch for finalized block\" ) return nil } // ForwardHeight increments the height at which the mapping operates, and updates the last indexed height. func ( t * Transitions ) ForwardHeight ( s * State ) error { if s . status != StatusForward { return fmt . Errorf ( \"invalid status for forwarding height (%s)\" , s . status ) } // After finishing the indexing of the payloads for a finalized block, or // skipping it, we should document the last indexed height. On the first // pass, we will also index the first indexed height here. var err error t . once . Do ( func () { err = t . write . First ( s . height ) }) if err != nil { return fmt . Errorf ( \"could not index first height: %w\" , err ) } err = t . write . Last ( s . height ) if err != nil { return fmt . Errorf ( \"could not index last height: %w\" , err ) } // Now that we have indexed the heights, we can forward to the next height, // and reset the forest to free up memory. s . height ++ s . forest . Reset ( s . next ) t . log . Info (). Uint64 ( \"height\" , s . height ). Msg ( \"forwarded finalized block to next height\" ) // Once the height is forwarded, we can set the status so that we index // the blockchain data next. s . status = StatusIndex return nil } This refactoring effort allowed us to write simple and concise tests that call a transition function upon the state machine and make assertions upon the resulting state. mapper_new_internal_test.go package mapper import ( \"sync\" \"testing\" \"github.com/stretchr/testify/assert\" \"github.com/stretchr/testify/require\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" \"github.com/optakt/flow-dps/testing/mocks\" ) func TestNewTransitions ( t * testing . T ) { t . Run ( \"nominal case, without options\" , func ( t * testing . T ) { load := mocks . BaselineLoader ( t ) chain := mocks . BaselineChain ( t ) feed := mocks . BaselineFeeder ( t ) read := mocks . BaselineReader ( t ) write := mocks . BaselineWriter ( t ) tr := NewTransitions ( mocks . NoopLogger , load , chain , feed , read , write ) assert . NotNil ( t , tr ) assert . Equal ( t , chain , tr . chain ) assert . Equal ( t , feed , tr . feed ) assert . Equal ( t , write , tr . write ) assert . NotNil ( t , tr . once ) assert . Equal ( t , DefaultConfig , tr . cfg ) }) t . Run ( \"nominal case, with option\" , func ( t * testing . T ) { load := mocks . BaselineLoader ( t ) chain := mocks . BaselineChain ( t ) feed := mocks . BaselineFeeder ( t ) read := mocks . BaselineReader ( t ) write := mocks . BaselineWriter ( t ) skip := true tr := NewTransitions ( mocks . NoopLogger , load , chain , feed , read , write , WithSkipRegisters ( skip ), ) assert . NotNil ( t , tr ) assert . Equal ( t , chain , tr . chain ) assert . Equal ( t , feed , tr . feed ) assert . Equal ( t , write , tr . write ) assert . NotNil ( t , tr . once ) assert . NotEqual ( t , DefaultConfig , tr . cfg ) assert . Equal ( t , skip , tr . cfg . SkipRegisters ) assert . Equal ( t , DefaultConfig . WaitInterval , tr . cfg . WaitInterval ) }) } func TestTransitions_BootstrapState ( t * testing . T ) { t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) // Copy state in local scope so that we can override its SaveFunc without impacting other // tests running in parallel. var saveCalled bool forest := mocks . BaselineForest ( t , true ) forest . SaveFunc = func ( tree * trie . MTrie , paths [] ledger . Path , parent flow . StateCommitment ) { if ! saveCalled { assert . True ( t , tree . IsEmpty ()) assert . Nil ( t , paths ) assert . Zero ( t , parent ) saveCalled = true return } assert . False ( t , tree . IsEmpty ()) assert . Len ( t , tree . AllPayloads (), len ( paths )) assert . Len ( t , paths , 3 ) // Expect the three paths from leaves. assert . NotZero ( t , parent ) } err := tr . BootstrapState ( st ) assert . NoError ( t , err ) }) t . Run ( \"invalid state\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusForward ) err := tr . BootstrapState ( st ) assert . Error ( t , err ) }) t . Run ( \"handles failure to get root height\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return 0 , mocks . GenericError } tr . chain = chain err := tr . BootstrapState ( st ) assert . Error ( t , err ) }) } func TestTransitions_IndexChain ( t * testing . T ) { t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . HeaderFunc = func ( height uint64 ) ( * flow . Header , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericHeader , nil } chain . CommitFunc = func ( height uint64 ) ( flow . StateCommitment , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericCommit ( 0 ), nil } chain . CollectionsFunc = func ( height uint64 ) ([] * flow . LightCollection , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericCollections ( 2 ), nil } chain . GuaranteesFunc = func ( height uint64 ) ([] * flow . CollectionGuarantee , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericGuarantees ( 2 ), nil } chain . TransactionsFunc = func ( height uint64 ) ([] * flow . TransactionBody , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericTransactions ( 4 ), nil } chain . ResultsFunc = func ( height uint64 ) ([] * flow . TransactionResult , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericResults ( 4 ), nil } chain . EventsFunc = func ( height uint64 ) ([] flow . Event , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericEvents ( 8 ), nil } chain . SealsFunc = func ( height uint64 ) ([] * flow . Seal , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericSeals ( 4 ), nil } write := mocks . BaselineWriter ( t ) write . HeaderFunc = func ( height uint64 , header * flow . Header ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericHeader , header ) return nil } write . CommitFunc = func ( height uint64 , commit flow . StateCommitment ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericCommit ( 0 ), commit ) return nil } write . HeightFunc = func ( blockID flow . Identifier , height uint64 ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericHeader . ID (), blockID ) return nil } write . CollectionsFunc = func ( height uint64 , collections [] * flow . LightCollection ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericCollections ( 2 ), collections ) return nil } write . GuaranteesFunc = func ( height uint64 , guarantees [] * flow . CollectionGuarantee ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericGuarantees ( 2 ), guarantees ) return nil } write . TransactionsFunc = func ( height uint64 , transactions [] * flow . TransactionBody ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericTransactions ( 4 ), transactions ) return nil } write . ResultsFunc = func ( results [] * flow . TransactionResult ) error { assert . Equal ( t , mocks . GenericResults ( 4 ), results ) return nil } write . EventsFunc = func ( height uint64 , events [] flow . Event ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericEvents ( 8 ), events ) return nil } write . SealsFunc = func ( height uint64 , seals [] * flow . Seal ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericSeals ( 4 ), seals ) return nil } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain tr . write = write err := tr . IndexChain ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusUpdate , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve commit\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return flow . DummyStateCommitment , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index commit\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . CommitFunc = func ( uint64 , flow . StateCommitment ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve header\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . HeaderFunc = func ( uint64 ) ( * flow . Header , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index header\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . HeaderFunc = func ( uint64 , * flow . Header ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve transactions\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . TransactionsFunc = func ( uint64 ) ([] * flow . TransactionBody , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve transaction results\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . ResultsFunc = func ( uint64 ) ([] * flow . TransactionResult , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index transactions\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . ResultsFunc = func ([] * flow . TransactionResult ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve collections\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . CollectionsFunc = func ( uint64 ) ([] * flow . LightCollection , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index collections\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . CollectionsFunc = func ( uint64 , [] * flow . LightCollection ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve guarantees\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . GuaranteesFunc = func ( uint64 ) ([] * flow . CollectionGuarantee , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index guarantees\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . GuaranteesFunc = func ( uint64 , [] * flow . CollectionGuarantee ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve events\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . EventsFunc = func ( uint64 ) ([] flow . Event , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index events\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . EventsFunc = func ( uint64 , [] flow . Event ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve seals\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . SealsFunc = func ( uint64 ) ([] * flow . Seal , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index seals\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . SealsFunc = func ( uint64 , [] * flow . Seal ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) } func TestTransitions_UpdateTree ( t * testing . T ) { update := mocks . GenericTrieUpdate ( 0 ) tree := mocks . GenericTrie t . Run ( \"nominal case without match\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusUpdate ) forest := mocks . BaselineForest ( t , false ) forest . SaveFunc = func ( tree * trie . MTrie , paths [] ledger . Path , parent flow . StateCommitment ) { // Parent is RootHash of the mocks.GenericTrie. assert . Equal ( t , update . RootHash [:], parent [:]) assert . ElementsMatch ( t , paths , update . Paths ) assert . NotZero ( t , tree ) } forest . TreeFunc = func ( commit flow . StateCommitment ) ( * trie . MTrie , bool ) { assert . Equal ( t , update . RootHash [:], commit [:]) return tree , true } st . forest = forest err := tr . UpdateTree ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusUpdate , st . status ) }) t . Run ( \"nominal case with no available update temporarily\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusUpdate ) // Set up the mock feeder to return an unavailable error on the first call and return successfully // to subsequent calls. var updateCalled bool feeder := mocks . BaselineFeeder ( t ) feeder . UpdateFunc = func () ( * ledger . TrieUpdate , error ) { if ! updateCalled { updateCalled = true return nil , dps . ErrUnavailable } return mocks . GenericTrieUpdate ( 0 ), nil } tr . feed = feeder forest := mocks . BaselineForest ( t , true ) forest . HasFunc = func ( flow . StateCommitment ) bool { return updateCalled } st . forest = forest // The first call should not error but should not change the status of the FSM to updating. It should // instead remain Updating until a match is found. err := tr . UpdateTree ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusUpdate , st . status ) // The second call is now successful and matches. err = tr . UpdateTree ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusCollect , st . status ) }) t . Run ( \"nominal case with match\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusUpdate ) err := tr . UpdateTree ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusCollect , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) err := tr . UpdateTree ( st ) assert . Error ( t , err ) }) t . Run ( \"handles feeder update failure\" , func ( t * testing . T ) { t . Parallel () feed := mocks . BaselineFeeder ( t ) feed . UpdateFunc = func () ( * ledger . TrieUpdate , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusUpdate ) st . forest = mocks . BaselineForest ( t , false ) tr . feed = feed err := tr . UpdateTree ( st ) assert . Error ( t , err ) }) t . Run ( \"handles forest parent tree not found\" , func ( t * testing . T ) { t . Parallel () forest := mocks . BaselineForest ( t , false ) forest . TreeFunc = func ( _ flow . StateCommitment ) ( * trie . MTrie , bool ) { return nil , false } tr , st := baselineFSM ( t , StatusUpdate ) st . forest = forest err := tr . UpdateTree ( st ) assert . NoError ( t , err ) }) } func TestTransitions_CollectRegisters ( t * testing . T ) { t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () forest := mocks . BaselineForest ( t , true ) forest . ParentFunc = func ( commit flow . StateCommitment ) ( flow . StateCommitment , bool ) { assert . Equal ( t , mocks . GenericCommit ( 0 ), commit ) return mocks . GenericCommit ( 1 ), true } tr , st := baselineFSM ( t , StatusCollect ) st . forest = forest err := tr . CollectRegisters ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusMap , st . status ) for _ , wantPath := range mocks . GenericLedgerPaths ( 6 ) { assert . Contains ( t , st . registers , wantPath ) } }) t . Run ( \"indexing payloads disabled\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusCollect ) tr . cfg . SkipRegisters = true err := tr . CollectRegisters ( st ) require . NoError ( t , err ) assert . Empty ( t , st . registers ) assert . Equal ( t , StatusForward , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) err := tr . CollectRegisters ( st ) assert . Error ( t , err ) assert . Empty ( t , st . registers ) }) t . Run ( \"handles missing tree for commit\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusCollect ) st . forest = mocks . BaselineForest ( t , false ) err := tr . CollectRegisters ( st ) assert . Error ( t , err ) assert . Empty ( t , st . registers ) }) } func TestTransitions_MapRegisters ( t * testing . T ) { t . Run ( \"nominal case with registers to write\" , func ( t * testing . T ) { t . Parallel () // Path 2 and 4 are the same so the map effectively contains 5 entries. testRegisters := map [ ledger . Path ] * ledger . Payload { mocks . GenericLedgerPath ( 0 ): mocks . GenericLedgerPayload ( 0 ), mocks . GenericLedgerPath ( 1 ): mocks . GenericLedgerPayload ( 1 ), mocks . GenericLedgerPath ( 2 ): mocks . GenericLedgerPayload ( 2 ), mocks . GenericLedgerPath ( 1 ): mocks . GenericLedgerPayload ( 3 ), mocks . GenericLedgerPath ( 4 ): mocks . GenericLedgerPayload ( 4 ), mocks . GenericLedgerPath ( 5 ): mocks . GenericLedgerPayload ( 5 ), } write := mocks . BaselineWriter ( t ) write . PayloadsFunc = func ( height uint64 , paths [] ledger . Path , value [] * ledger . Payload ) error { assert . Equal ( t , mocks . GenericHeight , height ) // Expect the 5 entries from the map. assert . Len ( t , paths , 5 ) assert . Len ( t , value , 5 ) return nil } tr , st := baselineFSM ( t , StatusMap ) tr . write = write st . registers = testRegisters err := tr . MapRegisters ( st ) require . NoError ( t , err ) // Should not be StateIndexed because registers map was not empty. assert . Empty ( t , st . registers ) assert . Equal ( t , StatusMap , st . status ) }) t . Run ( \"nominal case no more registers left to write\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusMap ) err := tr . MapRegisters ( st ) assert . NoError ( t , err ) assert . Equal ( t , StatusForward , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () testRegisters := map [ ledger . Path ] * ledger . Payload { mocks . GenericLedgerPath ( 0 ): mocks . GenericLedgerPayload ( 0 ), mocks . GenericLedgerPath ( 1 ): mocks . GenericLedgerPayload ( 1 ), mocks . GenericLedgerPath ( 2 ): mocks . GenericLedgerPayload ( 2 ), mocks . GenericLedgerPath ( 3 ): mocks . GenericLedgerPayload ( 3 ), mocks . GenericLedgerPath ( 4 ): mocks . GenericLedgerPayload ( 4 ), mocks . GenericLedgerPath ( 5 ): mocks . GenericLedgerPayload ( 5 ), } tr , st := baselineFSM ( t , StatusBootstrap ) st . registers = testRegisters err := tr . MapRegisters ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure\" , func ( t * testing . T ) { t . Parallel () testRegisters := map [ ledger . Path ] * ledger . Payload { mocks . GenericLedgerPath ( 0 ): mocks . GenericLedgerPayload ( 0 ), mocks . GenericLedgerPath ( 1 ): mocks . GenericLedgerPayload ( 1 ), mocks . GenericLedgerPath ( 2 ): mocks . GenericLedgerPayload ( 2 ), mocks . GenericLedgerPath ( 3 ): mocks . GenericLedgerPayload ( 3 ), mocks . GenericLedgerPath ( 4 ): mocks . GenericLedgerPayload ( 4 ), mocks . GenericLedgerPath ( 5 ): mocks . GenericLedgerPayload ( 5 ), } write := mocks . BaselineWriter ( t ) write . PayloadsFunc = func ( uint64 , [] ledger . Path , [] * ledger . Payload ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusMap ) tr . write = write st . registers = testRegisters err := tr . MapRegisters ( st ) assert . Error ( t , err ) }) } func TestTransitions_ForwardHeight ( t * testing . T ) { t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () var ( firstCalled int lastCalled int ) write := mocks . BaselineWriter ( t ) write . FirstFunc = func ( height uint64 ) error { assert . Equal ( t , mocks . GenericHeight , height ) firstCalled ++ return nil } write . LastFunc = func ( height uint64 ) error { assert . Equal ( t , mocks . GenericHeight + uint64 ( lastCalled ), height ) lastCalled ++ return nil } forest := mocks . BaselineForest ( t , true ) forest . ResetFunc = func ( finalized flow . StateCommitment ) { assert . Equal ( t , mocks . GenericCommit ( 0 ), finalized ) } tr , st := baselineFSM ( t , StatusForward ) st . forest = forest tr . write = write err := tr . ForwardHeight ( st ) assert . NoError ( t , err ) assert . Equal ( t , StatusIndex , st . status ) assert . Equal ( t , mocks . GenericHeight + 1 , st . height ) // Reset status to allow next call. st . status = StatusForward err = tr . ForwardHeight ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusIndex , st . status ) assert . Equal ( t , mocks . GenericHeight + 2 , st . height ) // First should have been called only once. assert . Equal ( t , 1 , firstCalled ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) err := tr . ForwardHeight ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer error on first\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . FirstFunc = func ( uint64 ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusForward ) tr . write = write err := tr . ForwardHeight ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer error on last\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . LastFunc = func ( uint64 ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusForward ) tr . write = write err := tr . ForwardHeight ( st ) assert . Error ( t , err ) }) } func TestTransitions_InitializeMapper ( t * testing . T ) { t . Run ( \"switches state to BootstrapState if configured to do so\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusInitialize ) tr . cfg . BootstrapState = true err := tr . InitializeMapper ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusBootstrap , st . status ) }) t . Run ( \"switches state to StatusResume if no bootstrapping configured\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusInitialize ) tr . cfg . BootstrapState = false err := tr . InitializeMapper ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusResume , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusForward ) err := tr . InitializeMapper ( st ) require . Error ( t , err ) }) } func TestTransitions_ResumeIndexing ( t * testing . T ) { header := mocks . GenericHeader tree := mocks . GenericTrie commit := flow . StateCommitment ( tree . RootHash ()) differentCommit := mocks . GenericCommit ( 0 ) t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } writer := mocks . BaselineWriter ( t ) writer . FirstFunc = func ( height uint64 ) error { assert . Equal ( t , header . Height , height ) return nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( height uint64 ) ( flow . StateCommitment , error ) { assert . Equal ( t , header . Height , height ) return commit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withWriter ( writer ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusIndex , st . status ) assert . Equal ( t , header . Height + 1 , st . height ) assert . Equal ( t , flow . DummyStateCommitment , st . last ) assert . Equal ( t , commit , st . next ) }) t . Run ( \"handles chain failure on Root\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return 0 , mocks . GenericError } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure on First\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } writer := mocks . BaselineWriter ( t ) writer . FirstFunc = func ( uint64 ) error { return mocks . GenericError } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusResume , withWriter ( writer ), withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles reader failure on Last\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return 0 , mocks . GenericError } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles reader failure on Commit\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return flow . DummyStateCommitment , mocks . GenericError } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles loader failure on Trie\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return nil , mocks . GenericError } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles mismatch between tree root hash and indexed commit\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return differentCommit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusForward , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) } func baselineFSM ( t * testing . T , status Status , opts ... func ( tr * Transitions )) ( * Transitions , * State ) { t . Helper () load := mocks . BaselineLoader ( t ) chain := mocks . BaselineChain ( t ) feeder := mocks . BaselineFeeder ( t ) read := mocks . BaselineReader ( t ) write := mocks . BaselineWriter ( t ) forest := mocks . BaselineForest ( t , true ) once := & sync . Once {} doneCh := make ( chan struct {}) tr := Transitions { cfg : Config { BootstrapState : false , SkipRegisters : false , WaitInterval : 0 , }, log : mocks . NoopLogger , load : load , chain : chain , feed : feeder , read : read , write : write , once : once , } for _ , opt := range opts { opt ( & tr ) } st := State { forest : forest , status : status , height : mocks . GenericHeight , last : mocks . GenericCommit ( 1 ), next : mocks . GenericCommit ( 0 ), registers : make ( map [ ledger . Path ] * ledger . Payload ), done : doneCh , } return & tr , & st } func withLoader ( load Loader ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . load = load } } func withChain ( chain dps . Chain ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . chain = chain } } func withFeeder ( feed Feeder ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . feed = feed } } func withReader ( read dps . Reader ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . read = read } } func withWriter ( write dps . Writer ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . write = write } } Integration Tests Integration tests are essential to ensure that components work together as expected. Those tests are usually much heavier and slower than unit tests, since they use real components instead of simple mocks, and often might run filesystem or network operations, wait for things to happen, or even run heavy computational tasks. Integration tests should always be specified in a separate test package and never run internally within the tested package. Build Tag Because integration tests are inherently slower than unit tests, they are placed in specific files that are suffixed with _integration_test.go and those files start with a build tag directive which prevents them from running unless the go test command is called with the integration tag. Both syntaxes should be specified, the <go1.17 one which is +build <tag> as well as the >=go1.17 one which is go:build <tag> . The former will be dropped when we feel like it is no longer relevant to support go 1.16 and prior. //go:build integration // +build integration package dps_test Examples In Go, good package documentation includes not only comments for each public type and method, but also runnable examples and benchmarks in some cases. Godoc allows defining examples which are verified by running them as tests and can be manually launched by readers of the documentation on the package's Godoc webpage. As for typical tests, examples are functions that reside in a package's _test.go files. Unlike normal test functions, though, example functions take no arguments and begin with the word Example instead of Test . In order to specify what is the expected output of a given example, a comment has to be written at the end of the Example function, in the form of // Output: <expected output> . If this is missing, examples will not be executed and therefore not included in the documentation. Benchmarks When a package exposes a performance-critical piece of code, it should be benchmarked, and benchmark tests must be available for anyone to reproduce the benchmark using their hardware. Writing benchmark results in a markdown file without providing a way to reproduce them is irrelevant.","title":"Testing"},{"location":"code/testing/#testing-guide","text":"At Optakt, we consistently write tests to ensure a reliable engineering environment where quality is paramount. Over the course of the product development life cycle, testing saves time and money, and helps developers write better code, more efficiently. Untested code is fragile, difficult to maintain and becomes questionable as soon as changes are made. This guide assumes that you are already familiar with Go testing.","title":"Testing Guide"},{"location":"code/testing/#unit-tests","text":"This section outlines a few of the rules we try to follow when it comes to Go unit tests.","title":"Unit Tests"},{"location":"code/testing/#naming-conventions","text":"Unit tests should have consistent names. The best way to go about it is to follow the official guidelines of the Go testing package , which states that: The naming convention to declare tests for the package, a function F , a type T and method M on type T are: func Test () { ... } func TestF () { ... } func TestT () { ... } func TestT_M () { ... } When it comes to subtests, the names of individual subtests should be lowercased and concise. The tests usually start with a subtest called nominal case which verifies that the tested component behaves as expected in a baseline situation, where no failures occur and no edge cases are handled.","title":"Naming Conventions"},{"location":"code/testing/#internal-unit-tests","text":"In most cases, packages can be tested using external tests only. When writing tests for a package called xyz , the external tests should be in the same folder, but in a package called xyz_test . This case is handled by Go natively and will therefore not result in complaints about there being two different packages within the same directory. Of course, there are exceptions. If you need to test some internal logic, those tests must be in a file suffixed with _internal_test.go .","title":"Internal Unit Tests"},{"location":"code/testing/#mocks","text":"When it comes to mocking dependencies for tests, we prefer to use simple hand-made mocks rather than to use testing frameworks to generate them. The Go language makes it easy to do so elegantly by creating structures that implement the interfaces for dependencies of the tested code and exposing functions that match the interface's signature as attributes that can be overridden externally. package mocks import ( \"testing\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" ) type Loader struct { TrieFunc func () ( * trie . MTrie , error ) } func BaselineLoader ( t * testing . T ) * Loader { t . Helper () l := Loader { TrieFunc : func () ( * trie . MTrie , error ) { return GenericTrie , nil }, } return & l } func ( l * Loader ) Trie () ( * trie . MTrie , error ) { return l . TrieFunc () } Using those mocks is as simple as instantiating a baseline version of the mock and setting its attributes to the desired functions: // ... t . Run ( \"handles failure to load checkpoint\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusEmpty ) load := mocks . BaselineLoader ( t ) load . CheckpointFunc = func () ( * trie . MTrie , error ) { return nil , mocks . GenericError } tr . load = load err := tr . BootstrapState ( st ) assert . Error ( t , err ) }) // ...","title":"Mocks"},{"location":"code/testing/#pseudorandom-generic-values","text":"When using test data for unit tests, it is always a good idea to use random generated data as the inputs. This avoids the bias where a test passes because it is given a valid set of inputs while some other inputs might have highlighted a flaw in the logic, by using an unconstrained data set. In order for the tests to be repeatable and for results to be consistent though, the given inputs should not be completely random, but instead they should be pseudorandom, with the same initial seed, to ensure the same sequence of \"random\" tests. Here is an example of such a value being generated. func GenericAddresses ( number int ) [] flow . Address { // Ensure consistent deterministic results. random := rand . New ( rand . NewSource ( 5 )) var addresses [] flow . Address for i := 0 ; i < number ; i ++ { var address flow . Address binary . BigEndian . PutUint64 ( address [ 0 :], random . Uint64 ()) addresses = append ( addresses , address ) } return addresses } func GenericAddress ( index int ) flow . Address { return GenericAddresses ( index + 1 )[ index ] } Warning While randomly generating valid inputs makes sense, randomly generating invalid inputs does not. In the case of invalid inputs, it is much better to have an exhaustive list of all types of cases that are expected to be invalid and always test each one of them.","title":"Pseudorandom Generic Values"},{"location":"code/testing/#parallelization","text":"Since the version 1.7 of Go, tests can be run in parallel. This can be done by calling t.Parallel in each subtest. Calling this function signals that the test is to be run in parallel with (and only with) other parallel tests, and the amount of tests running in parallel is limited by the value of runtime.GOMAXPROCS . There are multiple advantages to parallelizing tests: It ensures that regardless of the order in which inputs are given, components behave as expected. It maximizes performance, which in turns results in a faster CI and a faster workflow for everyone, which allows us to write more tests and therefore produce more actionable data to find bugs as well as improve tests cases and coverage. It makes it possible to ensure that the components you test are concurrency-safe Parallelizing table-driven tests When it comes to table-driven tests, a common pitfall developers fall into is to call t.Parallel in their subtest without capturing the loop variable with their test case. Here is an example of how it should be done: func TestGroupedParallel ( t * testing . T ) { for _ , tc := range tests { tc := tc // capture range variable t . Run ( tc . Name , func ( t * testing . T ) { t . Parallel () ... }) } }","title":"Parallelization"},{"location":"code/testing/#standard-testing-package","text":"The standard testing package is very powerful, and does not require additional frameworks to be used efficiently. The only exception we make to that are the stretchr/testify/assert and stretchr/testify/require packages which we use only for convenience, as they expose assertion functions that produce consistent outputs and make tests easy to understand.","title":"Standard testing Package"},{"location":"code/testing/#subtests-and-sub-benchmarks","text":"The testing package exposes a Run method on the T type which makes it possible to nest tests within tests. This can be very useful, as it enables creating a hierarchical structure within a test. index_internal_test.go func TestIndex ( t * testing . T ) { // ... t . Run ( \"collections\" , func ( t * testing . T ) { t . Parallel () collections := mocks . GenericCollections ( 4 ) reader , writer , db := setupIndex ( t ) defer db . Close () assert . NoError ( t , writer . Collections ( mocks . GenericHeight , collections )) // Close the writer to make it commit its transactions. require . NoError ( t , writer . Close ()) // NOTE: The following subtests should NOT be run in parallel, because of the deferral // to close the database above. t . Run ( \"retrieve collection by ID\" , func ( t * testing . T ) { got , err := reader . Collection ( collections [ 0 ]. ID ()) require . NoError ( t , err ) assert . Equal ( t , collections [ 0 ], got ) }) t . Run ( \"retrieve collections by height\" , func ( t * testing . T ) { got , err := reader . CollectionsByHeight ( mocks . GenericHeight ) require . NoError ( t , err ) assert . ElementsMatch ( t , mocks . GenericCollectionIDs ( 4 ), got ) }) t . Run ( \"retrieve transactions from collection\" , func ( t * testing . T ) { // For now this index is not used. }) }) // ... }","title":"Subtests and Sub-benchmarks"},{"location":"code/testing/#table-driven-tests","text":"It makes a lot of sense to use subtests when testing the behavior of complex components, but it is better to use table-driven tests when testing a simple function with an expected output for a given input, and that there are many cases to cover. For such cases, table-driven tests massively improve clarity and readability. They should not be used blindly for all tests, however. In cases where the tested component is complex and that testing its methods cannot be simplified to a common setup, call to a function and assertion of the output, trying to use table-driven tests at all costs might lead to messy code, where the subtest which runs the test case is full of conditions to try to handle each separate setup. This is usually a sign that using simple tests and subtests would be a better approach.","title":"Table-Driven Tests"},{"location":"code/testing/#case-study-the-flow-dps-mapper","text":"Sometimes, a core piece of software might seem impossible to test. That was the case for the mapper component in Flow DPS at some point, where its main function consisted of a 453-lines-long loop which orchestrated the use of all the other components of the application. mapper_old.go package mapper import ( \"bytes\" \"context\" \"errors\" \"fmt\" \"os\" \"sort\" \"sync\" \"github.com/gammazero/deque\" \"github.com/rs/zerolog\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/flattener\" \"github.com/onflow/flow-go/ledger/complete/mtrie/node\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/ledger/complete/wal\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" \"github.com/optakt/flow-dps/models/index\" ) type Mapper struct { log zerolog . Logger cfg Config chain Chain feed Feeder index index . Writer wg * sync . WaitGroup stop chan struct {} } // New creates a new mapper that uses chain data to map trie updates to blocks // and then passes on the details to the indexer for indexing. func New ( log zerolog . Logger , chain Chain , feed Feeder , index index . Writer , options ... func ( * Config )) ( * Mapper , error ) { // We don't use a checkpoint by default. The options can set one, in which // case we will add the checkpoint as a finalized state commitment in our // trie registry. cfg := Config { CheckpointFile : \"\" , PostProcessing : PostNoop , } for _ , option := range options { option ( & cfg ) } // Check if the checkpoint file exists. if cfg . CheckpointFile != \"\" { stat , err := os . Stat ( cfg . CheckpointFile ) if err != nil { return nil , fmt . Errorf ( \"invalid checkpoint file: %w\" , err ) } if stat . IsDir () { return nil , fmt . Errorf ( \"invalid checkpoint file: directory\" ) } } i := Mapper { log : log , chain : chain , feed : feed , index : index , cfg : cfg , wg : & sync . WaitGroup {}, stop : make ( chan struct {}), } return & i , nil } func ( m * Mapper ) Stop ( ctx context . Context ) error { close ( m . stop ) done := make ( chan struct {}) go func () { m . wg . Wait () close ( done ) }() select { case <- ctx . Done (): return ctx . Err () case <- done : return nil } } // NOTE: We might want to move height and tree (checkpoint) to parameters of the // run function; that would make it quite easy to resume from an arbitrary // point in the LedgerWAL and get rid of the related struct fields. func ( m * Mapper ) Run () error { m . wg . Add ( 1 ) defer m . wg . Done () // We start trying to map at the root height. height , err := m . chain . Root () if err != nil { return fmt . Errorf ( \"could not get root height: %w\" , err ) } // We always initialize an empty state trie to refer to the first step // before the checkpoint. If there is no checkpoint, then the step after the // checkpoint will also just be the empty trie. Otherwise, the second trie // will load the checkpoint trie. empty := trie . NewEmptyMTrie () var tree * trie . MTrie if m . cfg . CheckpointFile == \"\" { tree = empty } else { m . log . Info (). Msg ( \"checkpoint rebuild started\" ) file , err := os . Open ( m . cfg . CheckpointFile ) if err != nil { return fmt . Errorf ( \"could not open checkpoint file: %w\" , err ) } checkpoint , err := wal . ReadCheckpoint ( file ) if err != nil { return fmt . Errorf ( \"could not read checkpoint: %w\" , err ) } trees , err := flattener . RebuildTries ( checkpoint ) if err != nil { return fmt . Errorf ( \"could not rebuild tries: %w\" , err ) } if len ( trees ) != 1 { return fmt . Errorf ( \"should only have one trie in root checkpoint (tries: %d)\" , len ( trees )) } tree = trees [ 0 ] m . log . Info (). Msg ( \"checkpoint rebuild finished\" ) } m . log . Info (). Msg ( \"path collection started\" ) // We have to index all of the paths from the checkpoint; otherwise, we will // miss every single one of the bootstrapped registers. paths := make ([] ledger . Path , 0 , len ( tree . AllPayloads ())) queue := deque . New () root := tree . RootNode () if root != nil { queue . PushBack ( root ) } for queue . Len () > 0 { node := queue . PopBack ().( * node . Node ) if node . IsLeaf () { path := node . Path () paths = append ( paths , * path ) continue } if node . LeftChild () != nil { queue . PushBack ( node . LeftChild ()) } if node . RightChild () != nil { queue . PushBack ( node . RightChild ()) } } m . log . Info (). Int ( \"paths\" , len ( paths )). Msg ( \"path collection finished\" ) m . log . Info (). Msg ( \"path sorting started\" ) sort . Slice ( paths , func ( i int , j int ) bool { return bytes . Compare ( paths [ i ][:], paths [ j ][:]) < 0 }) m . log . Info (). Msg ( \"path sorting finished\" ) // When trying to go from one finalized block to the next, we keep a list // of intermediary tries until the full set of transitions have been // identified. We keep track of these transitions as steps in this map. steps := make ( map [ flow . StateCommitment ] * Step ) // We start at an \"imaginary\" step that refers to an empty trie, has no // paths and no previous commit. We consider this step already done, so it // will never be indexed; it's merely used as the sentinel value for // stopping when we index the first block. It also makes sure that we don't // return a `nil` trie if we abort indexing before the first block is done. emptyCommit := flow . DummyStateCommitment steps [ emptyCommit ] = & Step { Commit : flow . StateCommitment {}, Paths : nil , Tree : empty , } // We then add a second step that refers to the first step that is already // done, which uses the commit of the initial state trie after the // checkpoint has been loaded, and contains all of the paths found in the // initial checkpoint state trie. This will make sure that we index all the // data from the checkpoint as part of the first block. rootCommit := flow . StateCommitment ( tree . RootHash ()) steps [ rootCommit ] = & Step { Commit : emptyCommit , Paths : paths , Tree : tree , } // This is how we let the indexing loop know that the first \"imaginary\" step // was already indexed. The `commitPrev` value is used as a sentinel value // for when to stop going backwards through the steps when indexing a block. // This means the value is always set to the last already indexed step. commitPrev := emptyCommit m . log . Info (). Msg ( \"state indexing started\" ) // Next, we launch into the loop that is responsible for mapping all // incoming trie updates to a block. The loop itself has no concept of what // the next state commitment is that we should look at. It will simply try // to find a previous step for _any_ trie update that comes in. This means // that the first trie update needs to either apply to the empty trie or to // the trie after the checkpoint in order to be processed. once := & sync . Once {} Outer : for { // We want to check in this tight loop if we want to quit, just in case // we get stuck on a timed out network connection. select { case <- m . stop : break Outer default : // keep going } log := m . log . With (). Uint64 ( \"height\" , height ). Hex ( \"commit_prev\" , commitPrev [:]). Logger () // As a first step, we retrieve the state commitment of the finalized // block at the current height; we start at the root height and then // increase it each time we are done indexing a block. Once an applied // trie update gives us a state trie with the same root hash as // `commitNext`, we have reached the end state of the next finalized // block and can index all steps in-between for that block height. commitNext , err := m . chain . Commit ( height ) // If the retrieval times out, it's possible that we are on a live chain // and the next block has not been finalized yet. We should thus simply // retry until we have a new block. if errors . Is ( err , dps . ErrTimeout ) { log . Warn (). Msg ( \"commit retrieval timed out, retrying\" ) continue Outer } // If we have reached the end of the finalized blocks, we are probably // on a historical chain and there are no more finalized blocks for the // related spork. We can exit without error. if errors . Is ( err , dps . ErrFinished ) { log . Debug (). Msg ( \"reached end of finalized chain\" ) break Outer } // Any other error should not happen and should crash explicitly. if err != nil { return fmt . Errorf ( \"could not retrieve next commit (height: %d): %w\" , height , err ) } log = log . With (). Hex ( \"commit_next\" , commitNext [:]). Logger () Inner : for { // We want to check in this tight loop if we want to quit, just in case // we get stuck on a timed out network connection. select { case <- m . stop : break Outer default : // keep going } // When we have the state commitment of the next finalized block, we // check to see if we find a trie for it in our steps. If we do, it // means that we have steps from the last finalized block to the // finalized block at the current height. This condition will // trigger immediately for every empty block. _ , ok := steps [ commitNext ] if ok { break Inner } // If we don't find a trie for the current state commitment, we need // to keep applying trie updates to state tries until one of them // does have the correct commit. We simply feed the next trie update // here. update , err := m . feed . Update () // Once more, we might be on a live spork and the next delta might not // be available yet. In that case, keep trying. if errors . Is ( err , dps . ErrTimeout ) { log . Warn (). Msg ( \"delta retrieval timed out, retrying\" ) continue Inner } // Similarly, if no more deltas are available, we reached the end of // the WAL and we are done reconstructing the execution state. if errors . Is ( err , dps . ErrFinished ) { log . Debug (). Msg ( \"reached end of delta log\" ) break Outer } // Other errors should fail execution as they should not happen. if err != nil { return fmt . Errorf ( \"could not retrieve next delta: %w\" , err ) } // NOTE: We used to require a copy of the `RootHash` here, when it // was still a byte slice, as the underlying slice was being reused. // It was changed to a value type that is always copied now. commitBefore := flow . StateCommitment ( update . RootHash ) log := log . With (). Hex ( \"commit_before\" , commitBefore [:]). Logger () // Once we have our new update and know which trie it should be // applied to, we check to see if we have such a trie in our current // steps. If not, we can simply skip it; this can happen, for // example, when there is an execution fork and the trie update // applies to an obsolete part of the blockchain history. step , ok := steps [ commitBefore ] if ! ok { log . Debug (). Msg ( \"skipping trie update without matching trie\" ) continue Inner } // We de-duplicate the paths and payloads here. This replicates some // code that is part of the execution node and has moved between // different layers of the architecture. We keep it to be safe for // all versions of the Flow dependencies. // NOTE: Past versions of this code required paths to be copied, // because the underlying slice was being re-used. In contrary, // deep-copying payloads was a bad idea, because they were already // being copied by the trie insertion code, and it would have led to // twice the memory usage. paths = make ([] ledger . Path , 0 , len ( update . Paths )) lookup := make ( map [ ledger . Path ] * ledger . Payload ) for i , path := range update . Paths { _ , ok := lookup [ path ] if ! ok { paths = append ( paths , path ) } lookup [ path ] = update . Payloads [ i ] } sort . Slice ( paths , func ( i , j int ) bool { return bytes . Compare ( paths [ i ][:], paths [ j ][:]) < 0 }) payloads := make ([] ledger . Payload , 0 , len ( paths )) for _ , path := range paths { payloads = append ( payloads , * lookup [ path ]) } // We can now apply the trie update to the state trie as it was at // the previous step. This is where the trie code will deep-copy the // payloads. // NOTE: It's important that we don't shadow the variable here, // otherwise the root trie will never go out of scope and we will // never garbage collect any of the root trie payloads that have // been replaced by subsequent trie updates. tree , err = trie . NewTrieWithUpdatedRegisters ( step . Tree , paths , payloads ) if err != nil { return fmt . Errorf ( \"could not update trie: %w\" , err ) } // We then store the new trie along with the state commitment of its // parent and the paths that were changed. This will make it // available for subsequent trie updates to be applied to it, and it // will also allow us to reconstruct the payloads changed in this // step by retrieving them directly from the trie with the given // paths. commitAfter := flow . StateCommitment ( tree . RootHash ()) step = & Step { Commit : commitBefore , Paths : paths , Tree : tree , } steps [ commitAfter ] = step log . Debug (). Hex ( \"commit_after\" , commitAfter [:]). Msg ( \"trie update applied\" ) } // At this point we have identified a step that has lead to the state // commitment of the finalized block at the current height. We can // retrieve some additional indexing data, such as the block header and // the events that resulted from transactions in the block. header , err := m . chain . Header ( height ) if err != nil { return fmt . Errorf ( \"could not retrieve header: %w (height: %d)\" , err , height ) } events , err := m . chain . Events ( height ) if err != nil { return fmt . Errorf ( \"could not retrieve events: %w (height: %d)\" , err , height ) } transactions , err := m . chain . Transactions ( height ) if err != nil { return fmt . Errorf ( \"could not retrieve transactions: %w (height: %d)\" , err , height ) } collections , err := m . chain . Collections ( height ) if err != nil { return fmt . Errorf ( \"could not retrieve collections: %w (height: %d)\" , err , height ) } blockID := header . ID () // TODO: Refactor the mapper in https://github.com/optakt/flow-dps/issues/128 // and replace naive if statements around indexing. // We then index the data for the finalized block at the current height. if m . cfg . IndexHeaders { err = m . index . Header ( height , header ) if err != nil { return fmt . Errorf ( \"could not index header: %w\" , err ) } } if m . cfg . IndexCommit { err = m . index . Commit ( height , commitNext ) if err != nil { return fmt . Errorf ( \"could not index commit: %w\" , err ) } } if m . cfg . IndexEvents { err = m . index . Events ( height , events ) if err != nil { return fmt . Errorf ( \"could not index events: %w\" , err ) } } if m . cfg . IndexBlocks { err = m . index . Height ( blockID , height ) if err != nil { return fmt . Errorf ( \"could not index block heights: %w\" , err ) } } if m . cfg . IndexTransactions { err = m . index . Transactions ( blockID , collections , transactions ) if err != nil { return fmt . Errorf ( \"could not index transactions: %w\" , err ) } } // In order to index the payloads, we step back from the state // commitment of the finalized block at the current height to the state // commitment of the last finalized block that was indexed. For each // step, we collect all the payloads by using the paths for the step and // index them as we go. // NOTE: We keep track of the paths for which we already indexed // payloads, so we can skip them in earlier steps. One inherent benefit // of stepping from the last step to the first step is that this will // automatically use only the latest update of a register, which is // exactly what we want. commit := commitNext updated := make ( map [ ledger . Path ] struct {}) for commit != commitPrev { // In the first part, we get the step we are currently at and filter // out any paths that have already been updated. step := steps [ commit ] paths := make ([] ledger . Path , 0 , len ( step . Paths )) for _ , path := range step . Paths { _ , ok := updated [ path ] if ok { continue } paths = append ( paths , path ) updated [ path ] = struct {}{} } if ! m . cfg . IndexPayloads { commit = step . Commit continue } // We then divide the remaining paths into chunks of 1000. For each // batch, we retrieve the payloads from the state trie as it was at // the end of this block and index them. count := 0 n := 1000 total := ( len ( paths ) + n - 1 ) / n log . Debug (). Int ( \"num_paths\" , len ( paths )). Int ( \"num_batches\" , total ). Msg ( \"path batching executed\" ) for start := 0 ; start < len ( paths ); start += n { // This loop may take a while, especially for the root checkpoint // updates, so check if we should quit. select { case <- m . stop : break Outer default : // keep going } end := start + n if end > len ( paths ) { end = len ( paths ) } batch := paths [ start : end ] payloads := step . Tree . UnsafeRead ( batch ) err = m . index . Payloads ( height , batch , payloads ) if err != nil { return fmt . Errorf ( \"could not index payloads: %w\" , err ) } count ++ log . Debug (). Int ( \"batch\" , count ). Int ( \"start\" , start ). Int ( \"end\" , end ). Msg ( \"path batch indexed\" ) } // Finally, we forward the commit to the previous trie update and // repeat until we have stepped all the way back to the last indexed // commit. commit = step . Commit } // At this point, we can delete any trie that does not correspond to // the state that we have just reached. This will allow the garbage // collector to free up any payload that has been changed and which is // no longer part of the state trie at the newly indexed finalized // block. for key := range steps { if key != commitNext { delete ( steps , key ) } } // Last but not least, we take care of properly indexing the height of // the first indexed block and the height of the last indexed block. once . Do ( func () { err = m . index . First ( height ) }) if err != nil { return fmt . Errorf ( \"could not index first height: %w\" , err ) } err = m . index . Last ( height ) if err != nil { return fmt . Errorf ( \"could not index last height: %w\" , err ) } // We have now successfully indexed all state trie changes and other // data at the current height. We set the last indexed step to the last // step from our current height, and then increase the height to start // the indexing of the next block. commitPrev = commitNext height ++ log . Info (). Hex ( \"block\" , blockID [:]). Int ( \"num_changes\" , len ( updated )). Int ( \"num_events\" , len ( events )). Msg ( \"block data indexed\" ) } m . log . Info (). Msg ( \"state indexing finished\" ) step := steps [ commitPrev ] m . cfg . PostProcessing ( step . Tree ) return nil } As it was, this code was untestable. Covering each possible case from this huge piece of logic would have required immense, complex, unreadable tests, that would break whenever a piece of this logic would change, and this would require a huge amount of maintenance effort. To solve that massive problem, we refactored our original mapper into a finite-state machine which replicates the same computation logic by applying transitions to a state. mapper_new.go package mapper import ( \"errors\" \"fmt\" \"sync\" \"time\" \"github.com/rs/zerolog\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" ) // TransitionFunc is a function that is applied onto the state machine's // state. type TransitionFunc func ( * State ) error // Transitions is what applies transitions to the state of an FSM. type Transitions struct { cfg Config log zerolog . Logger load Loader chain dps . Chain feed Feeder read dps . Reader write dps . Writer once * sync . Once } // NewTransitions returns a Transitions component using the given dependencies and using the given options func NewTransitions ( log zerolog . Logger , load Loader , chain dps . Chain , feed Feeder , read dps . Reader , write dps . Writer , options ... Option ) * Transitions { cfg := DefaultConfig for _ , option := range options { option ( & cfg ) } t := Transitions { log : log . With (). Str ( \"component\" , \"mapper_transitions\" ). Logger (), cfg : cfg , load : load , chain : chain , feed : feed , read : read , write : write , once : & sync . Once {}, } return & t } // InitializeMapper initializes the mapper by either going into bootstrapping or // into resuming, depending on the configuration. func ( t * Transitions ) InitializeMapper ( s * State ) error { if s . status != StatusInitialize { return fmt . Errorf ( \"invalid status for initializing mapper (%s)\" , s . status ) } if t . cfg . BootstrapState { s . status = StatusBootstrap return nil } s . status = StatusResume return nil } // BootstrapState bootstraps the state by loading the checkpoint if there is one // and initializing the elements subsequently used by the FSM. func ( t * Transitions ) BootstrapState ( s * State ) error { if s . status != StatusBootstrap { return fmt . Errorf ( \"invalid status for bootstrapping state (%s)\" , s . status ) } // We always need at least one step in our forest, which is used as the // stopping point when indexing the payloads since the last finalized // block. We thus introduce an empty tree, with no paths and an // irrelevant previous commit. empty := trie . NewEmptyMTrie () s . forest . Save ( empty , nil , flow . DummyStateCommitment ) // The chain indexing will forward last to next and next to current height, // which will be the one for the checkpoint. first := flow . StateCommitment ( empty . RootHash ()) s . last = flow . DummyStateCommitment s . next = first t . log . Info (). Hex ( \"commit\" , first [:]). Msg ( \"added empty tree to forest\" ) // Then, we can load the root height and apply it to the state. That // will allow us to load the root blockchain data in the next step. height , err := t . chain . Root () if err != nil { return fmt . Errorf ( \"could not get root height: %w\" , err ) } s . height = height // When bootstrapping, the loader injected into the mapper loads the root // checkpoint. tree , err := t . load . Trie () if err != nil { return fmt . Errorf ( \"could not load root trie: %w\" , err ) } paths := allPaths ( tree ) s . forest . Save ( tree , paths , first ) second := tree . RootHash () t . log . Info (). Uint64 ( \"height\" , s . height ). Hex ( \"commit\" , second [:]). Int ( \"registers\" , len ( paths )). Msg ( \"added checkpoint tree to forest\" ) // We have successfully bootstrapped. However, no chain data for the root // block has been indexed yet. This is why we \"pretend\" that we just // forwarded the state to this height, so we go straight to the chain data // indexing. s . status = StatusIndex return nil } // ResumeIndexing resumes indexing the data from a previous run. func ( t * Transitions ) ResumeIndexing ( s * State ) error { if s . status != StatusResume { return fmt . Errorf ( \"invalid status for resuming indexing (%s)\" , s . status ) } // When resuming, we want to avoid overwriting the `first` height in the // index with the height we are resuming from. Theoretically, all that would // be needed would be to execute a no-op on `once`, which would subsequently // be skipped in the height forwarding code. However, this bug was already // released, so we have databases where `first` was incorrectly set to the // height we resume from. In order to fix them, we explicitly write the // correct `first` height here again, while at the same time using `once` to // disable any subsequent attempts to write it. first , err := t . chain . Root () if err != nil { return fmt . Errorf ( \"could not get root height: %w\" , err ) } t . once . Do ( func () { err = t . write . First ( first ) }) if err != nil { return fmt . Errorf ( \"could not write first: %w\" , err ) } // We need to know what the last indexed height was at the point we stopped // indexing. last , err := t . read . Last () if err != nil { return fmt . Errorf ( \"could not get last height: %w\" , err ) } // When resuming, the loader injected into the mapper rebuilds the trie from // the paths and payloads stored in the index database. tree , err := t . load . Trie () if err != nil { return fmt . Errorf ( \"could not restore index trie: %w\" , err ) } // After loading the trie, we should do a sanity check on its hash against // the commit we indexed for it. hash := flow . StateCommitment ( tree . RootHash ()) commit , err := t . read . Commit ( last ) if err != nil { return fmt . Errorf ( \"could not get last commit: %w\" , err ) } if hash != commit { return fmt . Errorf ( \"restored trie hash does not match last commit (hash: %x, commit: %x)\" , hash , commit ) } // At this point, we can store the restored trie in our forest, as the trie // for the last finalized block. We do not need to care about the parent // state commitment or the paths, as they should not be used. s . last = flow . DummyStateCommitment s . next = commit s . forest . Save ( tree , nil , flow . DummyStateCommitment ) // Lastly, we just need to point to the next height. The chain indexing will // then proceed with the first non-indexed block and forward the state // commitments accordingly. s . height = last + 1 // At this point, we should be able to start indexing the chain data for // the next height. s . status = StatusIndex return nil } // IndexChain indexes chain data for the current height. func ( t * Transitions ) IndexChain ( s * State ) error { if s . status != StatusIndex { return fmt . Errorf ( \"invalid status for indexing chain (%s)\" , s . status ) } log := t . log . With (). Uint64 ( \"height\" , s . height ). Logger () // We try to retrieve the next header until it becomes available, which // means all data coming from the protocol state is available after this // point. header , err := t . chain . Header ( s . height ) if errors . Is ( err , dps . ErrUnavailable ) { log . Debug (). Msg ( \"waiting for next header\" ) time . Sleep ( t . cfg . WaitInterval ) return nil } if err != nil { return fmt . Errorf ( \"could not get header: %w\" , err ) } // At this point, we can retrieve the data from the consensus state. This is // a slight optimization for the live indexer, as it allows us to process // some data before the full execution data becomes available. guarantees , err := t . chain . Guarantees ( s . height ) if err != nil { return fmt . Errorf ( \"could not get guarantees: %w\" , err ) } seals , err := t . chain . Seals ( s . height ) if err != nil { return fmt . Errorf ( \"could not get seals: %w\" , err ) } // We can also proceed to already indexing the data related to the consensus // state, before dealing with anything related to execution data, which // might go into the wait state. blockID := header . ID () err = t . write . Height ( blockID , s . height ) if err != nil { return fmt . Errorf ( \"could not index height: %w\" , err ) } err = t . write . Header ( s . height , header ) if err != nil { return fmt . Errorf ( \"could not index header: %w\" , err ) } err = t . write . Guarantees ( s . height , guarantees ) if err != nil { return fmt . Errorf ( \"could not index guarantees: %w\" , err ) } err = t . write . Seals ( s . height , seals ) if err != nil { return fmt . Errorf ( \"could not index seals: %w\" , err ) } // Next, we try to retrieve the next commit until it becomes available, // at which point all the data coming from the execution data should be // available. commit , err := t . chain . Commit ( s . height ) if errors . Is ( err , dps . ErrUnavailable ) { log . Debug (). Msg ( \"waiting for next state commitment\" ) time . Sleep ( t . cfg . WaitInterval ) return nil } if err != nil { return fmt . Errorf ( \"could not get commit: %w\" , err ) } collections , err := t . chain . Collections ( s . height ) if err != nil { return fmt . Errorf ( \"could not get collections: %w\" , err ) } transactions , err := t . chain . Transactions ( s . height ) if err != nil { return fmt . Errorf ( \"could not get transactions: %w\" , err ) } results , err := t . chain . Results ( s . height ) if err != nil { return fmt . Errorf ( \"could not get transaction results: %w\" , err ) } events , err := t . chain . Events ( s . height ) if err != nil { return fmt . Errorf ( \"could not get events: %w\" , err ) } // Next, all we need to do is index the remaining data and we have fully // processed indexing for this block height. err = t . write . Commit ( s . height , commit ) if err != nil { return fmt . Errorf ( \"could not index commit: %w\" , err ) } err = t . write . Collections ( s . height , collections ) if err != nil { return fmt . Errorf ( \"could not index collections: %w\" , err ) } err = t . write . Transactions ( s . height , transactions ) if err != nil { return fmt . Errorf ( \"could not index transactions: %w\" , err ) } err = t . write . Results ( results ) if err != nil { return fmt . Errorf ( \"could not index transaction results: %w\" , err ) } err = t . write . Events ( s . height , events ) if err != nil { return fmt . Errorf ( \"could not index events: %w\" , err ) } // At this point, we need to forward the `last` state commitment to // `next`, so we know what the state commitment was at the last finalized // block we processed. This will allow us to know when to stop when // walking back through the forest to collect trie updates. s . last = s . next // Last but not least, we need to update `next` to point to the commit we // have just retrieved for the new block height. This is the sentinel that // tells us when we have collected enough trie updates for the forest to // have reached the next finalized block. s . next = commit log . Info (). Msg ( \"indexed blockchain data for finalized block\" ) // After indexing the blockchain data, we can go back to updating the state // tree until we find the commit of the finalized block. This will allow us // to index the payloads then. s . status = StatusUpdate return nil } // UpdateTree updates the state's tree. If the state's forest already matches with the next block's state commitment, // it immediately returns and sets the state's status to StatusMatched. func ( t * Transitions ) UpdateTree ( s * State ) error { if s . status != StatusUpdate { return fmt . Errorf ( \"invalid status for updating tree (%s)\" , s . status ) } log := t . log . With (). Uint64 ( \"height\" , s . height ). Hex ( \"last\" , s . last [:]). Hex ( \"next\" , s . next [:]). Logger () // If the forest contains a tree for the commit of the next finalized block, // we have reached our goal, and we can go to the next step in order to // collect the register payloads we want to index for that block. ok := s . forest . Has ( s . next ) if ok { log . Info (). Hex ( \"commit\" , s . next [:]). Msg ( \"matched commit of finalized block\" ) s . status = StatusCollect return nil } // First, we get the next tree update from the feeder. We can skip it if // it doesn't have any updated paths, or if we can't find the tree to apply // it to in the forest. This usually means that it was meant for a pruned // branch of the execution forest. update , err := t . feed . Update () if errors . Is ( err , dps . ErrUnavailable ) { time . Sleep ( t . cfg . WaitInterval ) log . Debug (). Msg ( \"waiting for next trie update\" ) return nil } if err != nil { return fmt . Errorf ( \"could not feed update: %w\" , err ) } parent := flow . StateCommitment ( update . RootHash ) tree , ok := s . forest . Tree ( parent ) if ! ok { log . Warn (). Msg ( \"state commitment mismatch, retrieving next trie update\" ) return nil } // We then apply the update to the relevant tree, as retrieved from the // forest, and save the updated tree in the forest. If the tree is not new, // we should error, as that should not happen. paths , payloads := pathsPayloads ( update ) tree , err = trie . NewTrieWithUpdatedRegisters ( tree , paths , payloads ) if err != nil { return fmt . Errorf ( \"could not update tree: %w\" , err ) } s . forest . Save ( tree , paths , parent ) hash := tree . RootHash () log . Info (). Hex ( \"commit\" , hash [:]). Int ( \"registers\" , len ( paths )). Msg ( \"updated tree with register payloads\" ) return nil } // CollectRegisters reads the payloads for the next block to be indexed from the state's forest, unless payload // indexing is disabled. func ( t * Transitions ) CollectRegisters ( s * State ) error { log := t . log . With (). Uint64 ( \"height\" , s . height ). Hex ( \"commit\" , s . next [:]). Logger () if s . status != StatusCollect { return fmt . Errorf ( \"invalid status for collecting registers (%s)\" , s . status ) } // If indexing payloads is disabled, we can bypass collection and indexing // of payloads and just go straight to forwarding the height to the next // finalized block. if t . cfg . SkipRegisters { s . status = StatusForward return nil } // If we index payloads, we are basically stepping back from (and including) // the tree that corresponds to the next finalized block all the way up to // (and excluding) the tree for the last finalized block we indexed. To do // so, we will use the parent state commit to retrieve the parent trees from // the forest, and we use the paths we recorded changes on to retrieve the // changed payloads at each step. commit := s . next for commit != s . last { // We do this check only once, so that we don't need to do it for // each item we retrieve. The tree should always be there, but we // should check just to not fail silently. ok := s . forest . Has ( commit ) if ! ok { return fmt . Errorf ( \"could not load tree (commit: %x)\" , commit ) } // For each path, we retrieve the payload and add it to the registers we // will index later. If we already have a payload for the path, it is // more recent as we iterate backwards in time, so we can skip the // outdated payload. // NOTE: We read from the tree one by one here, as the performance // overhead is minimal compared to the disk i/o for badger, and it // allows us to ignore sorting of paths. tree , _ := s . forest . Tree ( commit ) paths , _ := s . forest . Paths ( commit ) for _ , path := range paths { _ , ok := s . registers [ path ] if ok { continue } payloads := tree . UnsafeRead ([] ledger . Path { path }) s . registers [ path ] = payloads [ 0 ] } log . Debug (). Int ( \"batch\" , len ( paths )). Msg ( \"collected register batch for finalized block\" ) // We now step back to the parent of the current state trie. parent , _ := s . forest . Parent ( commit ) commit = parent } log . Info (). Int ( \"registers\" , len ( s . registers )). Msg ( \"collected all registers for finalized block\" ) // At this point, we have collected all the payloads, so we go to the next // step, where we will index them. s . status = StatusMap return nil } // MapRegisters maps the collected registers to the current block. func ( t * Transitions ) MapRegisters ( s * State ) error { if s . status != StatusMap { return fmt . Errorf ( \"invalid status for indexing registers (%s)\" , s . status ) } log := t . log . With (). Uint64 ( \"height\" , s . height ). Hex ( \"commit\" , s . next [:]). Logger () // If there are no registers left to be indexed, we can go to the next step, // which is about forwarding the height to the next finalized block. if len ( s . registers ) == 0 { log . Info (). Msg ( \"indexed all registers for finalized block\" ) s . status = StatusForward return nil } // We will now collect and index 1000 registers at a time. This gives the // FSM the chance to exit the loop between every 1000 payloads we index. It // doesn't really matter for badger if they are in random order, so this // way of iterating should be fine. n := 1000 paths := make ([] ledger . Path , 0 , n ) payloads := make ([] * ledger . Payload , 0 , n ) for path , payload := range s . registers { paths = append ( paths , path ) payloads = append ( payloads , payload ) delete ( s . registers , path ) if len ( paths ) >= n { break } } // Then we store the (maximum) 1000 paths and payloads. err := t . write . Payloads ( s . height , paths , payloads ) if err != nil { return fmt . Errorf ( \"could not index registers: %w\" , err ) } log . Debug (). Int ( \"batch\" , len ( paths )). Int ( \"remaining\" , len ( s . registers )). Msg ( \"indexed register batch for finalized block\" ) return nil } // ForwardHeight increments the height at which the mapping operates, and updates the last indexed height. func ( t * Transitions ) ForwardHeight ( s * State ) error { if s . status != StatusForward { return fmt . Errorf ( \"invalid status for forwarding height (%s)\" , s . status ) } // After finishing the indexing of the payloads for a finalized block, or // skipping it, we should document the last indexed height. On the first // pass, we will also index the first indexed height here. var err error t . once . Do ( func () { err = t . write . First ( s . height ) }) if err != nil { return fmt . Errorf ( \"could not index first height: %w\" , err ) } err = t . write . Last ( s . height ) if err != nil { return fmt . Errorf ( \"could not index last height: %w\" , err ) } // Now that we have indexed the heights, we can forward to the next height, // and reset the forest to free up memory. s . height ++ s . forest . Reset ( s . next ) t . log . Info (). Uint64 ( \"height\" , s . height ). Msg ( \"forwarded finalized block to next height\" ) // Once the height is forwarded, we can set the status so that we index // the blockchain data next. s . status = StatusIndex return nil } This refactoring effort allowed us to write simple and concise tests that call a transition function upon the state machine and make assertions upon the resulting state. mapper_new_internal_test.go package mapper import ( \"sync\" \"testing\" \"github.com/stretchr/testify/assert\" \"github.com/stretchr/testify/require\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" \"github.com/optakt/flow-dps/testing/mocks\" ) func TestNewTransitions ( t * testing . T ) { t . Run ( \"nominal case, without options\" , func ( t * testing . T ) { load := mocks . BaselineLoader ( t ) chain := mocks . BaselineChain ( t ) feed := mocks . BaselineFeeder ( t ) read := mocks . BaselineReader ( t ) write := mocks . BaselineWriter ( t ) tr := NewTransitions ( mocks . NoopLogger , load , chain , feed , read , write ) assert . NotNil ( t , tr ) assert . Equal ( t , chain , tr . chain ) assert . Equal ( t , feed , tr . feed ) assert . Equal ( t , write , tr . write ) assert . NotNil ( t , tr . once ) assert . Equal ( t , DefaultConfig , tr . cfg ) }) t . Run ( \"nominal case, with option\" , func ( t * testing . T ) { load := mocks . BaselineLoader ( t ) chain := mocks . BaselineChain ( t ) feed := mocks . BaselineFeeder ( t ) read := mocks . BaselineReader ( t ) write := mocks . BaselineWriter ( t ) skip := true tr := NewTransitions ( mocks . NoopLogger , load , chain , feed , read , write , WithSkipRegisters ( skip ), ) assert . NotNil ( t , tr ) assert . Equal ( t , chain , tr . chain ) assert . Equal ( t , feed , tr . feed ) assert . Equal ( t , write , tr . write ) assert . NotNil ( t , tr . once ) assert . NotEqual ( t , DefaultConfig , tr . cfg ) assert . Equal ( t , skip , tr . cfg . SkipRegisters ) assert . Equal ( t , DefaultConfig . WaitInterval , tr . cfg . WaitInterval ) }) } func TestTransitions_BootstrapState ( t * testing . T ) { t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) // Copy state in local scope so that we can override its SaveFunc without impacting other // tests running in parallel. var saveCalled bool forest := mocks . BaselineForest ( t , true ) forest . SaveFunc = func ( tree * trie . MTrie , paths [] ledger . Path , parent flow . StateCommitment ) { if ! saveCalled { assert . True ( t , tree . IsEmpty ()) assert . Nil ( t , paths ) assert . Zero ( t , parent ) saveCalled = true return } assert . False ( t , tree . IsEmpty ()) assert . Len ( t , tree . AllPayloads (), len ( paths )) assert . Len ( t , paths , 3 ) // Expect the three paths from leaves. assert . NotZero ( t , parent ) } err := tr . BootstrapState ( st ) assert . NoError ( t , err ) }) t . Run ( \"invalid state\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusForward ) err := tr . BootstrapState ( st ) assert . Error ( t , err ) }) t . Run ( \"handles failure to get root height\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return 0 , mocks . GenericError } tr . chain = chain err := tr . BootstrapState ( st ) assert . Error ( t , err ) }) } func TestTransitions_IndexChain ( t * testing . T ) { t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . HeaderFunc = func ( height uint64 ) ( * flow . Header , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericHeader , nil } chain . CommitFunc = func ( height uint64 ) ( flow . StateCommitment , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericCommit ( 0 ), nil } chain . CollectionsFunc = func ( height uint64 ) ([] * flow . LightCollection , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericCollections ( 2 ), nil } chain . GuaranteesFunc = func ( height uint64 ) ([] * flow . CollectionGuarantee , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericGuarantees ( 2 ), nil } chain . TransactionsFunc = func ( height uint64 ) ([] * flow . TransactionBody , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericTransactions ( 4 ), nil } chain . ResultsFunc = func ( height uint64 ) ([] * flow . TransactionResult , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericResults ( 4 ), nil } chain . EventsFunc = func ( height uint64 ) ([] flow . Event , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericEvents ( 8 ), nil } chain . SealsFunc = func ( height uint64 ) ([] * flow . Seal , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericSeals ( 4 ), nil } write := mocks . BaselineWriter ( t ) write . HeaderFunc = func ( height uint64 , header * flow . Header ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericHeader , header ) return nil } write . CommitFunc = func ( height uint64 , commit flow . StateCommitment ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericCommit ( 0 ), commit ) return nil } write . HeightFunc = func ( blockID flow . Identifier , height uint64 ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericHeader . ID (), blockID ) return nil } write . CollectionsFunc = func ( height uint64 , collections [] * flow . LightCollection ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericCollections ( 2 ), collections ) return nil } write . GuaranteesFunc = func ( height uint64 , guarantees [] * flow . CollectionGuarantee ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericGuarantees ( 2 ), guarantees ) return nil } write . TransactionsFunc = func ( height uint64 , transactions [] * flow . TransactionBody ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericTransactions ( 4 ), transactions ) return nil } write . ResultsFunc = func ( results [] * flow . TransactionResult ) error { assert . Equal ( t , mocks . GenericResults ( 4 ), results ) return nil } write . EventsFunc = func ( height uint64 , events [] flow . Event ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericEvents ( 8 ), events ) return nil } write . SealsFunc = func ( height uint64 , seals [] * flow . Seal ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericSeals ( 4 ), seals ) return nil } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain tr . write = write err := tr . IndexChain ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusUpdate , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve commit\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return flow . DummyStateCommitment , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index commit\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . CommitFunc = func ( uint64 , flow . StateCommitment ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve header\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . HeaderFunc = func ( uint64 ) ( * flow . Header , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index header\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . HeaderFunc = func ( uint64 , * flow . Header ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve transactions\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . TransactionsFunc = func ( uint64 ) ([] * flow . TransactionBody , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve transaction results\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . ResultsFunc = func ( uint64 ) ([] * flow . TransactionResult , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index transactions\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . ResultsFunc = func ([] * flow . TransactionResult ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve collections\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . CollectionsFunc = func ( uint64 ) ([] * flow . LightCollection , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index collections\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . CollectionsFunc = func ( uint64 , [] * flow . LightCollection ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve guarantees\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . GuaranteesFunc = func ( uint64 ) ([] * flow . CollectionGuarantee , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index guarantees\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . GuaranteesFunc = func ( uint64 , [] * flow . CollectionGuarantee ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve events\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . EventsFunc = func ( uint64 ) ([] flow . Event , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index events\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . EventsFunc = func ( uint64 , [] flow . Event ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve seals\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . SealsFunc = func ( uint64 ) ([] * flow . Seal , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index seals\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . SealsFunc = func ( uint64 , [] * flow . Seal ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) } func TestTransitions_UpdateTree ( t * testing . T ) { update := mocks . GenericTrieUpdate ( 0 ) tree := mocks . GenericTrie t . Run ( \"nominal case without match\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusUpdate ) forest := mocks . BaselineForest ( t , false ) forest . SaveFunc = func ( tree * trie . MTrie , paths [] ledger . Path , parent flow . StateCommitment ) { // Parent is RootHash of the mocks.GenericTrie. assert . Equal ( t , update . RootHash [:], parent [:]) assert . ElementsMatch ( t , paths , update . Paths ) assert . NotZero ( t , tree ) } forest . TreeFunc = func ( commit flow . StateCommitment ) ( * trie . MTrie , bool ) { assert . Equal ( t , update . RootHash [:], commit [:]) return tree , true } st . forest = forest err := tr . UpdateTree ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusUpdate , st . status ) }) t . Run ( \"nominal case with no available update temporarily\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusUpdate ) // Set up the mock feeder to return an unavailable error on the first call and return successfully // to subsequent calls. var updateCalled bool feeder := mocks . BaselineFeeder ( t ) feeder . UpdateFunc = func () ( * ledger . TrieUpdate , error ) { if ! updateCalled { updateCalled = true return nil , dps . ErrUnavailable } return mocks . GenericTrieUpdate ( 0 ), nil } tr . feed = feeder forest := mocks . BaselineForest ( t , true ) forest . HasFunc = func ( flow . StateCommitment ) bool { return updateCalled } st . forest = forest // The first call should not error but should not change the status of the FSM to updating. It should // instead remain Updating until a match is found. err := tr . UpdateTree ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusUpdate , st . status ) // The second call is now successful and matches. err = tr . UpdateTree ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusCollect , st . status ) }) t . Run ( \"nominal case with match\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusUpdate ) err := tr . UpdateTree ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusCollect , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) err := tr . UpdateTree ( st ) assert . Error ( t , err ) }) t . Run ( \"handles feeder update failure\" , func ( t * testing . T ) { t . Parallel () feed := mocks . BaselineFeeder ( t ) feed . UpdateFunc = func () ( * ledger . TrieUpdate , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusUpdate ) st . forest = mocks . BaselineForest ( t , false ) tr . feed = feed err := tr . UpdateTree ( st ) assert . Error ( t , err ) }) t . Run ( \"handles forest parent tree not found\" , func ( t * testing . T ) { t . Parallel () forest := mocks . BaselineForest ( t , false ) forest . TreeFunc = func ( _ flow . StateCommitment ) ( * trie . MTrie , bool ) { return nil , false } tr , st := baselineFSM ( t , StatusUpdate ) st . forest = forest err := tr . UpdateTree ( st ) assert . NoError ( t , err ) }) } func TestTransitions_CollectRegisters ( t * testing . T ) { t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () forest := mocks . BaselineForest ( t , true ) forest . ParentFunc = func ( commit flow . StateCommitment ) ( flow . StateCommitment , bool ) { assert . Equal ( t , mocks . GenericCommit ( 0 ), commit ) return mocks . GenericCommit ( 1 ), true } tr , st := baselineFSM ( t , StatusCollect ) st . forest = forest err := tr . CollectRegisters ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusMap , st . status ) for _ , wantPath := range mocks . GenericLedgerPaths ( 6 ) { assert . Contains ( t , st . registers , wantPath ) } }) t . Run ( \"indexing payloads disabled\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusCollect ) tr . cfg . SkipRegisters = true err := tr . CollectRegisters ( st ) require . NoError ( t , err ) assert . Empty ( t , st . registers ) assert . Equal ( t , StatusForward , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) err := tr . CollectRegisters ( st ) assert . Error ( t , err ) assert . Empty ( t , st . registers ) }) t . Run ( \"handles missing tree for commit\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusCollect ) st . forest = mocks . BaselineForest ( t , false ) err := tr . CollectRegisters ( st ) assert . Error ( t , err ) assert . Empty ( t , st . registers ) }) } func TestTransitions_MapRegisters ( t * testing . T ) { t . Run ( \"nominal case with registers to write\" , func ( t * testing . T ) { t . Parallel () // Path 2 and 4 are the same so the map effectively contains 5 entries. testRegisters := map [ ledger . Path ] * ledger . Payload { mocks . GenericLedgerPath ( 0 ): mocks . GenericLedgerPayload ( 0 ), mocks . GenericLedgerPath ( 1 ): mocks . GenericLedgerPayload ( 1 ), mocks . GenericLedgerPath ( 2 ): mocks . GenericLedgerPayload ( 2 ), mocks . GenericLedgerPath ( 1 ): mocks . GenericLedgerPayload ( 3 ), mocks . GenericLedgerPath ( 4 ): mocks . GenericLedgerPayload ( 4 ), mocks . GenericLedgerPath ( 5 ): mocks . GenericLedgerPayload ( 5 ), } write := mocks . BaselineWriter ( t ) write . PayloadsFunc = func ( height uint64 , paths [] ledger . Path , value [] * ledger . Payload ) error { assert . Equal ( t , mocks . GenericHeight , height ) // Expect the 5 entries from the map. assert . Len ( t , paths , 5 ) assert . Len ( t , value , 5 ) return nil } tr , st := baselineFSM ( t , StatusMap ) tr . write = write st . registers = testRegisters err := tr . MapRegisters ( st ) require . NoError ( t , err ) // Should not be StateIndexed because registers map was not empty. assert . Empty ( t , st . registers ) assert . Equal ( t , StatusMap , st . status ) }) t . Run ( \"nominal case no more registers left to write\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusMap ) err := tr . MapRegisters ( st ) assert . NoError ( t , err ) assert . Equal ( t , StatusForward , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () testRegisters := map [ ledger . Path ] * ledger . Payload { mocks . GenericLedgerPath ( 0 ): mocks . GenericLedgerPayload ( 0 ), mocks . GenericLedgerPath ( 1 ): mocks . GenericLedgerPayload ( 1 ), mocks . GenericLedgerPath ( 2 ): mocks . GenericLedgerPayload ( 2 ), mocks . GenericLedgerPath ( 3 ): mocks . GenericLedgerPayload ( 3 ), mocks . GenericLedgerPath ( 4 ): mocks . GenericLedgerPayload ( 4 ), mocks . GenericLedgerPath ( 5 ): mocks . GenericLedgerPayload ( 5 ), } tr , st := baselineFSM ( t , StatusBootstrap ) st . registers = testRegisters err := tr . MapRegisters ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure\" , func ( t * testing . T ) { t . Parallel () testRegisters := map [ ledger . Path ] * ledger . Payload { mocks . GenericLedgerPath ( 0 ): mocks . GenericLedgerPayload ( 0 ), mocks . GenericLedgerPath ( 1 ): mocks . GenericLedgerPayload ( 1 ), mocks . GenericLedgerPath ( 2 ): mocks . GenericLedgerPayload ( 2 ), mocks . GenericLedgerPath ( 3 ): mocks . GenericLedgerPayload ( 3 ), mocks . GenericLedgerPath ( 4 ): mocks . GenericLedgerPayload ( 4 ), mocks . GenericLedgerPath ( 5 ): mocks . GenericLedgerPayload ( 5 ), } write := mocks . BaselineWriter ( t ) write . PayloadsFunc = func ( uint64 , [] ledger . Path , [] * ledger . Payload ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusMap ) tr . write = write st . registers = testRegisters err := tr . MapRegisters ( st ) assert . Error ( t , err ) }) } func TestTransitions_ForwardHeight ( t * testing . T ) { t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () var ( firstCalled int lastCalled int ) write := mocks . BaselineWriter ( t ) write . FirstFunc = func ( height uint64 ) error { assert . Equal ( t , mocks . GenericHeight , height ) firstCalled ++ return nil } write . LastFunc = func ( height uint64 ) error { assert . Equal ( t , mocks . GenericHeight + uint64 ( lastCalled ), height ) lastCalled ++ return nil } forest := mocks . BaselineForest ( t , true ) forest . ResetFunc = func ( finalized flow . StateCommitment ) { assert . Equal ( t , mocks . GenericCommit ( 0 ), finalized ) } tr , st := baselineFSM ( t , StatusForward ) st . forest = forest tr . write = write err := tr . ForwardHeight ( st ) assert . NoError ( t , err ) assert . Equal ( t , StatusIndex , st . status ) assert . Equal ( t , mocks . GenericHeight + 1 , st . height ) // Reset status to allow next call. st . status = StatusForward err = tr . ForwardHeight ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusIndex , st . status ) assert . Equal ( t , mocks . GenericHeight + 2 , st . height ) // First should have been called only once. assert . Equal ( t , 1 , firstCalled ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) err := tr . ForwardHeight ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer error on first\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . FirstFunc = func ( uint64 ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusForward ) tr . write = write err := tr . ForwardHeight ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer error on last\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . LastFunc = func ( uint64 ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusForward ) tr . write = write err := tr . ForwardHeight ( st ) assert . Error ( t , err ) }) } func TestTransitions_InitializeMapper ( t * testing . T ) { t . Run ( \"switches state to BootstrapState if configured to do so\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusInitialize ) tr . cfg . BootstrapState = true err := tr . InitializeMapper ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusBootstrap , st . status ) }) t . Run ( \"switches state to StatusResume if no bootstrapping configured\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusInitialize ) tr . cfg . BootstrapState = false err := tr . InitializeMapper ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusResume , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusForward ) err := tr . InitializeMapper ( st ) require . Error ( t , err ) }) } func TestTransitions_ResumeIndexing ( t * testing . T ) { header := mocks . GenericHeader tree := mocks . GenericTrie commit := flow . StateCommitment ( tree . RootHash ()) differentCommit := mocks . GenericCommit ( 0 ) t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } writer := mocks . BaselineWriter ( t ) writer . FirstFunc = func ( height uint64 ) error { assert . Equal ( t , header . Height , height ) return nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( height uint64 ) ( flow . StateCommitment , error ) { assert . Equal ( t , header . Height , height ) return commit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withWriter ( writer ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusIndex , st . status ) assert . Equal ( t , header . Height + 1 , st . height ) assert . Equal ( t , flow . DummyStateCommitment , st . last ) assert . Equal ( t , commit , st . next ) }) t . Run ( \"handles chain failure on Root\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return 0 , mocks . GenericError } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure on First\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } writer := mocks . BaselineWriter ( t ) writer . FirstFunc = func ( uint64 ) error { return mocks . GenericError } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusResume , withWriter ( writer ), withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles reader failure on Last\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return 0 , mocks . GenericError } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles reader failure on Commit\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return flow . DummyStateCommitment , mocks . GenericError } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles loader failure on Trie\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return nil , mocks . GenericError } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles mismatch between tree root hash and indexed commit\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return differentCommit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusForward , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) } func baselineFSM ( t * testing . T , status Status , opts ... func ( tr * Transitions )) ( * Transitions , * State ) { t . Helper () load := mocks . BaselineLoader ( t ) chain := mocks . BaselineChain ( t ) feeder := mocks . BaselineFeeder ( t ) read := mocks . BaselineReader ( t ) write := mocks . BaselineWriter ( t ) forest := mocks . BaselineForest ( t , true ) once := & sync . Once {} doneCh := make ( chan struct {}) tr := Transitions { cfg : Config { BootstrapState : false , SkipRegisters : false , WaitInterval : 0 , }, log : mocks . NoopLogger , load : load , chain : chain , feed : feeder , read : read , write : write , once : once , } for _ , opt := range opts { opt ( & tr ) } st := State { forest : forest , status : status , height : mocks . GenericHeight , last : mocks . GenericCommit ( 1 ), next : mocks . GenericCommit ( 0 ), registers : make ( map [ ledger . Path ] * ledger . Payload ), done : doneCh , } return & tr , & st } func withLoader ( load Loader ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . load = load } } func withChain ( chain dps . Chain ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . chain = chain } } func withFeeder ( feed Feeder ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . feed = feed } } func withReader ( read dps . Reader ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . read = read } } func withWriter ( write dps . Writer ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . write = write } }","title":"Case Study: The Flow DPS Mapper"},{"location":"code/testing/#integration-tests","text":"Integration tests are essential to ensure that components work together as expected. Those tests are usually much heavier and slower than unit tests, since they use real components instead of simple mocks, and often might run filesystem or network operations, wait for things to happen, or even run heavy computational tasks. Integration tests should always be specified in a separate test package and never run internally within the tested package.","title":"Integration Tests"},{"location":"code/testing/#build-tag","text":"Because integration tests are inherently slower than unit tests, they are placed in specific files that are suffixed with _integration_test.go and those files start with a build tag directive which prevents them from running unless the go test command is called with the integration tag. Both syntaxes should be specified, the <go1.17 one which is +build <tag> as well as the >=go1.17 one which is go:build <tag> . The former will be dropped when we feel like it is no longer relevant to support go 1.16 and prior. //go:build integration // +build integration package dps_test","title":"Build Tag"},{"location":"code/testing/#examples","text":"In Go, good package documentation includes not only comments for each public type and method, but also runnable examples and benchmarks in some cases. Godoc allows defining examples which are verified by running them as tests and can be manually launched by readers of the documentation on the package's Godoc webpage. As for typical tests, examples are functions that reside in a package's _test.go files. Unlike normal test functions, though, example functions take no arguments and begin with the word Example instead of Test . In order to specify what is the expected output of a given example, a comment has to be written at the end of the Example function, in the form of // Output: <expected output> . If this is missing, examples will not be executed and therefore not included in the documentation.","title":"Examples"},{"location":"code/testing/#benchmarks","text":"When a package exposes a performance-critical piece of code, it should be benchmarked, and benchmark tests must be available for anyone to reproduce the benchmark using their hardware. Writing benchmark results in a markdown file without providing a way to reproduce them is irrelevant.","title":"Benchmarks"},{"location":"flare/architecture/","text":"Flare Architecture This section contains information about some parts of the Flare Network. In addition, it contains some information regarding the Avalanche platform since Flare adopted some of its parts within its ecosystem. Avalanche Platform Avalanche is an open-source platform for launching decentralized applications and enterprise blockchain deployments in one interoperable, highly scalable ecosystem. It is a platform with a new approach to scaling that allows for fast finality of transactions and the ability to run Solidity out-of-the-box. The most significant difference between Avalanche and other decentralized networks is the consensus protocol . The Avalanche protocol employs a novel approach to a consensus to achieve strong safety guarantees, quick finality, and high throughput without compromising decentralization. The purpose of the Avalanche platform: creation of subnets and blockchains tied to the main Avalanche network for general and corporate purposes; creation and hosting of decentralized applications; creation of smart assets (for example, new financial derivatives and DeFi solutions). Subnets Avalanche is composed of logical entities called \"subnets,\" and every chain from Avalanche blockchain is a part of a subnet . A subnet, or subnetwork, is a dynamic set of validators working together to achieve consensus on the state of a set of blockchains. Each blockchain is validated by one subnet, but a subnet can validate many blockchains. The validators of each subnet must stake AVAX tokens to become a member of the network. Clients interact with Avalanche through API calls to nodes (each chain has its own API ). There is a particular subnet called the primary network , which validates Avalanche's built-in blockchains. A node may be a member of arbitrarily many subnets and must be part of the primary network. The primary network contains three build-in blockchains: P-Chain (Platform Chain), C-Chain (Contract Chain), and X-Chain (Exchange Chain). In total, the Avalanche ecosystem (as of the end of December 2021) operates 13 subnets and 10 blockchains (3 main plus 7 minor ones: Dtehereum, Btehereum, C and X-chain PoW, and others). All the details regarding the platform operation, tutorials, and tools are described in the official Avalanche website P-Chain The Platform Chain is the metadata blockchain on Avalanche that coordinates validators, keeps track of active subnets, and enables the creation of new subnets. The P-Chain uses a UTXO (Unspent Transaction Output) model and implements the Snowman consensus protocol. Adding or removing a validator from the validator set at some given time by the P-Chain needs absolute ordering, so parallel lines of computation (DAG approach) are not allowed. C-Chain With the C-Chain API, clients can create smart contracts. It is an example of the Ethereum Virtual Machine that the Avalanche Chain powers. The C-Chain also serves as the default smart contract blockchain of the Avalanche Chain. C-Chain is also very compatible with the Solidity smart contract and Ethereum tooling. Therefore, it makes it easy for Ethereum developers to port applications into the Avalanche chain easily. C-Chain uses an account-based Snowman Consensus Protocol since an application requires a total ordering of transactions and runs consensus on blocks, not transactions. That is, if it needs to be able to compare any two transactions and determine which came first, it can't use a DAG because the ordering of some transactions is not defined. X-Chain The X-Chain functions as a decentralized network for creating and trading digital assets and uses the Avalanche consensus protocol. The X-Chain uses a DAG with the full Avalanche consensus protocol, so it is UTXO (Unspent Transaction Output) based. UTXO-based systems are inherently more scalable than account-based systems. They are naturally parallelizable, whereas account-based systems must lock around the account to process transactions. It is a real-world representation of assets such as bonds and equity with specific rules that govern their actions. Whenever you initiate a transaction on the Avalanche blockchain, you get to pay a fee in AVAX. The Exchange Chain is an Avalanche Virtual Machine (AVM) example. Clients can create and trade assets on the X-Chain and other examples of the AVM with the help of the X-Chain API. Flare's FBA TODO add more details to each of the items Currently, Flare uses some parts of the Avalanche codebase with some changes: deactivate most of the P-Chain and X-Chain functionality; run the C-Chain with a locally defined set of validators instead of using the P-Chain for the list of validators; hijack the EVM state transition to implement Flare state connector functionality; implement the rest of the Flare systems on top of the C-Chain as Solidity smart contracts. State Connector The state connector in Flare's system is designed to observe the state of underlying chains and is one of the most critical protocols on the Flare Network. The state connector enables Flare smart contracts to act on state changes from underlying blockchains, for example, payments. The state connector adopts other blockchains' consensus mechanisms and validator ecosystems. Advantages The state connector system is a competitive approach for proving the state of an underlying chain to a smart contract, and it has the following advantages: Transaction validity references back to an underlying chain's genesis block: Other approaches like the SPV (Simplified Payment Verification) proof do not check the validity of a transaction because it is considered harmful for proving the state of an underlying chain to another network. Safety only depends on an underlying chain's validators: no trusted third-party service has its own set of economic incentives and risks. The need for trust is minimized by leveraging the guarantee that safety can only be lost in the state connector if an underlying chain's validators encounter a Byzantine fault. No cooperation is needed from an underlying chain's validators: validators from an underlying chain do not need to modify their chain's codebase to permit Flare to interpret their network. An underlying chain's validators do not even need to know that Flare exists for the state connector system to operate. Can read the state of any blockchain: the state connector can operate on any possible Sybil-resistance technique of an underlying chain. Constant-sized proofs: both the data availability proof and the payment proof are constant-sized, independent of the number of other payments in the data availability period being considered. Voting The state connector voting protocol is split into three phases, described in the following sub-sections. Request Phase Any user can submit a request to the state connector contract to have an event proven at any point in time. From its perspective, the time window that this request enters the network state is known as the request phase. Commit Phase During the next window, attestation providers have the opportunity to commit a hidden vote regarding their belief in the outcome of the events requested in the previous phase. Anyone may operate as an attestation provider without any capital requirement, but a default incentivized set is used as the minimal requirement for passing a vote about the events in the previous set. Reveal Phase Finally, in the next window of time, attestation providers reveal the votes they committed to in the previous round. Once this reveal phase concludes and the next phase begins, the revealed votes are automatically counted, and all valid events become immediately available to all contracts on Flare. Once these criteria are met, and that data is available, the stored transactions in the storage contract are now available to be referenced by any other contract on the Flare Network. Branching The state connector branching protocol protects Flare against the incorrect interpretation of real-world events proactively, such that there are never any rollbacks on the Flare blockchain state. Instead of having rollbacks, contention on state correctness is handled via automatic state branching into a correct and incorrect path. The security assumption is that if you, as an independent node operator, follow along with the correct real-world state, you will always end up on the correct branch of the blockchain state. Flare Time Series Oracle (FTSO) The FTSO provides externally sourced data estimates (e.g., the current price of XRP) to the Flare Network in a decentralized manner. It does so by leveraging the distributed nature of the network and its participants. At the moment, only Spark holders get to vote; in the future, each asset's voting power will be split 50-50 between Spark holders and the holders of the particular asset. Signal providers on the Flare Network are responsible for delivering continuous and accurate data estimates to the FTSO to represent off-chain values on the Flare Network. Signal providers exist in the off-chain to collect data, such as asset prices, and deliver it to the FTSO. For example, a provider may collect the price of XRP/$ from three different exchanges, average the price and have this price estimate ready for the FTSO every voting round to submit. Signal providers can provide their F-Asset and FLR votes to do this, but they also can receive delegated votes from other parties to perform this service on their behalf. Signal providers may take fees for this service. Additionally, the entire process is noncustodial (i.e., participants retain complete control of their data). The FTSO ensures the data is fair, accurate, and available to network applications. In a nutshell, signal providers submit their votes, which results in a weighted distribution regarding prices at a given time, where the weight is related to the number of tokens held by a given address. Next, the top and bottom 25% of these votes are deleted, resulting in a truncated distribution. The oracle estimate is computed as the median of this truncated distribution. Signal providers get rewarded with FLR tokens if their vote remained in the distribution after truncation. The signal provider is then responsible for distributing this award between itself and its delegators. Additional info and implementation details are available in this example of price provider . Delegation Anyone can submit price data to the FTSO and vote for their price estimate. Price submission may require effort and technical skills to build such a provider. Most Spark and F-Asset holders can delegate their tokens to an FTSO provider whom they trust will provide accurate price estimates. The delegation will be as simple as logging into a Flare wallet, choosing an FTSO provider, and finally selecting the amount tokens they wish to delegate. Rewards Signal providers receive a reward for accurately voting on prices, as determined by the algorithm. The top and bottom 25% of prices are discarded, and the remaining average is used. The 50% of price providers who are closest to the price then receive rewards for their tokens plus the percentage of fees, as defined on the smart contract. The default is 20%, and they can change it in 5% increments/decrements. The exact amount of compensation is based on the number of delegated tokens minus a fee set by the FTSO provider. For example, Alice, Bob, and Charlie choose to delegate their Spark tokens, 1000, 500 & 800 respectively, for a total of 2,300 Spark delegated to FTSO Provider X . FTSO Provider X submits their price estimate for the round and wins using their proprietary price estimate. Say 100 Spark is awarded as compensation to FTSO Provider X . The provider will take 10 Spark as a fee (10%) and divide the rest of the 90 tokens to the delegators based on their stake ratio (delegators stake / total Spark delegated to FTSO Provider X ). Thus, Alice gets 43.5% (39.15 Spark), Bob gets 21.7% (19.53 Spark) and Charlie gets 34.8% (31.32 Spark). Note: only Spark delegators are compensated, F-Asset votes are not compensated by the FTSO. F-Asset System The F-Asset System is a critical application on the Flare Network. It facilitates the trustless issuance of asset values onto the Flare Network. An F-Asset represents an asset from another blockchain on the Flare Network via the state connector system, i.e., XRP = FXRP. Agents are responsible for posting and maintaining the FLR collateral. Additionally, they are responsible for answering redemptions when users decide to have their underlying asset back. XRP, Litecoin, Dogecoin, and Stellar are the first four F-Assets, and the addition of more is subject to FLR holder governance. The F-Asset system gives the supported cryptos the capability to run smart contracts without changing the underlying blockchain. The reason is that nearly 65% of the value on public blockchains cannot be used in a trustless manner with smart contracts. Two important parties in the F-Asset System are responsible for issuing F-Assets: agents and originators . An originator is a person or an institution that wants to convert its cryptocurrency into an F-asset. An agent (more likely to be an institution since they usually hold more assets) is a participant on the Flare network that issues the F-asset desired by the originator. Agents are also responsible for posting and managing the FLR collateral used to back the minting of F-Assets. Additionally, as F-Asset holders have the right to claim the underlying asset, the agents are responsible for honoring redemptions promptly. In the chance of the F-Asset position becoming undercollateralized, the F-Asset System encourages other agents to re-collateralize a failing agent's position via incentives. If a default were to occur on an F-Asset position, the originator of the F-Assets would be paid out the value of their position in FLR plus an additional amount to cover trading fees to exchange back into their underlying asset. These external resources contain more detailed information about F-Asset System The F-Asset System - Smart Contracts for Everyone F-Asset System","title":"Architecture"},{"location":"flare/architecture/#flare-architecture","text":"This section contains information about some parts of the Flare Network. In addition, it contains some information regarding the Avalanche platform since Flare adopted some of its parts within its ecosystem.","title":"Flare Architecture"},{"location":"flare/architecture/#avalanche-platform","text":"Avalanche is an open-source platform for launching decentralized applications and enterprise blockchain deployments in one interoperable, highly scalable ecosystem. It is a platform with a new approach to scaling that allows for fast finality of transactions and the ability to run Solidity out-of-the-box. The most significant difference between Avalanche and other decentralized networks is the consensus protocol . The Avalanche protocol employs a novel approach to a consensus to achieve strong safety guarantees, quick finality, and high throughput without compromising decentralization. The purpose of the Avalanche platform: creation of subnets and blockchains tied to the main Avalanche network for general and corporate purposes; creation and hosting of decentralized applications; creation of smart assets (for example, new financial derivatives and DeFi solutions).","title":"Avalanche Platform"},{"location":"flare/architecture/#subnets","text":"Avalanche is composed of logical entities called \"subnets,\" and every chain from Avalanche blockchain is a part of a subnet . A subnet, or subnetwork, is a dynamic set of validators working together to achieve consensus on the state of a set of blockchains. Each blockchain is validated by one subnet, but a subnet can validate many blockchains. The validators of each subnet must stake AVAX tokens to become a member of the network. Clients interact with Avalanche through API calls to nodes (each chain has its own API ). There is a particular subnet called the primary network , which validates Avalanche's built-in blockchains. A node may be a member of arbitrarily many subnets and must be part of the primary network. The primary network contains three build-in blockchains: P-Chain (Platform Chain), C-Chain (Contract Chain), and X-Chain (Exchange Chain). In total, the Avalanche ecosystem (as of the end of December 2021) operates 13 subnets and 10 blockchains (3 main plus 7 minor ones: Dtehereum, Btehereum, C and X-chain PoW, and others). All the details regarding the platform operation, tutorials, and tools are described in the official Avalanche website","title":"Subnets"},{"location":"flare/architecture/#p-chain","text":"The Platform Chain is the metadata blockchain on Avalanche that coordinates validators, keeps track of active subnets, and enables the creation of new subnets. The P-Chain uses a UTXO (Unspent Transaction Output) model and implements the Snowman consensus protocol. Adding or removing a validator from the validator set at some given time by the P-Chain needs absolute ordering, so parallel lines of computation (DAG approach) are not allowed.","title":"P-Chain"},{"location":"flare/architecture/#c-chain","text":"With the C-Chain API, clients can create smart contracts. It is an example of the Ethereum Virtual Machine that the Avalanche Chain powers. The C-Chain also serves as the default smart contract blockchain of the Avalanche Chain. C-Chain is also very compatible with the Solidity smart contract and Ethereum tooling. Therefore, it makes it easy for Ethereum developers to port applications into the Avalanche chain easily. C-Chain uses an account-based Snowman Consensus Protocol since an application requires a total ordering of transactions and runs consensus on blocks, not transactions. That is, if it needs to be able to compare any two transactions and determine which came first, it can't use a DAG because the ordering of some transactions is not defined.","title":"C-Chain"},{"location":"flare/architecture/#x-chain","text":"The X-Chain functions as a decentralized network for creating and trading digital assets and uses the Avalanche consensus protocol. The X-Chain uses a DAG with the full Avalanche consensus protocol, so it is UTXO (Unspent Transaction Output) based. UTXO-based systems are inherently more scalable than account-based systems. They are naturally parallelizable, whereas account-based systems must lock around the account to process transactions. It is a real-world representation of assets such as bonds and equity with specific rules that govern their actions. Whenever you initiate a transaction on the Avalanche blockchain, you get to pay a fee in AVAX. The Exchange Chain is an Avalanche Virtual Machine (AVM) example. Clients can create and trade assets on the X-Chain and other examples of the AVM with the help of the X-Chain API.","title":"X-Chain"},{"location":"flare/architecture/#flares-fba","text":"TODO add more details to each of the items Currently, Flare uses some parts of the Avalanche codebase with some changes: deactivate most of the P-Chain and X-Chain functionality; run the C-Chain with a locally defined set of validators instead of using the P-Chain for the list of validators; hijack the EVM state transition to implement Flare state connector functionality; implement the rest of the Flare systems on top of the C-Chain as Solidity smart contracts.","title":"Flare's FBA"},{"location":"flare/architecture/#state-connector","text":"The state connector in Flare's system is designed to observe the state of underlying chains and is one of the most critical protocols on the Flare Network. The state connector enables Flare smart contracts to act on state changes from underlying blockchains, for example, payments. The state connector adopts other blockchains' consensus mechanisms and validator ecosystems.","title":"State Connector"},{"location":"flare/architecture/#advantages","text":"The state connector system is a competitive approach for proving the state of an underlying chain to a smart contract, and it has the following advantages: Transaction validity references back to an underlying chain's genesis block: Other approaches like the SPV (Simplified Payment Verification) proof do not check the validity of a transaction because it is considered harmful for proving the state of an underlying chain to another network. Safety only depends on an underlying chain's validators: no trusted third-party service has its own set of economic incentives and risks. The need for trust is minimized by leveraging the guarantee that safety can only be lost in the state connector if an underlying chain's validators encounter a Byzantine fault. No cooperation is needed from an underlying chain's validators: validators from an underlying chain do not need to modify their chain's codebase to permit Flare to interpret their network. An underlying chain's validators do not even need to know that Flare exists for the state connector system to operate. Can read the state of any blockchain: the state connector can operate on any possible Sybil-resistance technique of an underlying chain. Constant-sized proofs: both the data availability proof and the payment proof are constant-sized, independent of the number of other payments in the data availability period being considered.","title":"Advantages"},{"location":"flare/architecture/#voting","text":"The state connector voting protocol is split into three phases, described in the following sub-sections.","title":"Voting"},{"location":"flare/architecture/#request-phase","text":"Any user can submit a request to the state connector contract to have an event proven at any point in time. From its perspective, the time window that this request enters the network state is known as the request phase.","title":"Request Phase"},{"location":"flare/architecture/#commit-phase","text":"During the next window, attestation providers have the opportunity to commit a hidden vote regarding their belief in the outcome of the events requested in the previous phase. Anyone may operate as an attestation provider without any capital requirement, but a default incentivized set is used as the minimal requirement for passing a vote about the events in the previous set.","title":"Commit Phase"},{"location":"flare/architecture/#reveal-phase","text":"Finally, in the next window of time, attestation providers reveal the votes they committed to in the previous round. Once this reveal phase concludes and the next phase begins, the revealed votes are automatically counted, and all valid events become immediately available to all contracts on Flare. Once these criteria are met, and that data is available, the stored transactions in the storage contract are now available to be referenced by any other contract on the Flare Network.","title":"Reveal Phase"},{"location":"flare/architecture/#branching","text":"The state connector branching protocol protects Flare against the incorrect interpretation of real-world events proactively, such that there are never any rollbacks on the Flare blockchain state. Instead of having rollbacks, contention on state correctness is handled via automatic state branching into a correct and incorrect path. The security assumption is that if you, as an independent node operator, follow along with the correct real-world state, you will always end up on the correct branch of the blockchain state.","title":"Branching"},{"location":"flare/architecture/#flare-time-series-oracle-ftso","text":"The FTSO provides externally sourced data estimates (e.g., the current price of XRP) to the Flare Network in a decentralized manner. It does so by leveraging the distributed nature of the network and its participants. At the moment, only Spark holders get to vote; in the future, each asset's voting power will be split 50-50 between Spark holders and the holders of the particular asset. Signal providers on the Flare Network are responsible for delivering continuous and accurate data estimates to the FTSO to represent off-chain values on the Flare Network. Signal providers exist in the off-chain to collect data, such as asset prices, and deliver it to the FTSO. For example, a provider may collect the price of XRP/$ from three different exchanges, average the price and have this price estimate ready for the FTSO every voting round to submit. Signal providers can provide their F-Asset and FLR votes to do this, but they also can receive delegated votes from other parties to perform this service on their behalf. Signal providers may take fees for this service. Additionally, the entire process is noncustodial (i.e., participants retain complete control of their data). The FTSO ensures the data is fair, accurate, and available to network applications. In a nutshell, signal providers submit their votes, which results in a weighted distribution regarding prices at a given time, where the weight is related to the number of tokens held by a given address. Next, the top and bottom 25% of these votes are deleted, resulting in a truncated distribution. The oracle estimate is computed as the median of this truncated distribution. Signal providers get rewarded with FLR tokens if their vote remained in the distribution after truncation. The signal provider is then responsible for distributing this award between itself and its delegators. Additional info and implementation details are available in this example of price provider .","title":"Flare Time Series Oracle (FTSO)"},{"location":"flare/architecture/#delegation","text":"Anyone can submit price data to the FTSO and vote for their price estimate. Price submission may require effort and technical skills to build such a provider. Most Spark and F-Asset holders can delegate their tokens to an FTSO provider whom they trust will provide accurate price estimates. The delegation will be as simple as logging into a Flare wallet, choosing an FTSO provider, and finally selecting the amount tokens they wish to delegate.","title":"Delegation"},{"location":"flare/architecture/#rewards","text":"Signal providers receive a reward for accurately voting on prices, as determined by the algorithm. The top and bottom 25% of prices are discarded, and the remaining average is used. The 50% of price providers who are closest to the price then receive rewards for their tokens plus the percentage of fees, as defined on the smart contract. The default is 20%, and they can change it in 5% increments/decrements. The exact amount of compensation is based on the number of delegated tokens minus a fee set by the FTSO provider. For example, Alice, Bob, and Charlie choose to delegate their Spark tokens, 1000, 500 & 800 respectively, for a total of 2,300 Spark delegated to FTSO Provider X . FTSO Provider X submits their price estimate for the round and wins using their proprietary price estimate. Say 100 Spark is awarded as compensation to FTSO Provider X . The provider will take 10 Spark as a fee (10%) and divide the rest of the 90 tokens to the delegators based on their stake ratio (delegators stake / total Spark delegated to FTSO Provider X ). Thus, Alice gets 43.5% (39.15 Spark), Bob gets 21.7% (19.53 Spark) and Charlie gets 34.8% (31.32 Spark). Note: only Spark delegators are compensated, F-Asset votes are not compensated by the FTSO.","title":"Rewards"},{"location":"flare/architecture/#f-asset-system","text":"The F-Asset System is a critical application on the Flare Network. It facilitates the trustless issuance of asset values onto the Flare Network. An F-Asset represents an asset from another blockchain on the Flare Network via the state connector system, i.e., XRP = FXRP. Agents are responsible for posting and maintaining the FLR collateral. Additionally, they are responsible for answering redemptions when users decide to have their underlying asset back. XRP, Litecoin, Dogecoin, and Stellar are the first four F-Assets, and the addition of more is subject to FLR holder governance. The F-Asset system gives the supported cryptos the capability to run smart contracts without changing the underlying blockchain. The reason is that nearly 65% of the value on public blockchains cannot be used in a trustless manner with smart contracts. Two important parties in the F-Asset System are responsible for issuing F-Assets: agents and originators . An originator is a person or an institution that wants to convert its cryptocurrency into an F-asset. An agent (more likely to be an institution since they usually hold more assets) is a participant on the Flare network that issues the F-asset desired by the originator. Agents are also responsible for posting and managing the FLR collateral used to back the minting of F-Assets. Additionally, as F-Asset holders have the right to claim the underlying asset, the agents are responsible for honoring redemptions promptly. In the chance of the F-Asset position becoming undercollateralized, the F-Asset System encourages other agents to re-collateralize a failing agent's position via incentives. If a default were to occur on an F-Asset position, the originator of the F-Assets would be paid out the value of their position in FLR plus an additional amount to cover trading fees to exchange back into their underlying asset. These external resources contain more detailed information about F-Asset System The F-Asset System - Smart Contracts for Everyone F-Asset System","title":"F-Asset System"},{"location":"flare/consensus/","text":"Consensus Algorithms This document explains, at a high level, what are the mechanisms behind the different consensus algorithms and how they evolved. After reading it, the engineer should understand the ideas behind Flare's consensus mechanisms. FBA and Separating Sybil Attack Resistance from Consensus by Making it a Local Selection Consensus A consensus algorithm is an agreement system that follows a predefined set of steps for a group of participants (nodes) to reach the same decision about a statement. In a blockchain network nodes repeatedly decide what the complete history of a shared ledger is, from among multiple possible versions that occasionally conflict with each other. This network-wide agreement allows the recipient of a crypto coin to have faith that the coin is both valid (not counterfeit) and not already spent elsewhere. Ripple Ripple consensus is a voting consensus that works with nodes voting for each of its rounds. The nodes that run the Ripple Server software and participate in the consensus protocol are called servers. Each of them maintains a list of other servers \u2014 Unique Node List (UNL), that it queries for each validation round. The UNL is not a list of all servers in the system, but rather only a snapshot that the node knows about. Additionally, no participant knows what is the exact number of nodes in the system at any give time and does not need to. Each server queries the nodes from its UNL and receives their votes and depending on what the majority (a defined percentage of them) of the list votes on a transaction, it is accepted or rejected. The record of confirmed transaction history by the network is the ledger. It is kept in each user account and represents the ground truth of the network. The last-closed ledger is the most recent ledger that has been ratified by the consensus process and thus represents the current state of the network, as opposed to the \"open ledger\", which is the current operating ledger status of each node. When transactions are sent from the users they become part of the open ledger for each node and are included in the last-closed ledger after they pass through the consensus process. Each server includes every valid transaction it knows about in its broadcast messages to others when the consensus round starts, creating a list of transactions called a candidate set . Each server queries the nodes from its UNL when executing consensus and only the votes from UNL members are considered. The UNL is trustful only as a whole, meaning the set is expected not to collude in an attempt to defraud the network, but every single member of it is not implicitly considered trustful. Transactions with more than a certain percentage of no -votes are either discarded or included in the next candidate set for the next consensus round. If at least 80% of a UNL list agrees on a transaction, it is applied to the ledger. After the servers exchange their candidate sets, each of them unites its own with the ones received from its UNL peers, and on the next step servers then vote on the accuracy of the transactions. When servers vote, the process is a simple binary decision problem: deciding on a value 0 or 1 for the given transaction. The assumptions that define the Ripple consensus are: Every well-behaved node makes its decision for a finite time All well-behaved nodes reach the same decision value 0 and 1 are both possible values for all well-behaved nodes The goal is reaching a consensus amongst all nodes and preventing a situation, in which two disjoint sets of nodes each reach a consensus independently thus having two last-closed ledgers. The consensus algorithm is applied every few seconds by all nodes and once the consensus is reached \u2014 the ledger is considered closed and all well-behaved nodes will have identical ledger history. Each of the servers can join or leave the network at some point, which means that in some voting rounds some servers will have different nodes in their UNL lists. The notion of majority rule is applied on the UNL set requirement for agreement, but a hard majority cannot be applied, because the total number of servers in the system is unknown. As a result, there is no hard number of voters for any of the participants and existing groups of validating nodes overlap with each other. This is a version of a consensus algorithm running in a federated manner, where each node knows about some number of others validating nodes, but not about all nodes in the network. Stellar and the Federated Byzantine Agreement Introduction Any agreement system in a distributed computing network needs to be fault-tolerant. This means it must produce consistent results despite errors like slow communication links, unresponsive nodes, and misordered messages. A Byzantine agreement system is additionally tolerant of Byzantine faults : nodes that give false information, whether due to error or in a deliberate attempt to subvert the system or gain some advantage. Such a system prevents the double-spending problem by using a form of majority called quorum . A node in such a network refuses to commit to a particular version of the ledger history until it sees that enough of its peers are also prepared to commit to it. The number of peers that is enough to make a decision is the quorum and once each of the peers commits to a value after its quorum had committed, the system forms a large enough voting block to force the remaining nodes to agree with the decision. In such a system, attempts from malicious nodes to create double-spending events, or in another way invalid transactions, are overwhelmed by the network votes from honest nodes. For a hard majority to be formed, the number of all nodes in the system must be known and its quorum calculated at any time. Opposed to this concept, in a Federated Byzantine Agreement (FBA) system, the participants in the network are loosely defined and can join and leave at any time without coordinating with any central authority. Quorums are incomplete membership snapshots that can change at any time and consensus is reached in a decentralized form. This makes the network consensus probabilistic with some uncertainty level (probability of not reaching a consensus not equal to zero) present at all times. When making a decision, a node in an FBA system waits for its quorum nodes to agree on a value before considering a transaction settled. Each node is potentially a member of another node's quorum and eventually when enough network quorums accept a transaction, it becomes infeasible to be rolled back. On a higher level, the structure looks like multiple overlapping groups of participants, that function as a federation of groups, reaching consensus in a decentralized way, therefore the term federated . This structure brings a high level of decentralization into decision-making process among network members and mimics the behavior of social groups spreading a piece of information. FBA concept is defined as the Stellar Consensus Protocol (SCP) . Quorum Slices In an FBA consensus protocol implementation, nodes exchange messages asserting statements. When a node hears a sufficient set of nodes assert one statement, it assumes no functioning node will ever contradict that statement. This sufficient set of nodes is called a quorum slice or just a slice . A node in an FBA network refuses to commit to a particular value until it sees its peers are also committing to it. Since in FBA the collections of participants are loosely defined, quorums are dynamically defined from an ever-changing and inevitably incomplete snapshot of membership. Each node has multiple slices, any of which is sufficient to convince it of a value, so we can say that an FBA system consists of a loose federation of nodes each of which has chosen one or more slices to serve as a benchmark for making decisions. To form a quorum slice, a node adds all nodes it knows about to its quorum slice, as well as all nodes' peers. Continuing this process leads to encountering more and more nodes, and when no new nodes are found the quorum is defined. This process of dynamically creating quorums uses the transitive closure property of the network. With the help of this property, in an FBA system, every node is potentially connected by its indirect connections to other nodes. Nodes know about other nodes' quorum slices, in the same manner, they know the rest of the information about their network peers \u2014 from the broadcast information that every node sends to the network whenever its' voting state changes. Each broadcast message includes the details of the sending node's IP and its quorum slices nodes \u2014 a typical implementation used in FBA systems (and generally in other blockchains) is a gossip protocol . FBA and Stellar Implementation Stellar Consensus Protocol (SCP) is used to build Stellar Payment Network and implemented FBA as an essential building block of its system. FBA systems create confidence in the outcome of a decentralized vote by running a process called \"federated voting\". This is a procedure for discovering whether a network of participants can agree on a proposal. In the cryptocurrency world a proposal is a ledger state with included transactions. In a round of federated voting, each node chooses one of the potentially many possible values as an outcome of the round, but it first needs to confirm the nodes from its quorum will choose the same outcome. The Stellar network's nodes conduct multiple federated votes on multiple values until some of them are accepted. Since it is a payment network, it is logical to assume consensus needs to be reached on ledger values, but in fact, it is on \"statements\" about these values. In an open system, someone can gain influence on the network by conducting a so-called Sybil attack . In short, a Sybil attack is an attempt to control a peer network by creating multiple fake identities that appear as unique peers to the outside world. For the federated voting to be successful, Stellar's solution is to require a network property called quorum intersection . This means that any two quorums that can be constructed in the network will always share at least one node after all Byzantine nodes are excluded. For determining the prevailing sentiment of the network, this is as good as having a majority. Intuitively, if any quorum agrees to statement X , no other quorum can ever agree to not-X , because it will necessarily include some node from the first quorum that has already voted for X . This is the transitive closure property that these types of systems typically use to fight against Sybil attacks. A living example of such a network is the Internet itself. It consists of independent nodes that know about a few other nodes local to them, but the sets overlap with each other and every node is reachable through any other by routes and gateways. In the same manner, an FBA network resembles inter-domain network routing with no central authority dictating or arbitraging agreements. Quorum slices can be perceived as analogous to internet networks in terms of reachability. Servers in the internet networks can be reached via requests from neighboring networks, using gateways and hubs, making each server accessible through any client. In quorums, nodes are reachable through their neighboring nodes, which might be in another quorum, but because of the nodes' connections between each other, each node can be reached theoretically from any other. The Internet's nearly complete transitive reachability suggests it can likewise ensure a worldwide consensus with FBA. Voting in Stellar consists of two phases: nomination and balloting. At the beginning of the nomination phase, a node randomly votes for a statement on a ledger version it receives from another node in the network. The nodes cast out their nomination votes via messages to their peers, as well as the nominations of their peers, meaning separate federated votes are batched together. Voting against a value is not defined and a node can nominate as many statements as it likes so after the FBA process is run on each node, a nominee is confirmed, and it becomes a candidate . The nomination may produce multiple confirmable candidates, so SCP requires the application layer to supply some method for combining them into a single composite, with the only requirement for this method to be deterministic. SCP is an asynchronous protocol and nodes are not coordinated by time, but by the messages they exchange. Thus, from a node\u2019s perspective, it is not clear when the nomination process has ended. During the nomination, it cannot be said which of the composites is the final one, but this is expected because the purpose is to produce several virtuous candidates for the balloting. In the balloting phase nodes exchange their ballots on created composites and repeatedly conduct federated votes on statements about ballots. From the point of view of a given node, a consensus is reached when it finds a ballot for which it can confirm (find a quorum accepting) a statement, and then it can be sure that every other well-behaved node has committed, or inevitably will commit to, the same value. Nodes can be categorized as either well-behaved or ill-behaved. A well-behaved node chooses sensible quorum slices and obeys the protocol, including eventually responding to all requests. Ill-behaved nodes do not follow the protocol, can suffer Byzantine failure, and behave arbitrarily. Since the blockchain network's most important goal is to reach consensus deterministically, nodes that generally act arbitrarily (non-deterministically) cannot be considered functioning parts of the system. The expected behavior of well-behaved nodes is always deterministic in the sense that they choose the same right value among others every time. The goal of the Byzantine agreement is to ensure that well-behaved nodes agree on the same values despite the presence of the ill-behaved nodes. The first part of this goal is to prevent nodes from diverging and committing different values for the same ledger. The second part is to ensure the nodes will always go through the consensus process, as opposed to getting blocked in some dead-state. The first property corresponds to the notion of safety, while the second one corresponds to the notion of liveness. FBA networks implementations reach consensus in a decentralized, but the deterministic way and are the next step in building safe and live networks. The loosely coupled quorums structures are the building blocks of the system and consensus rules need to manage only their behavior because the network is reaching its consensus in its decentralized manner itself. In FBA networks the exact number of participants is never available, and it is not needed for anyone to know it to run through the consensus successfully. Avalanche and How It Uses Sampling to Achieve Deterministic Probabilistic Consensus on a Leaderless Manner that Can Scale Avalanche Avalanche is a probabilistic consensus protocol based on Proof of Stake. It was proposed by a pseudonymous group called Team Rocket . The most important property that sets it apart from most of the existing protocols, is that like Nakamoto consensus and open permissionless blockchains, it can operate with no known upper bound of participants in the network (in the way FBA functions). It aims to reach the decentralization level of an open network, resilience to 51% attacks, as well as high throughput and low latency. It follows the idea of having some neglectable error when reaching of consensus as Nakamoto suggested, so it is probabilistic and there is no 100% certainty of reaching a consensus. The number of messages each node has to handle per decision has O(log N) complexity and does not increase as the network scales up to a higher number of nodes. Avalanche is a validator protocol and each node has a known number of validators (other nodes) called validator list . Transactions that appear virtuous are accepted by validators, and validators are always expected to have deterministic behavior. A transaction should be rejected if it conflicts with a previous one that the validator had voted for. Each validator is a completely independent decision-maker and there is no leader election. At a high level, each node randomly selects others from its validator list to get their current statements on some value. This process is referred to as \"repeated random subsampling\" and it determines whether the rest of the network agrees on a transaction. The procedure is repeated multiple times on new randomly selected nodes until the node builds up enough data to determine that the probability of being correct is high enough that it can be considered impossible to consider the opposite value. To perform the consensus algorithm, Avalanche contains multiple decree Snowball instances instantiated as a multi-decree protocol that maintains a dynamic, append-only Directed Acyclic Graph (DAG) of all known transactions. Maintaining a DAG improves efficiency in the sense that a single vote on a DAG vertex implicitly votes for all transactions on the path to the genesis vertex. When a client creates a transaction, it names one or more parent transactions, which are included inseparably in the inputs and their connections from the edges of the DAG. Transactions that are reachable via parent edges back in history are called ancestor set and child transactions and their offspring are called progeny . The central challenge in the maintenance of the DAG is to choose among conflicting transactions, for example, transactions that spend the same funds form a conflict set and only one of them can be accepted. Avalanche instantiates a Snowball instance for each conflict set to run repeated queries and capture the level of confidence in each conflicting transaction. In a given round each validator randomly selects some nodes from its list to query their preferred decision on a value. The validators respond with their statements and if the majority of responses returned in a round differ from the one the query performer has, then it will update its decision and respond to other nodes with that answer. When a transaction is queried, all transactions reachable from it by following the DAG edges towards its ancestor set, and its progeny are implicitly part of the query and the node will respond positively for it if the transaction and its entire ancestry are currently the preferred option in their respective conflict sets. Each time a specific threshold of positive responders is reached, the transaction collects a chit and nodes compute confidence in transactions as the total amount of chits they obtained. Nodes perform the querying round independently and asynchronously. So since one node does not query every other node like in classical consensus, each node performs its sample of randomly selected nodes, thus the total number of network participants is unknown and no one needs to know about it to reach consensus. Avalanche follows the approach to building a probabilistic consensus algorithm in a decentralized way with having multiple validator lists that do not represent all validators in the network. If the network is parameterized for a 33% attacker, and an attacker has a 34% stake, unlike with Classical consensus protocols where launching a double-spend attack is guaranteed to succeed, with Avalanche it just means they have a slightly higher chance to succeed rather than guaranteed. Turning Avalanche into FBA by Making the Algorithm with Locally Defined Validator Set Hash Graph as a Step before FCP HashGraph is a consensus algorithm that offers a different approach to distributed ledger technology. Compared to most blockchain networks, it is a privately held network with its only public version being the Hedera Hashgraph network. Hashgraph is another distributed ledger technology, and it is described as a continuator or successor to the blockchain. In Hedera, as well as in other blockchains, a user can create a transaction, that will eventually be put in a block and then spread out in the network as a part of this block. But, instead of choosing one block to be chained to the previous one forming a chain of blocks, Hedera incorporates every block in the ledger without discarding any of the records. The Hashgraph algorithm is an asynchronous Byzantine Fault Tolerant (aBFT) and the Hedera system is completely asynchronous. The main assumption upon which the consensus relies is that 2/3 of the nodes are well-behaved nodes, which are expected to act deterministically. So on a high level, the network operates on a DAG structure that records time-sequenced transactions at its vertices. Each message contains one or more transactions, a timestamp, digital signature, hashes of two earlier events. Hashgraph is also implemented using a DAG and gossip protocol for communication between network nodes. Flare System Flare system is a multi-chain platform that manages multiple blockchains and handles value exchange between them. It uses FBA and implements its own component (State Connector System) for connectivity and validation of the chains. Currently, Flare network borrows concepts from Avalanche. Avalanche implementation consists of multiple subnets that have multiple chains connected to them . Subnets in Avalanche are dynamic sets of validators working to achieve consensus on one or multiple blockchain networks. Generally, there are three chains: the Platform Chain , Contract Chain and Exchange Chain . Flare takes advantage mainly from the Contract Chain to run FBA voting locally on its nodes. There are currently three sets of participants in the Flare system: validators, block producers and attestors. Flare validators set is set and run locally by each node. Attestors vote on state changes on underlying chains, including voting on the number of blocks produced by validators. Block producers correspond to validators on the underlying chains. They propose blocks to the network, according to their weight voted on by attestors. The component that monitors the state of the different blockchains is the State Connector . This system provides the Flare Network with flexibility to integrate other blockchains to the Flare Network. The model design of the State Connector System is crucial to maintaining the accurate digesting of integrated blockchain state changes. This design requires attackers to alter the view of integrated blockchains through their own Flare Network validator and a quorum of other Flare Network validators. Flare system runs its own integrated Ethereum Virtual Machine (EVM) that is locally maintained. In order for the State Connector to digest the changes on the different blockchains connected to the network, it needs to know about the number of blocks in each of the chains. In the Flare network, the problem of updating replicated states of blockchains is addressed using quorums of validators. They are snapshots of the validator capacity of the network and function as a federated group of quorums. This way of building the network is following the logic of the Flare consensus algorithm , but not fully following it. The full implementation is expected for the near future. The Future with the Flare Consensus Protocol Flare Consensus Protocol (FCP) uses FBA and virtual voting (like Hashgraph ). Flare follows the FBA requirements for reaching network quorum intersection after removing malicious nodes from the system, consequently, quorum intersection is again required between well-behaved nodes. The protocol relies on two other assumptions. The first is the impossibility of forging messages. When communicating with each other, nodes are named by public keys and use cryptographically secure digital signatures when exchanging messages. In the context of asynchronous communication, it is assumed also, that messages exchanged between well-behaved nodes can get delayed but will eventually be delivered so the order in which they were sent might not be the order they are received. In Flare network nodes synchronize with each other using the exchanged messages information via a gossip protocol, consequently functioning asynchronously. After each node finds its quorum slices, they start to send messages to a random number of peers in them. The first messages are the initial messages, and they contain information about the transaction amount, the recipient, and a hash of the sender's quorum of nodes. Recipients of messages in their turn create new messages that contain information about the received message and the messages they previously created themselves (and sent to some other node in a previous step). In the second turn, messages contain more information including the hash of previous messages received and sent by the node that are used to build up the connections between them. The creation and propagation of messages across the network are represented by a global DAG structure. It tracks the history of all messages exchanged by the recipients over time. In the DAG, messages are connected by edges and an existing edge between two messages means that it is possible to go from one vertex to the other, each message being a vertex of the structure. Using the topology of the network that the DAG brings, ancestors of a message are all the messages that it is referencing as previous to it, in the order in time the node receives them. To keep track of the information they have, nodes maintain also DAGs that represent the state of their local knowledge. As time progresses, some messages, by being encoded in the history of the new message, will be more widely shared throughout the network. Once all participants are aware of them, the spread messages and their content are in the scope of global knowledge of the network. This is captured by the idea of reachability. Following the logic of FBA, an important part of exchanging messages between nodes is sharing them not amongst any participant, but only with the important ones for each node. This implementation of the FBA federated quorum overlapping structure aims to reach flexible trust between the groups. Each participant in the network must end up with the same ordering of messages to reach a consensus on the ledger history. After each participant has constructed its local DAG, the protocol proceeds to Federated Virtual Voting process . An FBA construction does not rely on economic mechanisms for securing consensus because it enables individual nodes to independently make quorum slice decisions. Market forces cause quorum slices to overlap and this gives rise to the network-wide rule for consensus running in a decentralized manner. FCP achieves fairness by being both leaderless and ordered, making it excessively difficult for an attacker to influence which of two transactions will be ordered first in a transaction set. These properties make FCP a compelling model for internet-level Turing-complete consensus. Byzantine-Resistant Total Ordering Algorithms Multicast group communication protocols are used extensively in fault-tolerant distributed systems. For many such systems, the acknowledgements for individual messages define a casual (partial) order of messages. Maintaining the consistency of information, that is being replicated on several node instances to protect it against faults, is greatly simplified by reaching a total ordering on messages. Algorithms that process the casual (partial) order of messages into a total order exist under the group name \"Total Ordering\" algorithms. Characteristics Modern fault-tolerant distributed systems are often built on multicast group communication protocols, which impose a partial order of messages and on many of them information is replicated on several (or all) processors in order to achieve faster access and protection against faults. Maintaining the consistency of replicated information is an important and problematic aspect of the implementations, and it can be simplified by imposing a total order, rather than a partial order, on messages. The total order on messages ensures that exactly the same updates are applied to the replicated data in the same order at all nodes. The Total Ordering algorithms are executed independently and concurrently by each process and each of them incrementally creates a total order, without requiring the transmission of any additional messages in the general case. They operate in the presence of asynchrony and communication faults, which may cause different processes to receive messages in different orders, and they tolerate both crash and Byzantine process fault. The system is asynchronous in the sense that no bound can be placed on the time required for a computation or for a communication of a message. A crash fault causes a process to stop processing and generating messages. A Byzantine fault is some arbitrary behavior by the faulty process, including generating of messages intended to mislead the other processes. The total order ensures consistency by having the same updates applied to the replicated pieces of data in the same order at all sites. Each broadcast message has a header consisting of a process identifier, a message sequence number and payload content. A typical implementation might use a digital signature to substantiate the source of a message (process identifier) for each node. Systems capable of tolerating Byzantine faults must be able to confirm the source of a message and to detect messages that have the same header but different contents and/or acknowledgements within the message. Another option is to also embed a digest of the message content in its identifier, so that acknowledgements of messages with different contents but same sequence number will be regarded as acknowledgements of distinct messages. The acknowledgement and re-transmission mechanisms can ensure that all versions of a message are delivered to all destinations. Digital signatures and digests are computationally expensive, but do not require the additional rounds of message exchange of alternative strategies. The output of the algorithms is a total order of messages that is identical at all non-faulty processes in the system. Messages from non-Byzantine processes follow all previously broadcast messages. The relation between messages is transitive, and since each message also follows itself, it is reflexive as well. Messages need to acknowledge previously broadcast messages to be considered non-faulty. These requirements cannot be assured for messages from Byzantine processes, because such process might have transmitted multiple messages that acknowledge the same or different prior messages or even transmitted messages of which other processes are unaware. A Byzantine process can also originate a message that occurs within a cycle in the partial order. A non-Byzantine process cannot originate a message that occurs in a cycle, because a message from a non-Byzantine process cannot acknowledge another message that itself acknowledges the first one. A non-faulty process executing a Total ordering algorithm does not advance a message to the total order unless it has already advanced to the total order all the messages that precede it in the partial order. Thus, a non-faulty process does not advance to the total order any message in a cycle, or any message that follows a message in a cycle. If a Byzantine process ever participates in a cycle, then none of its subsequent messages will be ordered. A Byzantine process can originate two or more concurrent messages with the same header, but with different contents and/or acknowledgements, neither of which follows the other and possibly each of which purports to follow different messages. These messages are called mutants . The input to the Total Ordering algorithms is a Byzantine partial order of messages and the output is a total order of messages. Only information derived from the Byzantine casual order it used to construct the total order, meaning that no additional communication between processes is required. A \"candidate message\" is a message from the process's Byzantine casual order that is not yet in the total order but only follows a message from it. A set of candidate messages is called a \"candidate set\". Thus, messages that occur in a cycle cannot be candidate messages, however mutant messages that are not in a cycle can be candidate messages and even members of the same candidate set. Except for mutants, at most one candidate message from a given process is considered for the next advancement to the total order. Prefixes in the Byzantine casual order are subsets of messages that are related to each other by their follows relations and can contain multiple messages followed by a single one. The size of a Byzantine casual order is the number of messages in that subset at any point. A prefix in the total order of messages is a set of the messages that are totally ordered and can be represented as a finite sequence. The internal state of a process consists of a prefix of the Byzantine casual order and a prefix of the total order, and the initial state consists of an empty prefix of the Byzantine casual order and an empty prefix of the total order. Messages in the Byzantine casual order prefix (except those in a cycle) provide votes on the candidate sets. These votes are not contained explicitly in the messages, but are deduced from the partial relationship between messages. Messages relation in most of the cases is represented by a DAG structure, so votes are implicitly counted using edges between vertices from each position towards its ancestry and progeny. A non-faulty process decides to advance a candidate set to the total order based on the votes of the messages in its partial order prefix and each process makes its own decisions independently. Even though the messages can be delayed or lost, every non-faulty process must decide on the same message by which to extend to the total order. An execution step consists of adding one message to the Byzantine casual (partial) order prefix and executing the ordering algorithms. In a step, all candidate sets that can be constructed from the candidate messages in the casual order prefix are considered. Examples and further specifications of Total Ordering algorithms specify conditions for each one of these algorithms and include calculations on their communication complexity and performance. The algorithms employ a multistage voting strategy to achieve agreements on the total order and exploits the random structure of the casual order to achieve probabilistic termination. They ensure that non-faulty processes construct identical total orders on messages and that non-Byzantine processes construct consistent total orders, provided that the resilience requirements are satisfied. A process has no control over which message extends its casual order prefix in a step, and it cannot determine whether any future extension of its casual order prefix will involve a message that follows a particular message. The asynchronous nature and faulty behavior of the process and the communication medium are reflected in the Byzantine partial order of messages that is input to the algorithms. Aleph Algorithm One of the examples of Total Order algorithms with total ordering execution is Aleph . Aleph is a leaderless, fully asynchronous, Byzantine Fault Tolerant consensus protocol for total ordering of messages that are exchanged between entities in a network. Consequently, it is based on a distributed construction of a partially ordered set and the execution step - the algorithm for reaching a consensus on its extension to a total order. To achieve a consensus, the processes perform computations based on only a local copy of the data structure, however, they are bound to end up with the same result as required by the total ordering process. The nodes in Aleph protocol network send arbitrary messages to each other using a combination of multicast and random gossip for communication. Each node knows the identities of all other nodes and identifies itself through its public key, for which it holds a private key. From the high level perspective Aleph consensus algorithm functions on a set of nodes that listen for transactions. The nodes need to agree on a total ordering of the received transactions arriving in some unreliable streams of data that the nodes observe locally . The set of transactions that each one sees is a growing blockchain that does not have a built-in notion of finality and the streams of data are an advancing sequence of blockchain \"tips\" that each of the participants sees. The problem of ordering transactions in asynchronous types of protocols, where they are received with some time delay and in mixed order, is also known as Atomic Broadcast . It describes the situation where a set of nodes commonly constructs a total ordering of a set of transactions, where these transactions might not arrive all at the same time. Every time a node receives a transactions, it processes it and places it at its position. The sequence of transactions must be output in an incremental manner, meaning that before putting a transaction at some position, previous positions must have their respective transactions set. It is assumed that whenever an honest node outputs a transaction at some position, every other honest node will also eventually output the same transaction for this position. Also, every transaction that is input at some honest node is eventually output by all honest nodes. This ensures that not only all honest nodes will produce the same ordering, but also that no transaction is lost due to censorship because all honest nodes will output it. So Aleph produces a single stream of data that \"combines\" all the individual streams of the nodes and is consistent among them, meaning the nodes produce the same unique sequence of finalized blocks. As a Byzantine Tolerant type of protocol, Aleph implements Atomic Broadcast over a set of 3f+1 number of nodes, where f denotes the number of dishonest nodes. The protocol deals with an expected latency for each transaction, which is the time it is output by all honest nodes after it has been input to at least some preset number of nodes. The consensus separates the network communication from the protocol logic. The only role for the network layer is to maintain a common data structure in the form of a DAG tree among all nodes. The purpose of using it is to divide the algorithm execution into separate virtual rounds. In each round the nodes emits exactly one \"unit\" that should be thought of as a message broadcast to all the other nodes, and it is a vertex position on its local DAG version. Every unit contains information about its creator, its parents and additional data that can be included in it. All the nodes are expected to generate such units and maintain their local copies of the common DAG, to which new units are continuously added. Aleph uses transitive properties of the underlying DAG. Each unit has a \"DAG-round\" that is defined by the maximum length of a downward chain starting from the current transaction, meaning getting as long as possible path of its ancestors. A unit with no parents has a DAG-round of 0 , otherwise it has a DAG-round equal to the maximum of DAG-rounds of its parents plus 1 . Every node should create one unit in every round, and it should do so after learning a large portion (at least 2f+1 ) of units created in the previous round. Then the protocol is stated entirely through combinatorial properties of this structure Each of the nodes votes on the units it sees and decisions on accepting it or not are binary. In a network running Aleph protocol, where a steady inflow of transactions is assumed, the communication complexity achieved is O(N\u00b2 logN) . There is no requirement for a trusted dealer to distribute certain cryptographic keys among nodes, but Aleph implements an ABFT Randomness Beacon with a trustless setup. Aleph protocol is an example of a Total Ordering algorithm that creates a total order of blocks, thus creating a finalized chain at run, and keep its DAG history records only locally. This gives to the nodes a higher level of independence when executing the algorithm, and the weight of synchronizing towards reaching the same total order of blocks is transferred to the combinatoric selection of messages from the Byzantine flow. Blockmania Blockmania is also a leaderless Byzantine Fault Tolerant consensus protocol, detailed in the Blockmania academic paper . It is a part of the Chainspace platform , but can be used on its own. Blockmania is another example of a Total Ordering algorithm. Typically, clients are external to the network nodes and emit transactions that need to be agreed upon. Nodes in the network maintain an asymmetric signature scheme and all others can authentically verify their messages via public keys. Blockmania keeps consistent blocks history using a DAG structure, that is subsequently interpreted by each node separately. The basic inter-node network operation in Blockmania consists of each node periodically broadcasting a block to all other nodes. The nodes are loosely synchronized, using a byzantine clock synchronization and emit blocks at regular intervals. Blocks from each node form a sequence and each refers to the hash of the previous one, forming a hash chain. A block contains information about its creator node, its sequence number in the chain, a sequence of entries representing the content of the block. Each block emitted by a node is first a candidate block and contains a position defined by its creator node, its sequence number and its content. Each entry in the block content is either a transaction received from a client or a reference to the hash of a valid block received from another node. The full block is signed using the private key of its creator and broadcast to all other nodes. Besides broadcasting blocks, nodes listen to requests from other nodes for specific blocks, and respond by sending them the full contents of the blocks requested. An honest node includes all transactions received from clients as entries into a block it emits. Also, all honest nodes receive and checks for validity all blocks they directly or indirectly reference in their created blocks. This means that an honest node receives and stores locally a copy of the full DAG record, starting from the genesis vertex until the last block it emits. And if an honest node emits a block, all honest nodes eventually receive and consider it valid (after validating) as well as all blocks that it references directly or indirectly in its content entries. A dishonest node may contradict itself and send two distinct blocks for the same position in the chain or not send a block for a position for some time, or ever. All honest nodes reference all valid blocks in their own blocks, including contradictory ones, and proceed with the next phase of the protocol called \"interpretation\". The aim of the interpretation phase is for all honest nodes, despite holding a different subset of the DAG record at any point, to eventually agree on a specific block for any position, or alternatively, to agree that there is no such block, and to produce the next position in the total ordering of blocks. Blockmania is created as a simplified variant of PBFT algorithm and all parties need to agree on a single position rather than the sequence and all nodes perform their interpretation process independently. For reaching a decision for a single position, Blockmania runs an abstract terminating reliable broadcast protocol that is never materialized in real exchanged messages but rather the structure of the block DAG is interpreted by each node. At any time, a node can emit a message to propose a block in a pre-prepare message to all nodes. Honest nodes only propose one block for any given position. This message contains a view value that represents the change in the blockchain state and has a sequence number starting from zero. Upon receiving the pre-prepare message, nodes that have the same view broadcast a prepare message that contain their identity, view sequence number, block hash and content. If the view sequence number is zero, meaning this is the first proposed view change, the proposer signature is checked before accepting message. If the node does not have the same view interpreted, it does not send out prepare message, but sets the received pre-prepare and prepare messages for that view in its input records buffer. Upon receiving 2t prepare messages for a view and a corresponding pre-prepare message in its input buffer, nodes broadcast commit messages for that view. After receiving 2t+1 number of commit messages (also saved inside the input buffer), nodes consider view with the next position \"decided\" and update their local DAG versions. The consensus assumes that if two honest nodes reach a decision about a position in the chain, defined by block creator and a block sequence number, they always reach the same decision, assuming at most f byzantine nodes. A decision is eventually reached for any position in the total order of blocks by all honest nodes. The Blockmania considers a reliable transmission between honest nodes to be a Byzantine network that may arbitrarily delay blocks, but where they eventually get delivered. In practice, this is achieved using re-transmissions. Each node monitors chains of other nodes and in case their own blocks are not included into those within an estimated round trip time, they may re-transmit them. Also, an honest node may request missing blocks from other nodes, when those have been included in their chains as valid. Nodes use the gossip protocol to communicate with each other and do not need to know all participants in the network to reach to a decision. The goal of Blockmania protocol is to ensure that all honest nodes arrive at the same ordering of blocks, despite the presence of Byzantine nodes. Blockmania may be used with a Proof of Stake system by dynamically defining the quorum of nodes that reach agreement on a block before committing ( t ), thus two honest parties with a partial view of the Block DAG, can reach an agreement on which blocks will be accepted by all. In the Blockmania consensus, nodes update their block history using a DAG tree available to all, parts of which they just keep as copies in their local memory. The consensus is built in a modular manner and combines the quorum-based reaching of agreement with the total ordering creation, meaning it is one step further from PBFT types of algorithms. SafetyScore SafetyScore consensus is an example of an algorithm that builds a total ordered ledger, inspired by the idea of an epidemic spread of a virus that is hard to detect. It is a decentralized protocol that disseminates risk measures among the network participants based on their interactions with a disease. It is basically a digital tracing algorithm that can be used for all kinds of applications, while preventing leaks of personal data of network participants. It is Byzantine Fault tolerant, permissionless consensus that aims to produce a global consistent totally-ordered log of records, which is the finalized blockchain state at any time. Network can tolerate up to f faulty nodes for total number of network participants 2f+1 . The database is a distributed ledger. Each node generates its own sequence of blocks and all nodes collectively write history to a DAG structure that acts as an underlying representation of a distributed ledger current state. Nodes that can validate blocks published on the distributed ledger are known as validator nodes. The right to become a validator is earned by nodes that are operating reliably and are as far away as possible from each other in order to increase the overall resilience of the network. Nodes use asymmetric model of digital signatures that are used for authentication of messages they transmit and use shared secrets derived from ECDG X25519. In SafetyScore certain organizations are able to run specialized nodes that act as a distributed notary. They perform roles similar to notaries in the real world and act as witnesses ensuring the documents and transactions are properly executed. The right to operate a Distributed Notary is limited to organizations that have had a track record of working to preserve digital rights and freedoms, e.g. Electronic Frontier Foundation, Mozilla Foundation, Open Rights Group, etc. The distributed notary nodes have access to some confidential information like link between two activity keys and the operators should push back against parties looking to de-anonymize users. Participants in the distributed notary entity also enforce punishments to network nodes that violate the protocol, after a valid claim for the faulty act is presented. Each node validation of blocks is based on Kairos algorithm for face recognition. The IDs and the public keys of initial validators set are used to bootstrap the network. The Kairos algorithm uses each of the participants IDs to calculate their risk rate of infection based on interactions with other participants that also have some non-zero risk records in the network. Among other specifics, risk is calculated based on time after last contact and current level of recent interactions as well as on risk tokens calculated for each calendar day. The maximum proximity distance allowed, that devices take into consideration when calculating interactions, is 3 meters of each other. Nodes act as servers to clients and receive requests from them and can read from and write to distributed ledger Nodes public keys are broadcast over the network and participants do not need to know all the others in the network. In the case of SafetyScore, transactions and data coming from clients are records that are preset to be sent by software on different types of devices. The aim of the algorithm is to build a total order of records with metadata from the incoming Byzantine casual order of messages. SafetyScore is not a network for records of any value transfer, but rather a network for keeping records of history activity that is available to a certain number of participants in the network. It is built as a Total Ordering algorithm in its consensus process in the sense that it outputs a total ordered blocks with data that contain references to records that all network participants agreed on. After the ledger is updated with the latest record, all nodes update their local copies. SafetyScore is an example of a blockchain network with multiple logical layers, that implements the total ordering of blocks for its network layer and validator committee on its communication layer. It shows that the total ordering in an asynchronous environment, not only can be successfully reached, but also included as a building block in a much more complicated network of records.","title":"Consensus"},{"location":"flare/consensus/#consensus-algorithms","text":"This document explains, at a high level, what are the mechanisms behind the different consensus algorithms and how they evolved. After reading it, the engineer should understand the ideas behind Flare's consensus mechanisms.","title":"Consensus Algorithms"},{"location":"flare/consensus/#fba-and-separating-sybil-attack-resistance-from-consensus-by-making-it-a-local-selection","text":"","title":"FBA and Separating Sybil Attack Resistance from Consensus by Making it a Local Selection"},{"location":"flare/consensus/#consensus","text":"A consensus algorithm is an agreement system that follows a predefined set of steps for a group of participants (nodes) to reach the same decision about a statement. In a blockchain network nodes repeatedly decide what the complete history of a shared ledger is, from among multiple possible versions that occasionally conflict with each other. This network-wide agreement allows the recipient of a crypto coin to have faith that the coin is both valid (not counterfeit) and not already spent elsewhere.","title":"Consensus"},{"location":"flare/consensus/#ripple","text":"Ripple consensus is a voting consensus that works with nodes voting for each of its rounds. The nodes that run the Ripple Server software and participate in the consensus protocol are called servers. Each of them maintains a list of other servers \u2014 Unique Node List (UNL), that it queries for each validation round. The UNL is not a list of all servers in the system, but rather only a snapshot that the node knows about. Additionally, no participant knows what is the exact number of nodes in the system at any give time and does not need to. Each server queries the nodes from its UNL and receives their votes and depending on what the majority (a defined percentage of them) of the list votes on a transaction, it is accepted or rejected. The record of confirmed transaction history by the network is the ledger. It is kept in each user account and represents the ground truth of the network. The last-closed ledger is the most recent ledger that has been ratified by the consensus process and thus represents the current state of the network, as opposed to the \"open ledger\", which is the current operating ledger status of each node. When transactions are sent from the users they become part of the open ledger for each node and are included in the last-closed ledger after they pass through the consensus process. Each server includes every valid transaction it knows about in its broadcast messages to others when the consensus round starts, creating a list of transactions called a candidate set . Each server queries the nodes from its UNL when executing consensus and only the votes from UNL members are considered. The UNL is trustful only as a whole, meaning the set is expected not to collude in an attempt to defraud the network, but every single member of it is not implicitly considered trustful. Transactions with more than a certain percentage of no -votes are either discarded or included in the next candidate set for the next consensus round. If at least 80% of a UNL list agrees on a transaction, it is applied to the ledger. After the servers exchange their candidate sets, each of them unites its own with the ones received from its UNL peers, and on the next step servers then vote on the accuracy of the transactions. When servers vote, the process is a simple binary decision problem: deciding on a value 0 or 1 for the given transaction. The assumptions that define the Ripple consensus are: Every well-behaved node makes its decision for a finite time All well-behaved nodes reach the same decision value 0 and 1 are both possible values for all well-behaved nodes The goal is reaching a consensus amongst all nodes and preventing a situation, in which two disjoint sets of nodes each reach a consensus independently thus having two last-closed ledgers. The consensus algorithm is applied every few seconds by all nodes and once the consensus is reached \u2014 the ledger is considered closed and all well-behaved nodes will have identical ledger history. Each of the servers can join or leave the network at some point, which means that in some voting rounds some servers will have different nodes in their UNL lists. The notion of majority rule is applied on the UNL set requirement for agreement, but a hard majority cannot be applied, because the total number of servers in the system is unknown. As a result, there is no hard number of voters for any of the participants and existing groups of validating nodes overlap with each other. This is a version of a consensus algorithm running in a federated manner, where each node knows about some number of others validating nodes, but not about all nodes in the network.","title":"Ripple"},{"location":"flare/consensus/#stellar-and-the-federated-byzantine-agreement","text":"","title":"Stellar and the Federated Byzantine Agreement"},{"location":"flare/consensus/#introduction","text":"Any agreement system in a distributed computing network needs to be fault-tolerant. This means it must produce consistent results despite errors like slow communication links, unresponsive nodes, and misordered messages. A Byzantine agreement system is additionally tolerant of Byzantine faults : nodes that give false information, whether due to error or in a deliberate attempt to subvert the system or gain some advantage. Such a system prevents the double-spending problem by using a form of majority called quorum . A node in such a network refuses to commit to a particular version of the ledger history until it sees that enough of its peers are also prepared to commit to it. The number of peers that is enough to make a decision is the quorum and once each of the peers commits to a value after its quorum had committed, the system forms a large enough voting block to force the remaining nodes to agree with the decision. In such a system, attempts from malicious nodes to create double-spending events, or in another way invalid transactions, are overwhelmed by the network votes from honest nodes. For a hard majority to be formed, the number of all nodes in the system must be known and its quorum calculated at any time. Opposed to this concept, in a Federated Byzantine Agreement (FBA) system, the participants in the network are loosely defined and can join and leave at any time without coordinating with any central authority. Quorums are incomplete membership snapshots that can change at any time and consensus is reached in a decentralized form. This makes the network consensus probabilistic with some uncertainty level (probability of not reaching a consensus not equal to zero) present at all times. When making a decision, a node in an FBA system waits for its quorum nodes to agree on a value before considering a transaction settled. Each node is potentially a member of another node's quorum and eventually when enough network quorums accept a transaction, it becomes infeasible to be rolled back. On a higher level, the structure looks like multiple overlapping groups of participants, that function as a federation of groups, reaching consensus in a decentralized way, therefore the term federated . This structure brings a high level of decentralization into decision-making process among network members and mimics the behavior of social groups spreading a piece of information. FBA concept is defined as the Stellar Consensus Protocol (SCP) .","title":"Introduction"},{"location":"flare/consensus/#quorum-slices","text":"In an FBA consensus protocol implementation, nodes exchange messages asserting statements. When a node hears a sufficient set of nodes assert one statement, it assumes no functioning node will ever contradict that statement. This sufficient set of nodes is called a quorum slice or just a slice . A node in an FBA network refuses to commit to a particular value until it sees its peers are also committing to it. Since in FBA the collections of participants are loosely defined, quorums are dynamically defined from an ever-changing and inevitably incomplete snapshot of membership. Each node has multiple slices, any of which is sufficient to convince it of a value, so we can say that an FBA system consists of a loose federation of nodes each of which has chosen one or more slices to serve as a benchmark for making decisions. To form a quorum slice, a node adds all nodes it knows about to its quorum slice, as well as all nodes' peers. Continuing this process leads to encountering more and more nodes, and when no new nodes are found the quorum is defined. This process of dynamically creating quorums uses the transitive closure property of the network. With the help of this property, in an FBA system, every node is potentially connected by its indirect connections to other nodes. Nodes know about other nodes' quorum slices, in the same manner, they know the rest of the information about their network peers \u2014 from the broadcast information that every node sends to the network whenever its' voting state changes. Each broadcast message includes the details of the sending node's IP and its quorum slices nodes \u2014 a typical implementation used in FBA systems (and generally in other blockchains) is a gossip protocol .","title":"Quorum Slices"},{"location":"flare/consensus/#fba-and-stellar-implementation","text":"Stellar Consensus Protocol (SCP) is used to build Stellar Payment Network and implemented FBA as an essential building block of its system. FBA systems create confidence in the outcome of a decentralized vote by running a process called \"federated voting\". This is a procedure for discovering whether a network of participants can agree on a proposal. In the cryptocurrency world a proposal is a ledger state with included transactions. In a round of federated voting, each node chooses one of the potentially many possible values as an outcome of the round, but it first needs to confirm the nodes from its quorum will choose the same outcome. The Stellar network's nodes conduct multiple federated votes on multiple values until some of them are accepted. Since it is a payment network, it is logical to assume consensus needs to be reached on ledger values, but in fact, it is on \"statements\" about these values. In an open system, someone can gain influence on the network by conducting a so-called Sybil attack . In short, a Sybil attack is an attempt to control a peer network by creating multiple fake identities that appear as unique peers to the outside world. For the federated voting to be successful, Stellar's solution is to require a network property called quorum intersection . This means that any two quorums that can be constructed in the network will always share at least one node after all Byzantine nodes are excluded. For determining the prevailing sentiment of the network, this is as good as having a majority. Intuitively, if any quorum agrees to statement X , no other quorum can ever agree to not-X , because it will necessarily include some node from the first quorum that has already voted for X . This is the transitive closure property that these types of systems typically use to fight against Sybil attacks. A living example of such a network is the Internet itself. It consists of independent nodes that know about a few other nodes local to them, but the sets overlap with each other and every node is reachable through any other by routes and gateways. In the same manner, an FBA network resembles inter-domain network routing with no central authority dictating or arbitraging agreements. Quorum slices can be perceived as analogous to internet networks in terms of reachability. Servers in the internet networks can be reached via requests from neighboring networks, using gateways and hubs, making each server accessible through any client. In quorums, nodes are reachable through their neighboring nodes, which might be in another quorum, but because of the nodes' connections between each other, each node can be reached theoretically from any other. The Internet's nearly complete transitive reachability suggests it can likewise ensure a worldwide consensus with FBA. Voting in Stellar consists of two phases: nomination and balloting. At the beginning of the nomination phase, a node randomly votes for a statement on a ledger version it receives from another node in the network. The nodes cast out their nomination votes via messages to their peers, as well as the nominations of their peers, meaning separate federated votes are batched together. Voting against a value is not defined and a node can nominate as many statements as it likes so after the FBA process is run on each node, a nominee is confirmed, and it becomes a candidate . The nomination may produce multiple confirmable candidates, so SCP requires the application layer to supply some method for combining them into a single composite, with the only requirement for this method to be deterministic. SCP is an asynchronous protocol and nodes are not coordinated by time, but by the messages they exchange. Thus, from a node\u2019s perspective, it is not clear when the nomination process has ended. During the nomination, it cannot be said which of the composites is the final one, but this is expected because the purpose is to produce several virtuous candidates for the balloting. In the balloting phase nodes exchange their ballots on created composites and repeatedly conduct federated votes on statements about ballots. From the point of view of a given node, a consensus is reached when it finds a ballot for which it can confirm (find a quorum accepting) a statement, and then it can be sure that every other well-behaved node has committed, or inevitably will commit to, the same value. Nodes can be categorized as either well-behaved or ill-behaved. A well-behaved node chooses sensible quorum slices and obeys the protocol, including eventually responding to all requests. Ill-behaved nodes do not follow the protocol, can suffer Byzantine failure, and behave arbitrarily. Since the blockchain network's most important goal is to reach consensus deterministically, nodes that generally act arbitrarily (non-deterministically) cannot be considered functioning parts of the system. The expected behavior of well-behaved nodes is always deterministic in the sense that they choose the same right value among others every time. The goal of the Byzantine agreement is to ensure that well-behaved nodes agree on the same values despite the presence of the ill-behaved nodes. The first part of this goal is to prevent nodes from diverging and committing different values for the same ledger. The second part is to ensure the nodes will always go through the consensus process, as opposed to getting blocked in some dead-state. The first property corresponds to the notion of safety, while the second one corresponds to the notion of liveness. FBA networks implementations reach consensus in a decentralized, but the deterministic way and are the next step in building safe and live networks. The loosely coupled quorums structures are the building blocks of the system and consensus rules need to manage only their behavior because the network is reaching its consensus in its decentralized manner itself. In FBA networks the exact number of participants is never available, and it is not needed for anyone to know it to run through the consensus successfully.","title":"FBA and Stellar Implementation"},{"location":"flare/consensus/#avalanche-and-how-it-uses-sampling-to-achieve-deterministic-probabilistic-consensus-on-a-leaderless-manner-that-can-scale","text":"","title":"Avalanche and How It Uses Sampling to Achieve Deterministic Probabilistic Consensus on a Leaderless Manner that Can Scale"},{"location":"flare/consensus/#avalanche","text":"Avalanche is a probabilistic consensus protocol based on Proof of Stake. It was proposed by a pseudonymous group called Team Rocket . The most important property that sets it apart from most of the existing protocols, is that like Nakamoto consensus and open permissionless blockchains, it can operate with no known upper bound of participants in the network (in the way FBA functions). It aims to reach the decentralization level of an open network, resilience to 51% attacks, as well as high throughput and low latency. It follows the idea of having some neglectable error when reaching of consensus as Nakamoto suggested, so it is probabilistic and there is no 100% certainty of reaching a consensus. The number of messages each node has to handle per decision has O(log N) complexity and does not increase as the network scales up to a higher number of nodes. Avalanche is a validator protocol and each node has a known number of validators (other nodes) called validator list . Transactions that appear virtuous are accepted by validators, and validators are always expected to have deterministic behavior. A transaction should be rejected if it conflicts with a previous one that the validator had voted for. Each validator is a completely independent decision-maker and there is no leader election. At a high level, each node randomly selects others from its validator list to get their current statements on some value. This process is referred to as \"repeated random subsampling\" and it determines whether the rest of the network agrees on a transaction. The procedure is repeated multiple times on new randomly selected nodes until the node builds up enough data to determine that the probability of being correct is high enough that it can be considered impossible to consider the opposite value. To perform the consensus algorithm, Avalanche contains multiple decree Snowball instances instantiated as a multi-decree protocol that maintains a dynamic, append-only Directed Acyclic Graph (DAG) of all known transactions. Maintaining a DAG improves efficiency in the sense that a single vote on a DAG vertex implicitly votes for all transactions on the path to the genesis vertex. When a client creates a transaction, it names one or more parent transactions, which are included inseparably in the inputs and their connections from the edges of the DAG. Transactions that are reachable via parent edges back in history are called ancestor set and child transactions and their offspring are called progeny . The central challenge in the maintenance of the DAG is to choose among conflicting transactions, for example, transactions that spend the same funds form a conflict set and only one of them can be accepted. Avalanche instantiates a Snowball instance for each conflict set to run repeated queries and capture the level of confidence in each conflicting transaction. In a given round each validator randomly selects some nodes from its list to query their preferred decision on a value. The validators respond with their statements and if the majority of responses returned in a round differ from the one the query performer has, then it will update its decision and respond to other nodes with that answer. When a transaction is queried, all transactions reachable from it by following the DAG edges towards its ancestor set, and its progeny are implicitly part of the query and the node will respond positively for it if the transaction and its entire ancestry are currently the preferred option in their respective conflict sets. Each time a specific threshold of positive responders is reached, the transaction collects a chit and nodes compute confidence in transactions as the total amount of chits they obtained. Nodes perform the querying round independently and asynchronously. So since one node does not query every other node like in classical consensus, each node performs its sample of randomly selected nodes, thus the total number of network participants is unknown and no one needs to know about it to reach consensus. Avalanche follows the approach to building a probabilistic consensus algorithm in a decentralized way with having multiple validator lists that do not represent all validators in the network. If the network is parameterized for a 33% attacker, and an attacker has a 34% stake, unlike with Classical consensus protocols where launching a double-spend attack is guaranteed to succeed, with Avalanche it just means they have a slightly higher chance to succeed rather than guaranteed.","title":"Avalanche"},{"location":"flare/consensus/#turning-avalanche-into-fba-by-making-the-algorithm-with-locally-defined-validator-set","text":"","title":"Turning Avalanche into FBA by Making the Algorithm with Locally Defined Validator Set"},{"location":"flare/consensus/#hash-graph-as-a-step-before-fcp","text":"HashGraph is a consensus algorithm that offers a different approach to distributed ledger technology. Compared to most blockchain networks, it is a privately held network with its only public version being the Hedera Hashgraph network. Hashgraph is another distributed ledger technology, and it is described as a continuator or successor to the blockchain. In Hedera, as well as in other blockchains, a user can create a transaction, that will eventually be put in a block and then spread out in the network as a part of this block. But, instead of choosing one block to be chained to the previous one forming a chain of blocks, Hedera incorporates every block in the ledger without discarding any of the records. The Hashgraph algorithm is an asynchronous Byzantine Fault Tolerant (aBFT) and the Hedera system is completely asynchronous. The main assumption upon which the consensus relies is that 2/3 of the nodes are well-behaved nodes, which are expected to act deterministically. So on a high level, the network operates on a DAG structure that records time-sequenced transactions at its vertices. Each message contains one or more transactions, a timestamp, digital signature, hashes of two earlier events. Hashgraph is also implemented using a DAG and gossip protocol for communication between network nodes.","title":"Hash Graph as a Step before FCP"},{"location":"flare/consensus/#flare-system","text":"Flare system is a multi-chain platform that manages multiple blockchains and handles value exchange between them. It uses FBA and implements its own component (State Connector System) for connectivity and validation of the chains. Currently, Flare network borrows concepts from Avalanche. Avalanche implementation consists of multiple subnets that have multiple chains connected to them . Subnets in Avalanche are dynamic sets of validators working to achieve consensus on one or multiple blockchain networks. Generally, there are three chains: the Platform Chain , Contract Chain and Exchange Chain . Flare takes advantage mainly from the Contract Chain to run FBA voting locally on its nodes. There are currently three sets of participants in the Flare system: validators, block producers and attestors. Flare validators set is set and run locally by each node. Attestors vote on state changes on underlying chains, including voting on the number of blocks produced by validators. Block producers correspond to validators on the underlying chains. They propose blocks to the network, according to their weight voted on by attestors. The component that monitors the state of the different blockchains is the State Connector . This system provides the Flare Network with flexibility to integrate other blockchains to the Flare Network. The model design of the State Connector System is crucial to maintaining the accurate digesting of integrated blockchain state changes. This design requires attackers to alter the view of integrated blockchains through their own Flare Network validator and a quorum of other Flare Network validators. Flare system runs its own integrated Ethereum Virtual Machine (EVM) that is locally maintained. In order for the State Connector to digest the changes on the different blockchains connected to the network, it needs to know about the number of blocks in each of the chains. In the Flare network, the problem of updating replicated states of blockchains is addressed using quorums of validators. They are snapshots of the validator capacity of the network and function as a federated group of quorums. This way of building the network is following the logic of the Flare consensus algorithm , but not fully following it. The full implementation is expected for the near future.","title":"Flare System"},{"location":"flare/consensus/#the-future-with-the-flare-consensus-protocol","text":"Flare Consensus Protocol (FCP) uses FBA and virtual voting (like Hashgraph ). Flare follows the FBA requirements for reaching network quorum intersection after removing malicious nodes from the system, consequently, quorum intersection is again required between well-behaved nodes. The protocol relies on two other assumptions. The first is the impossibility of forging messages. When communicating with each other, nodes are named by public keys and use cryptographically secure digital signatures when exchanging messages. In the context of asynchronous communication, it is assumed also, that messages exchanged between well-behaved nodes can get delayed but will eventually be delivered so the order in which they were sent might not be the order they are received. In Flare network nodes synchronize with each other using the exchanged messages information via a gossip protocol, consequently functioning asynchronously. After each node finds its quorum slices, they start to send messages to a random number of peers in them. The first messages are the initial messages, and they contain information about the transaction amount, the recipient, and a hash of the sender's quorum of nodes. Recipients of messages in their turn create new messages that contain information about the received message and the messages they previously created themselves (and sent to some other node in a previous step). In the second turn, messages contain more information including the hash of previous messages received and sent by the node that are used to build up the connections between them. The creation and propagation of messages across the network are represented by a global DAG structure. It tracks the history of all messages exchanged by the recipients over time. In the DAG, messages are connected by edges and an existing edge between two messages means that it is possible to go from one vertex to the other, each message being a vertex of the structure. Using the topology of the network that the DAG brings, ancestors of a message are all the messages that it is referencing as previous to it, in the order in time the node receives them. To keep track of the information they have, nodes maintain also DAGs that represent the state of their local knowledge. As time progresses, some messages, by being encoded in the history of the new message, will be more widely shared throughout the network. Once all participants are aware of them, the spread messages and their content are in the scope of global knowledge of the network. This is captured by the idea of reachability. Following the logic of FBA, an important part of exchanging messages between nodes is sharing them not amongst any participant, but only with the important ones for each node. This implementation of the FBA federated quorum overlapping structure aims to reach flexible trust between the groups. Each participant in the network must end up with the same ordering of messages to reach a consensus on the ledger history. After each participant has constructed its local DAG, the protocol proceeds to Federated Virtual Voting process . An FBA construction does not rely on economic mechanisms for securing consensus because it enables individual nodes to independently make quorum slice decisions. Market forces cause quorum slices to overlap and this gives rise to the network-wide rule for consensus running in a decentralized manner. FCP achieves fairness by being both leaderless and ordered, making it excessively difficult for an attacker to influence which of two transactions will be ordered first in a transaction set. These properties make FCP a compelling model for internet-level Turing-complete consensus.","title":"The Future with the Flare Consensus Protocol"},{"location":"flare/consensus/#byzantine-resistant-total-ordering-algorithms","text":"Multicast group communication protocols are used extensively in fault-tolerant distributed systems. For many such systems, the acknowledgements for individual messages define a casual (partial) order of messages. Maintaining the consistency of information, that is being replicated on several node instances to protect it against faults, is greatly simplified by reaching a total ordering on messages. Algorithms that process the casual (partial) order of messages into a total order exist under the group name \"Total Ordering\" algorithms.","title":"Byzantine-Resistant Total Ordering Algorithms"},{"location":"flare/consensus/#characteristics","text":"Modern fault-tolerant distributed systems are often built on multicast group communication protocols, which impose a partial order of messages and on many of them information is replicated on several (or all) processors in order to achieve faster access and protection against faults. Maintaining the consistency of replicated information is an important and problematic aspect of the implementations, and it can be simplified by imposing a total order, rather than a partial order, on messages. The total order on messages ensures that exactly the same updates are applied to the replicated data in the same order at all nodes. The Total Ordering algorithms are executed independently and concurrently by each process and each of them incrementally creates a total order, without requiring the transmission of any additional messages in the general case. They operate in the presence of asynchrony and communication faults, which may cause different processes to receive messages in different orders, and they tolerate both crash and Byzantine process fault. The system is asynchronous in the sense that no bound can be placed on the time required for a computation or for a communication of a message. A crash fault causes a process to stop processing and generating messages. A Byzantine fault is some arbitrary behavior by the faulty process, including generating of messages intended to mislead the other processes. The total order ensures consistency by having the same updates applied to the replicated pieces of data in the same order at all sites. Each broadcast message has a header consisting of a process identifier, a message sequence number and payload content. A typical implementation might use a digital signature to substantiate the source of a message (process identifier) for each node. Systems capable of tolerating Byzantine faults must be able to confirm the source of a message and to detect messages that have the same header but different contents and/or acknowledgements within the message. Another option is to also embed a digest of the message content in its identifier, so that acknowledgements of messages with different contents but same sequence number will be regarded as acknowledgements of distinct messages. The acknowledgement and re-transmission mechanisms can ensure that all versions of a message are delivered to all destinations. Digital signatures and digests are computationally expensive, but do not require the additional rounds of message exchange of alternative strategies. The output of the algorithms is a total order of messages that is identical at all non-faulty processes in the system. Messages from non-Byzantine processes follow all previously broadcast messages. The relation between messages is transitive, and since each message also follows itself, it is reflexive as well. Messages need to acknowledge previously broadcast messages to be considered non-faulty. These requirements cannot be assured for messages from Byzantine processes, because such process might have transmitted multiple messages that acknowledge the same or different prior messages or even transmitted messages of which other processes are unaware. A Byzantine process can also originate a message that occurs within a cycle in the partial order. A non-Byzantine process cannot originate a message that occurs in a cycle, because a message from a non-Byzantine process cannot acknowledge another message that itself acknowledges the first one. A non-faulty process executing a Total ordering algorithm does not advance a message to the total order unless it has already advanced to the total order all the messages that precede it in the partial order. Thus, a non-faulty process does not advance to the total order any message in a cycle, or any message that follows a message in a cycle. If a Byzantine process ever participates in a cycle, then none of its subsequent messages will be ordered. A Byzantine process can originate two or more concurrent messages with the same header, but with different contents and/or acknowledgements, neither of which follows the other and possibly each of which purports to follow different messages. These messages are called mutants . The input to the Total Ordering algorithms is a Byzantine partial order of messages and the output is a total order of messages. Only information derived from the Byzantine casual order it used to construct the total order, meaning that no additional communication between processes is required. A \"candidate message\" is a message from the process's Byzantine casual order that is not yet in the total order but only follows a message from it. A set of candidate messages is called a \"candidate set\". Thus, messages that occur in a cycle cannot be candidate messages, however mutant messages that are not in a cycle can be candidate messages and even members of the same candidate set. Except for mutants, at most one candidate message from a given process is considered for the next advancement to the total order. Prefixes in the Byzantine casual order are subsets of messages that are related to each other by their follows relations and can contain multiple messages followed by a single one. The size of a Byzantine casual order is the number of messages in that subset at any point. A prefix in the total order of messages is a set of the messages that are totally ordered and can be represented as a finite sequence. The internal state of a process consists of a prefix of the Byzantine casual order and a prefix of the total order, and the initial state consists of an empty prefix of the Byzantine casual order and an empty prefix of the total order. Messages in the Byzantine casual order prefix (except those in a cycle) provide votes on the candidate sets. These votes are not contained explicitly in the messages, but are deduced from the partial relationship between messages. Messages relation in most of the cases is represented by a DAG structure, so votes are implicitly counted using edges between vertices from each position towards its ancestry and progeny. A non-faulty process decides to advance a candidate set to the total order based on the votes of the messages in its partial order prefix and each process makes its own decisions independently. Even though the messages can be delayed or lost, every non-faulty process must decide on the same message by which to extend to the total order. An execution step consists of adding one message to the Byzantine casual (partial) order prefix and executing the ordering algorithms. In a step, all candidate sets that can be constructed from the candidate messages in the casual order prefix are considered. Examples and further specifications of Total Ordering algorithms specify conditions for each one of these algorithms and include calculations on their communication complexity and performance. The algorithms employ a multistage voting strategy to achieve agreements on the total order and exploits the random structure of the casual order to achieve probabilistic termination. They ensure that non-faulty processes construct identical total orders on messages and that non-Byzantine processes construct consistent total orders, provided that the resilience requirements are satisfied. A process has no control over which message extends its casual order prefix in a step, and it cannot determine whether any future extension of its casual order prefix will involve a message that follows a particular message. The asynchronous nature and faulty behavior of the process and the communication medium are reflected in the Byzantine partial order of messages that is input to the algorithms.","title":"Characteristics"},{"location":"flare/consensus/#aleph-algorithm","text":"One of the examples of Total Order algorithms with total ordering execution is Aleph . Aleph is a leaderless, fully asynchronous, Byzantine Fault Tolerant consensus protocol for total ordering of messages that are exchanged between entities in a network. Consequently, it is based on a distributed construction of a partially ordered set and the execution step - the algorithm for reaching a consensus on its extension to a total order. To achieve a consensus, the processes perform computations based on only a local copy of the data structure, however, they are bound to end up with the same result as required by the total ordering process. The nodes in Aleph protocol network send arbitrary messages to each other using a combination of multicast and random gossip for communication. Each node knows the identities of all other nodes and identifies itself through its public key, for which it holds a private key. From the high level perspective Aleph consensus algorithm functions on a set of nodes that listen for transactions. The nodes need to agree on a total ordering of the received transactions arriving in some unreliable streams of data that the nodes observe locally . The set of transactions that each one sees is a growing blockchain that does not have a built-in notion of finality and the streams of data are an advancing sequence of blockchain \"tips\" that each of the participants sees. The problem of ordering transactions in asynchronous types of protocols, where they are received with some time delay and in mixed order, is also known as Atomic Broadcast . It describes the situation where a set of nodes commonly constructs a total ordering of a set of transactions, where these transactions might not arrive all at the same time. Every time a node receives a transactions, it processes it and places it at its position. The sequence of transactions must be output in an incremental manner, meaning that before putting a transaction at some position, previous positions must have their respective transactions set. It is assumed that whenever an honest node outputs a transaction at some position, every other honest node will also eventually output the same transaction for this position. Also, every transaction that is input at some honest node is eventually output by all honest nodes. This ensures that not only all honest nodes will produce the same ordering, but also that no transaction is lost due to censorship because all honest nodes will output it. So Aleph produces a single stream of data that \"combines\" all the individual streams of the nodes and is consistent among them, meaning the nodes produce the same unique sequence of finalized blocks. As a Byzantine Tolerant type of protocol, Aleph implements Atomic Broadcast over a set of 3f+1 number of nodes, where f denotes the number of dishonest nodes. The protocol deals with an expected latency for each transaction, which is the time it is output by all honest nodes after it has been input to at least some preset number of nodes. The consensus separates the network communication from the protocol logic. The only role for the network layer is to maintain a common data structure in the form of a DAG tree among all nodes. The purpose of using it is to divide the algorithm execution into separate virtual rounds. In each round the nodes emits exactly one \"unit\" that should be thought of as a message broadcast to all the other nodes, and it is a vertex position on its local DAG version. Every unit contains information about its creator, its parents and additional data that can be included in it. All the nodes are expected to generate such units and maintain their local copies of the common DAG, to which new units are continuously added. Aleph uses transitive properties of the underlying DAG. Each unit has a \"DAG-round\" that is defined by the maximum length of a downward chain starting from the current transaction, meaning getting as long as possible path of its ancestors. A unit with no parents has a DAG-round of 0 , otherwise it has a DAG-round equal to the maximum of DAG-rounds of its parents plus 1 . Every node should create one unit in every round, and it should do so after learning a large portion (at least 2f+1 ) of units created in the previous round. Then the protocol is stated entirely through combinatorial properties of this structure Each of the nodes votes on the units it sees and decisions on accepting it or not are binary. In a network running Aleph protocol, where a steady inflow of transactions is assumed, the communication complexity achieved is O(N\u00b2 logN) . There is no requirement for a trusted dealer to distribute certain cryptographic keys among nodes, but Aleph implements an ABFT Randomness Beacon with a trustless setup. Aleph protocol is an example of a Total Ordering algorithm that creates a total order of blocks, thus creating a finalized chain at run, and keep its DAG history records only locally. This gives to the nodes a higher level of independence when executing the algorithm, and the weight of synchronizing towards reaching the same total order of blocks is transferred to the combinatoric selection of messages from the Byzantine flow.","title":"Aleph Algorithm"},{"location":"flare/consensus/#blockmania","text":"Blockmania is also a leaderless Byzantine Fault Tolerant consensus protocol, detailed in the Blockmania academic paper . It is a part of the Chainspace platform , but can be used on its own. Blockmania is another example of a Total Ordering algorithm. Typically, clients are external to the network nodes and emit transactions that need to be agreed upon. Nodes in the network maintain an asymmetric signature scheme and all others can authentically verify their messages via public keys. Blockmania keeps consistent blocks history using a DAG structure, that is subsequently interpreted by each node separately. The basic inter-node network operation in Blockmania consists of each node periodically broadcasting a block to all other nodes. The nodes are loosely synchronized, using a byzantine clock synchronization and emit blocks at regular intervals. Blocks from each node form a sequence and each refers to the hash of the previous one, forming a hash chain. A block contains information about its creator node, its sequence number in the chain, a sequence of entries representing the content of the block. Each block emitted by a node is first a candidate block and contains a position defined by its creator node, its sequence number and its content. Each entry in the block content is either a transaction received from a client or a reference to the hash of a valid block received from another node. The full block is signed using the private key of its creator and broadcast to all other nodes. Besides broadcasting blocks, nodes listen to requests from other nodes for specific blocks, and respond by sending them the full contents of the blocks requested. An honest node includes all transactions received from clients as entries into a block it emits. Also, all honest nodes receive and checks for validity all blocks they directly or indirectly reference in their created blocks. This means that an honest node receives and stores locally a copy of the full DAG record, starting from the genesis vertex until the last block it emits. And if an honest node emits a block, all honest nodes eventually receive and consider it valid (after validating) as well as all blocks that it references directly or indirectly in its content entries. A dishonest node may contradict itself and send two distinct blocks for the same position in the chain or not send a block for a position for some time, or ever. All honest nodes reference all valid blocks in their own blocks, including contradictory ones, and proceed with the next phase of the protocol called \"interpretation\". The aim of the interpretation phase is for all honest nodes, despite holding a different subset of the DAG record at any point, to eventually agree on a specific block for any position, or alternatively, to agree that there is no such block, and to produce the next position in the total ordering of blocks. Blockmania is created as a simplified variant of PBFT algorithm and all parties need to agree on a single position rather than the sequence and all nodes perform their interpretation process independently. For reaching a decision for a single position, Blockmania runs an abstract terminating reliable broadcast protocol that is never materialized in real exchanged messages but rather the structure of the block DAG is interpreted by each node. At any time, a node can emit a message to propose a block in a pre-prepare message to all nodes. Honest nodes only propose one block for any given position. This message contains a view value that represents the change in the blockchain state and has a sequence number starting from zero. Upon receiving the pre-prepare message, nodes that have the same view broadcast a prepare message that contain their identity, view sequence number, block hash and content. If the view sequence number is zero, meaning this is the first proposed view change, the proposer signature is checked before accepting message. If the node does not have the same view interpreted, it does not send out prepare message, but sets the received pre-prepare and prepare messages for that view in its input records buffer. Upon receiving 2t prepare messages for a view and a corresponding pre-prepare message in its input buffer, nodes broadcast commit messages for that view. After receiving 2t+1 number of commit messages (also saved inside the input buffer), nodes consider view with the next position \"decided\" and update their local DAG versions. The consensus assumes that if two honest nodes reach a decision about a position in the chain, defined by block creator and a block sequence number, they always reach the same decision, assuming at most f byzantine nodes. A decision is eventually reached for any position in the total order of blocks by all honest nodes. The Blockmania considers a reliable transmission between honest nodes to be a Byzantine network that may arbitrarily delay blocks, but where they eventually get delivered. In practice, this is achieved using re-transmissions. Each node monitors chains of other nodes and in case their own blocks are not included into those within an estimated round trip time, they may re-transmit them. Also, an honest node may request missing blocks from other nodes, when those have been included in their chains as valid. Nodes use the gossip protocol to communicate with each other and do not need to know all participants in the network to reach to a decision. The goal of Blockmania protocol is to ensure that all honest nodes arrive at the same ordering of blocks, despite the presence of Byzantine nodes. Blockmania may be used with a Proof of Stake system by dynamically defining the quorum of nodes that reach agreement on a block before committing ( t ), thus two honest parties with a partial view of the Block DAG, can reach an agreement on which blocks will be accepted by all. In the Blockmania consensus, nodes update their block history using a DAG tree available to all, parts of which they just keep as copies in their local memory. The consensus is built in a modular manner and combines the quorum-based reaching of agreement with the total ordering creation, meaning it is one step further from PBFT types of algorithms.","title":"Blockmania"},{"location":"flare/consensus/#safetyscore","text":"SafetyScore consensus is an example of an algorithm that builds a total ordered ledger, inspired by the idea of an epidemic spread of a virus that is hard to detect. It is a decentralized protocol that disseminates risk measures among the network participants based on their interactions with a disease. It is basically a digital tracing algorithm that can be used for all kinds of applications, while preventing leaks of personal data of network participants. It is Byzantine Fault tolerant, permissionless consensus that aims to produce a global consistent totally-ordered log of records, which is the finalized blockchain state at any time. Network can tolerate up to f faulty nodes for total number of network participants 2f+1 . The database is a distributed ledger. Each node generates its own sequence of blocks and all nodes collectively write history to a DAG structure that acts as an underlying representation of a distributed ledger current state. Nodes that can validate blocks published on the distributed ledger are known as validator nodes. The right to become a validator is earned by nodes that are operating reliably and are as far away as possible from each other in order to increase the overall resilience of the network. Nodes use asymmetric model of digital signatures that are used for authentication of messages they transmit and use shared secrets derived from ECDG X25519. In SafetyScore certain organizations are able to run specialized nodes that act as a distributed notary. They perform roles similar to notaries in the real world and act as witnesses ensuring the documents and transactions are properly executed. The right to operate a Distributed Notary is limited to organizations that have had a track record of working to preserve digital rights and freedoms, e.g. Electronic Frontier Foundation, Mozilla Foundation, Open Rights Group, etc. The distributed notary nodes have access to some confidential information like link between two activity keys and the operators should push back against parties looking to de-anonymize users. Participants in the distributed notary entity also enforce punishments to network nodes that violate the protocol, after a valid claim for the faulty act is presented. Each node validation of blocks is based on Kairos algorithm for face recognition. The IDs and the public keys of initial validators set are used to bootstrap the network. The Kairos algorithm uses each of the participants IDs to calculate their risk rate of infection based on interactions with other participants that also have some non-zero risk records in the network. Among other specifics, risk is calculated based on time after last contact and current level of recent interactions as well as on risk tokens calculated for each calendar day. The maximum proximity distance allowed, that devices take into consideration when calculating interactions, is 3 meters of each other. Nodes act as servers to clients and receive requests from them and can read from and write to distributed ledger Nodes public keys are broadcast over the network and participants do not need to know all the others in the network. In the case of SafetyScore, transactions and data coming from clients are records that are preset to be sent by software on different types of devices. The aim of the algorithm is to build a total order of records with metadata from the incoming Byzantine casual order of messages. SafetyScore is not a network for records of any value transfer, but rather a network for keeping records of history activity that is available to a certain number of participants in the network. It is built as a Total Ordering algorithm in its consensus process in the sense that it outputs a total ordered blocks with data that contain references to records that all network participants agreed on. After the ledger is updated with the latest record, all nodes update their local copies. SafetyScore is an example of a blockchain network with multiple logical layers, that implements the total ordering of blocks for its network layer and validator committee on its communication layer. It shows that the total ordering in an asynchronous environment, not only can be successfully reached, but also included as a building block in a much more complicated network of records.","title":"SafetyScore"},{"location":"flare/scaling/","text":"A Brief History of Decentralized Scaling This article covers how the scaling solutions for decentralized systems in the crypto world have evolved with regards to the consensus algorithms that have been used, what their problems are and how Flare tries to address them. It also addresses how various blockchains fare in the realm of the blockchain trilemma which roughly states that it is hard to achieve more than 2 out of 3 from security, scalability and decentralization. The concepts at the foundation of blockchain technology, such as cryptography (for data encryption) and peer-to-peer networks (that handle information exchange in a distributed way) were around before the 2000s. However, there was no concept of a truly digital currency due to the double spending problem . Proof of Work Then came Satoshi Nakamoto who introduced the pre-existing concept of Proof of Work (PoW) on top of a Blockchain which acts as a distributed ledger of all the transactions of currency. So now imagine Brian has 100 bitcoins. There are many machines/nodes which have this information that Brian has 100 bitcoins. If Brian sends 50 of them to Alice then certain things happen before the transfer gets finalized. Nodes collect pending transactions (sent out by wallet applications) in a memory pool. When a node solves the computational problem, it gets to pick the transactions from its memory pool that get included in the next block and thus become part of the consensus state. However, only under the condition that the block follows the rules on transaction validity, i.e. does not include any transactions spending the same unit of account twice. This is termed as \"Nakamoto consensus\". Check this visualization of how PoW consensus might work and create a chain of blocks. PoW by itself is not very new. A similar implementation was thought of in 1992 to combat email spam . Using the Hashcash system, every email has some form of simple proof of work, which makes it is easy for someone to send an email to a friend but makes it difficult for a spammer to send millions of emails at the same time. In theory, PoW can scale infinitely. The more successful the network is, the higher the demand for the token. The higher the demand for the token, the more its value increases. As the price increases, more miners can profitably run miners, and more power is wasted. Also, some PoW based blockchains like Ethereum suffer from huge transaction fees as well as low throughput of 10-15 transactions per second which make them impractical for true global usage. Bitcoin's PoW has the potential to scale a lot in terms of a payment system, but the wastage of energy is its biggest problem. Federated Byzantine Agreement Byzantine Generals Problem is the problem when there are various nodes communicating and a few of them are \u201cByzantine\u201d \u2014 either faulty or malicious. Byzantine nodes might not reply to messages, or deliberately send wrong information like false signatures, double spent transactions, etc. Therefore, any blockchain should have mechanisms to solve the Byzantine Generals Problem. In case of Bitcoin, the mechanism is PoW. Federated Byzantine Agreement (FBA) consensus in a family of consensus protocols, which eliminates Byzantine faults, and provides deterministic finality (unlike PoW which has only probabilistic finality), without having the selection of validators as part of the protocol itself. This was introduced by Stellar . In FBA, every node has its \u201cquorum set\u201d which is a list of other nodes which this particular node trusts. Therefore, to achieve consensus for a transaction, the node relies on the members in its quorum set. A node's \"quorum slices\" are the list of nodes from quorum set such that if all members of the quorum slice agree about the state of the system, then the node considers them right. Each node unilaterally declares its quorum slices. As long as the majority of nodes are not malicious, the security of the system is ensured. Various overlapping quorum slices across the network make it almost impossible for the majority of nodes to collude to control consensus. Safety and fault tolerance are preferred over liveness . In case of an accidental fork, the system is halted until consensus is reached. This is important in banking applications. A consensus that requires only message exchange followed by a voting process leads to high messages volume per second and less expenditure of electricity than PoW. However, a criticism of pure FBA is that it leads to fragile structures of constituent nodes, permitting topology scenarios where a single node failure can cause a network-wide failure. For example, in November 2021 , four validators in Ripple went down which halted the whole network for 15 minutes because consensus could no longer be reached. Proof of Stake Then came the Proof of Stake (PoS) consensus algorithms (e.g. Casper). This type of consensus algorithms no longer needs the computational power used for block validation but rather an amount staked by network participants (being single or in clusters). Staking is putting some native cryptocurrency as collateral to become a validating entity in the network. The higher the stake, the higher the chances of getting to write a block on behalf of the whole ecosystem and in return receive some cryptocurrency. Two keywords are different here compared to PoW: computers that participate in PoS consensus are called validators , and once a block has been created and accepted by the network, it is said to be forged (not mined). Then the validator gets some reward. If a user does not want to run his own validator, then he can generally simply \"delegate\" his tokens to the validators, which is like staking without running his own validator. The weight of the delegated tokens is added to the staked tokens of the chosen validator, who gets to keep part of the reward. Malicious validators get penalized by losing some of their staked currency. This is often referred to as slashing . One key difference between PoS and PoW is that a block on a PoS chain has deterministic finality, meaning that once it has been accepted fully, there is no way to ever undo it. In PoW, it can theoretically always be undone by creating a longer valid chain of blocks. PoS is relatively faster than PoW as there is no need to solve complex cryptographic puzzles to get to be the block writer and the process is much more streamlined. Also no wastage of energy means PoS is more environment friendly. PoS algorithms based on BFT need 2/3+1 of the nodes to be honest, so the required threshold is higher than PoW where an attacker needs 51% of all computing power. Sustainability and security are achieved in a PoS model like the one in Cardano. In PoS, an explicit agreement is required before a block becomes valid. If a sufficient number of validators do not sign the block, it gets rejected, even if all consensus rule is followed by the block. A block becomes valid only after the necessary number of participants to the consensus algorithm explicitly vote for it by signing it. This means that a lot of communication is needed. In general, voting on the next correct block requires multiple rounds of communication (preparation round, confirmation round and commit round), and the more validators there are, the more overhead there is for everyone to get their messages to the leader of a round. Scalability is negatively impacted by the high amount of communication. Also the higher the adoption, the higher the total value represented by the native cryptocurrency and therefore higher the value that needs to be locked up as staking by the nodes. This means the native currency's full potential is not utilized as, once it is locked, it can no longer be used for any other purposes. The network\u2019s security is proportional to the value of stake committed. Sharding and Layer 2 Solutions For solving this scaling issue, 'sharding' is introduced (e.g. in the case of Ethereum 2.0 ). Sharding is a way of spreading the computing and storage workload from a blockchain network, so that each node no longer has to process the entire network's transactional load. Each node only needs to maintain info corresponding to its specific shard or partition. For example, addresses starting with \u201c0x00\u201d are part of shard 1 and addresses starting with \u201c0x01\u201d are part of shard 2 etc. A mechanism for inter-shard communication exists so that the whole blockchain is still viewable from any node. All the shards are connected to the \u201cBeacon chain\u201d which has access to all the chain's history and acts as an orchestrator of the whole network. The consensus between all the shards is maintained by a \"beacon chain\". However, sharding has its challenges. Corrupting nodes in a given shard by an attacker may lead to permanent loss of data. One way to tackle this issue is by randomly assigning nodes to certain shards and constantly reassigning them at random intervals. This random sampling would make it difficult for hackers to know when and where to corrupt a shard. Sharding has another problem: smart contract compatible blockchains are no longer composable. For example, if a Decentralized Finance (DeFi) platform built on top of Ethereum relies on or combines few other projects, then in a sharded system of Ethereum 2.0, ideally they should be on the same shard which may not always be the case. If they are not on the same shard then they all need to support asynchronous updates which is not ideal . Avalanche The next level solution for scalability in terms of consensus algorithms is by Avalanche consensus which uses Snow algorithm with PoS as the underlying sybil control mechanism . In Snow consensus, in short, a particular node randomly sub-samples from other nodes multiple times across multiple cycles and eventually arrives at a consensus. This is then recorded in a Directed Acyclic Graph (DAG) as opposed to networks that work with only a chain of Blocks and no structure for the transaction history. Every node has its own view of the current consensus state; they do not progress in lock-step as they do for BFT algorithms. However, the local view of each node is eventually consistent with the view of all other nodes on the network. Snow consensus can easily scale to a higher level compared to PoW or PoS. Because the nodes only need to communicate with random sub-samples of the network for each round, the communicational complexity is much lower than BFT. If these are logarithmically smaller than the entire set of validators on the network, the performance does not decrease exponentially with the size of the validator set. However, one common issue for PoS blockchains remains: the security of the network depends on the ratio of capital locked up for validation. Usually many PoS based networks impose a penalty for malicious validators or validators that are not online continuously by slashing which is taking away some of the staked tokens as a punishment. In case of Snow consensus, nodes monitor uptime and nodes do not receive rewards when they are not up long enough. Avalanche is a leaderless protocol, which is the first one of its kind. In BFT PoS, there is a leader for every round that has to propose the block; in Avalanche, everyone can propose state changes concurrently. Flare Flare network combines the best of both worlds by introducing FBA into the Snow consensus of Avalanche. Flare\u2019s FBA achieves safety without relying too much on economic incentives that can interfere with high-value use cases. In Flare, each node has its own local set of validators or quorum set also known as Unique Node List (UNL). An added requirement compared to the usual FBA is that there should be a minimum level of overlap between UNLs. This makes the network more resistant to single node failure as well as Sybil attack resistant. This is because a malicious actor is unlikely to be able to spin up many node instances that manage to integrate the UNL list of other existing quorum sets. Flare Consensus Protocol (FCP) is leaderless, asynchronous Byzantine fault tolerant and highly scalable due to its usage of a novel networking complexity-reduction technique called federated virtual voting. Details on this can be found in this article on consensus . The native token, Spark, is used to delegate to the price providers for Flare Time Series Oracle (FTSO) without having any locking thereby enabling other usage of Spark at the same time. There is no hard link between the safety of the network and the value of native token Spark which allows greater flexibility for how Spark can be used. The incentive to become a validator in Flare is as follows. In Flare, there are two potentially overlapping sets of validators: the local set of validators that we rely on for consensus the set of underlying chain validators/miners, that can propose blocks When someone proposes a block, they receive a reward. However, someone running a price provider or a proof attestor also needs to run a node, which results in many more nodes than just the underlying chain validators/miners. Below is a summary of characteristics of various distributed networks: Crypto Project Decentralization Security Scalability Energy Efficient Composability Throughput Bitcoin(PoW) \u2705 \u2705 \u274c \u274c \u274c \u274c Ethereum(PoW) \u2705 \u2705 \u274c \u274c \u2705 \u274c Eth2.0(PoS+Sharding) \u2705 \u2705 \u2705 \u2705 \u274c \u2705 Avalanche(PoS) \u2705 \u2705 \u274c \u2705 \u2705 \u2705 Flare(Ava+FBA+UNL) \u2705 \u2705 \u2705 \u2705 \u2705 \u2705","title":"Scaling"},{"location":"flare/scaling/#a-brief-history-of-decentralized-scaling","text":"This article covers how the scaling solutions for decentralized systems in the crypto world have evolved with regards to the consensus algorithms that have been used, what their problems are and how Flare tries to address them. It also addresses how various blockchains fare in the realm of the blockchain trilemma which roughly states that it is hard to achieve more than 2 out of 3 from security, scalability and decentralization. The concepts at the foundation of blockchain technology, such as cryptography (for data encryption) and peer-to-peer networks (that handle information exchange in a distributed way) were around before the 2000s. However, there was no concept of a truly digital currency due to the double spending problem .","title":"A Brief History of Decentralized Scaling"},{"location":"flare/scaling/#proof-of-work","text":"Then came Satoshi Nakamoto who introduced the pre-existing concept of Proof of Work (PoW) on top of a Blockchain which acts as a distributed ledger of all the transactions of currency. So now imagine Brian has 100 bitcoins. There are many machines/nodes which have this information that Brian has 100 bitcoins. If Brian sends 50 of them to Alice then certain things happen before the transfer gets finalized. Nodes collect pending transactions (sent out by wallet applications) in a memory pool. When a node solves the computational problem, it gets to pick the transactions from its memory pool that get included in the next block and thus become part of the consensus state. However, only under the condition that the block follows the rules on transaction validity, i.e. does not include any transactions spending the same unit of account twice. This is termed as \"Nakamoto consensus\". Check this visualization of how PoW consensus might work and create a chain of blocks. PoW by itself is not very new. A similar implementation was thought of in 1992 to combat email spam . Using the Hashcash system, every email has some form of simple proof of work, which makes it is easy for someone to send an email to a friend but makes it difficult for a spammer to send millions of emails at the same time. In theory, PoW can scale infinitely. The more successful the network is, the higher the demand for the token. The higher the demand for the token, the more its value increases. As the price increases, more miners can profitably run miners, and more power is wasted. Also, some PoW based blockchains like Ethereum suffer from huge transaction fees as well as low throughput of 10-15 transactions per second which make them impractical for true global usage. Bitcoin's PoW has the potential to scale a lot in terms of a payment system, but the wastage of energy is its biggest problem.","title":"Proof of Work"},{"location":"flare/scaling/#federated-byzantine-agreement","text":"Byzantine Generals Problem is the problem when there are various nodes communicating and a few of them are \u201cByzantine\u201d \u2014 either faulty or malicious. Byzantine nodes might not reply to messages, or deliberately send wrong information like false signatures, double spent transactions, etc. Therefore, any blockchain should have mechanisms to solve the Byzantine Generals Problem. In case of Bitcoin, the mechanism is PoW. Federated Byzantine Agreement (FBA) consensus in a family of consensus protocols, which eliminates Byzantine faults, and provides deterministic finality (unlike PoW which has only probabilistic finality), without having the selection of validators as part of the protocol itself. This was introduced by Stellar . In FBA, every node has its \u201cquorum set\u201d which is a list of other nodes which this particular node trusts. Therefore, to achieve consensus for a transaction, the node relies on the members in its quorum set. A node's \"quorum slices\" are the list of nodes from quorum set such that if all members of the quorum slice agree about the state of the system, then the node considers them right. Each node unilaterally declares its quorum slices. As long as the majority of nodes are not malicious, the security of the system is ensured. Various overlapping quorum slices across the network make it almost impossible for the majority of nodes to collude to control consensus. Safety and fault tolerance are preferred over liveness . In case of an accidental fork, the system is halted until consensus is reached. This is important in banking applications. A consensus that requires only message exchange followed by a voting process leads to high messages volume per second and less expenditure of electricity than PoW. However, a criticism of pure FBA is that it leads to fragile structures of constituent nodes, permitting topology scenarios where a single node failure can cause a network-wide failure. For example, in November 2021 , four validators in Ripple went down which halted the whole network for 15 minutes because consensus could no longer be reached.","title":"Federated Byzantine Agreement"},{"location":"flare/scaling/#proof-of-stake","text":"Then came the Proof of Stake (PoS) consensus algorithms (e.g. Casper). This type of consensus algorithms no longer needs the computational power used for block validation but rather an amount staked by network participants (being single or in clusters). Staking is putting some native cryptocurrency as collateral to become a validating entity in the network. The higher the stake, the higher the chances of getting to write a block on behalf of the whole ecosystem and in return receive some cryptocurrency. Two keywords are different here compared to PoW: computers that participate in PoS consensus are called validators , and once a block has been created and accepted by the network, it is said to be forged (not mined). Then the validator gets some reward. If a user does not want to run his own validator, then he can generally simply \"delegate\" his tokens to the validators, which is like staking without running his own validator. The weight of the delegated tokens is added to the staked tokens of the chosen validator, who gets to keep part of the reward. Malicious validators get penalized by losing some of their staked currency. This is often referred to as slashing . One key difference between PoS and PoW is that a block on a PoS chain has deterministic finality, meaning that once it has been accepted fully, there is no way to ever undo it. In PoW, it can theoretically always be undone by creating a longer valid chain of blocks. PoS is relatively faster than PoW as there is no need to solve complex cryptographic puzzles to get to be the block writer and the process is much more streamlined. Also no wastage of energy means PoS is more environment friendly. PoS algorithms based on BFT need 2/3+1 of the nodes to be honest, so the required threshold is higher than PoW where an attacker needs 51% of all computing power. Sustainability and security are achieved in a PoS model like the one in Cardano. In PoS, an explicit agreement is required before a block becomes valid. If a sufficient number of validators do not sign the block, it gets rejected, even if all consensus rule is followed by the block. A block becomes valid only after the necessary number of participants to the consensus algorithm explicitly vote for it by signing it. This means that a lot of communication is needed. In general, voting on the next correct block requires multiple rounds of communication (preparation round, confirmation round and commit round), and the more validators there are, the more overhead there is for everyone to get their messages to the leader of a round. Scalability is negatively impacted by the high amount of communication. Also the higher the adoption, the higher the total value represented by the native cryptocurrency and therefore higher the value that needs to be locked up as staking by the nodes. This means the native currency's full potential is not utilized as, once it is locked, it can no longer be used for any other purposes. The network\u2019s security is proportional to the value of stake committed.","title":"Proof of Stake"},{"location":"flare/scaling/#sharding-and-layer-2-solutions","text":"For solving this scaling issue, 'sharding' is introduced (e.g. in the case of Ethereum 2.0 ). Sharding is a way of spreading the computing and storage workload from a blockchain network, so that each node no longer has to process the entire network's transactional load. Each node only needs to maintain info corresponding to its specific shard or partition. For example, addresses starting with \u201c0x00\u201d are part of shard 1 and addresses starting with \u201c0x01\u201d are part of shard 2 etc. A mechanism for inter-shard communication exists so that the whole blockchain is still viewable from any node. All the shards are connected to the \u201cBeacon chain\u201d which has access to all the chain's history and acts as an orchestrator of the whole network. The consensus between all the shards is maintained by a \"beacon chain\". However, sharding has its challenges. Corrupting nodes in a given shard by an attacker may lead to permanent loss of data. One way to tackle this issue is by randomly assigning nodes to certain shards and constantly reassigning them at random intervals. This random sampling would make it difficult for hackers to know when and where to corrupt a shard. Sharding has another problem: smart contract compatible blockchains are no longer composable. For example, if a Decentralized Finance (DeFi) platform built on top of Ethereum relies on or combines few other projects, then in a sharded system of Ethereum 2.0, ideally they should be on the same shard which may not always be the case. If they are not on the same shard then they all need to support asynchronous updates which is not ideal .","title":"Sharding and Layer 2 Solutions"},{"location":"flare/scaling/#avalanche","text":"The next level solution for scalability in terms of consensus algorithms is by Avalanche consensus which uses Snow algorithm with PoS as the underlying sybil control mechanism . In Snow consensus, in short, a particular node randomly sub-samples from other nodes multiple times across multiple cycles and eventually arrives at a consensus. This is then recorded in a Directed Acyclic Graph (DAG) as opposed to networks that work with only a chain of Blocks and no structure for the transaction history. Every node has its own view of the current consensus state; they do not progress in lock-step as they do for BFT algorithms. However, the local view of each node is eventually consistent with the view of all other nodes on the network. Snow consensus can easily scale to a higher level compared to PoW or PoS. Because the nodes only need to communicate with random sub-samples of the network for each round, the communicational complexity is much lower than BFT. If these are logarithmically smaller than the entire set of validators on the network, the performance does not decrease exponentially with the size of the validator set. However, one common issue for PoS blockchains remains: the security of the network depends on the ratio of capital locked up for validation. Usually many PoS based networks impose a penalty for malicious validators or validators that are not online continuously by slashing which is taking away some of the staked tokens as a punishment. In case of Snow consensus, nodes monitor uptime and nodes do not receive rewards when they are not up long enough. Avalanche is a leaderless protocol, which is the first one of its kind. In BFT PoS, there is a leader for every round that has to propose the block; in Avalanche, everyone can propose state changes concurrently.","title":"Avalanche"},{"location":"flare/scaling/#flare","text":"Flare network combines the best of both worlds by introducing FBA into the Snow consensus of Avalanche. Flare\u2019s FBA achieves safety without relying too much on economic incentives that can interfere with high-value use cases. In Flare, each node has its own local set of validators or quorum set also known as Unique Node List (UNL). An added requirement compared to the usual FBA is that there should be a minimum level of overlap between UNLs. This makes the network more resistant to single node failure as well as Sybil attack resistant. This is because a malicious actor is unlikely to be able to spin up many node instances that manage to integrate the UNL list of other existing quorum sets. Flare Consensus Protocol (FCP) is leaderless, asynchronous Byzantine fault tolerant and highly scalable due to its usage of a novel networking complexity-reduction technique called federated virtual voting. Details on this can be found in this article on consensus . The native token, Spark, is used to delegate to the price providers for Flare Time Series Oracle (FTSO) without having any locking thereby enabling other usage of Spark at the same time. There is no hard link between the safety of the network and the value of native token Spark which allows greater flexibility for how Spark can be used. The incentive to become a validator in Flare is as follows. In Flare, there are two potentially overlapping sets of validators: the local set of validators that we rely on for consensus the set of underlying chain validators/miners, that can propose blocks When someone proposes a block, they receive a reward. However, someone running a price provider or a proof attestor also needs to run a node, which results in many more nodes than just the underlying chain validators/miners. Below is a summary of characteristics of various distributed networks: Crypto Project Decentralization Security Scalability Energy Efficient Composability Throughput Bitcoin(PoW) \u2705 \u2705 \u274c \u274c \u274c \u274c Ethereum(PoW) \u2705 \u2705 \u274c \u274c \u2705 \u274c Eth2.0(PoS+Sharding) \u2705 \u2705 \u2705 \u2705 \u274c \u2705 Avalanche(PoS) \u2705 \u2705 \u274c \u2705 \u2705 \u2705 Flare(Ava+FBA+UNL) \u2705 \u2705 \u2705 \u2705 \u2705 \u2705","title":"Flare"},{"location":"flow/cadence/","text":"Cadence Introduction Cadence is a resource-oriented programming language specifically designed for smart-contract programming. Goals Cadence Language was designed with these three goals in mind: Safety and security Safety is the underlying reliability of any smart contract. Security is the prevention of attacks on the network or smart contracts. Clarity Code needs to be easy to read, and its meaning should be as unambiguous as possible. Simplicity Writing code and creating programs should be as approachable as possible. Features Some features of the Cadence programming language are: type safety and a strong static type system resource-oriented programming resources are types that can only exist in one location at a time and cannot be copied, lost or stolen; Thus there is a concept of scarcity and ownership over these objects built-in pre-conditions and post-conditions for functions and transactions capability-based security access to objects is restricted only to the owner of the object and those who have a valid reference to it Terminology Invalid : The invalid program is not even be allowed to run. The program error is detected and reported statically by the type checker. Run-time error : The erroneous program can run, but bad behavior will result in the execution of the program being aborted. Account Accounts are address on the flow network, e.g. 0xf233dcee88fe0abe . Every account can be accessed through two types: PublicAccount and AuthAccount , each corresponding to a level of accessibility. Cadence transactions consist of four optional phases: prepare , precondition , execution and postconditions . Phases can be omitted, but, if present, they must appear in that order. Each account signing the transaction appears as an argument in the prepare() phase. These signers appear as arguments of AuthAccount type, and their number must match the number of signers of the transaction. The prepare phase is the only place where direct access to the signing accounts is possible. transaction { prepare ( account : AuthAccount , account2 : AuthAccount , ( .. .), accountN : AuthAccount ) { // ... } } PublicAccount PublicAccount represents the publicly available information about an account. This information can be retrieved about any account using the getAccount() function. struct PublicAccount { let address : Address // The FLOW balance of the default vault of this account let balance : UFix64 // The FLOW balance of the default vault of this account that is available to be moved let availableBalance : UFix64 // Amount of storage used by the account, in bytes let storageUsed : UInt64 // storage capacity of the account, in bytes let storageCapacity : UInt64 // Contracts deployed to the account let contracts : PublicAccount . Contracts // Keys assigned to the account let keys : PublicAccount . Keys // Storage operations fun getCapability < T > ( _ path : PublicPath ) : Capability < T > fun getLinkTarget ( _ path : CapabilityPath ) : Path ? struct Contracts { let names : [ String ] fun get ( name : String ) : DeployedContract ? } struct Keys { // Returns the key at the given index, if it exists. // Revoked keys are always returned, but they have `isRevoked` field set to true. fun get ( keyIndex : Int ) : AccountKey ? } } AuthAccount On the other hand, AuthAccount represents an authorized account. Authorized accounts can only be encountered in the prepare function of a signed transaction. struct AuthAccount { let address : Address // The FLOW balance of the default vault of this account let balance : UFix64 // The FLOW balance of the default vault of this account that is available to be moved let availableBalance : UFix64 // Amount of storage used by the account, in bytes let storageUsed : UInt64 // storage capacity of the account, in bytes let storageCapacity : UInt64 // Contracts deployed to the account let contracts : AuthAccount . Contracts // Keys assigned to the account let keys : AuthAccount . Keys // Key management // Adds a public key to the account. // The public key must be encoded together with their signature algorithm, hashing algorithm and weight. // This method is currently deprecated and is available only for the backward compatibility. // `keys.add` method can be use instead. fun addPublicKey ( _ publicKey : [ UInt8 ]) // Revokes the key at the given index. // This method is currently deprecated and is available only for the backward compatibility. // `keys.revoke` method can be use instead. fun removePublicKey ( _ index : Int ) // Account storage API (see the section below for documentation) fun save < T > ( _ value : T , to : StoragePath ) fun load < T > ( from : StoragePath ) : T ? fun copy < T : AnyStruct > ( from : StoragePath ) : T ? fun borrow < T : & Any > ( from : StoragePath ) : T ? fun link < T : & Any > ( _ newCapabilityPath : CapabilityPath , target : Path ) : Capability < T >? fun getCapability < T > ( _ path : CapabilityPath ) : Capability < T > fun getLinkTarget ( _ path : CapabilityPath ) : Path ? fun unlink ( _ path : CapabilityPath ) struct Contracts { // The names of each contract deployed to the account let names : [ String ] fun add ( name : String , code : [ UInt8 ], .. . contractInitializerArguments ) : DeployedContract fun update__experimental ( name : String , code : [ UInt8 ]) : DeployedContract fun get ( name : String ) : DeployedContract ? fun remove ( name : String ) : DeployedContract ? } struct Keys { // Adds a new key with the given hashing algorithm and a weight, and returns the added key. fun add ( publicKey : PublicKey , hashAlgorithm : HashAlgorithm , weight : UFix64 ) : AccountKey // Returns the key at the given index, if it exists, or nil otherwise. // Revoked keys are always returned, but they have `isRevoked` field set to true. fun get ( keyIndex : Int ) : AccountKey ? // Marks the key at the given index revoked, but does not delete it. // Returns the revoked key if it exists, or nil otherwise. fun revoke ( keyIndex : Int ) : AccountKey ? } } struct DeployedContract { let name : String let code : [ UInt8 ] } Account Storage Each account has storage, and this is where resources and structs can be persisted. Authorized accounts have full access to the account storage. Objects in storage are stored under paths. Each storage path location corresponds to a single register. Paths correspond to the Key part of the register ID. Paths have a format of /<domain>/<identifier> . There are three valid domains \u2014 storage , public and private . Objects in storage are always stored in the storage domain \u2014 meaning this is where resources and any other data is kept. The public domain allows the account to let other accounts access the objects, links to storage , inside it. Resources Resources are types that can exist only in one memory location at a time. At the end of a function which has resources in scope, resources must either be moved or destroyed . After it was destroyed , the resource can no longer be used. Moving a resource means either assigning it to a different constant or a variable, passing it as an argument to another function, or returning it from a function. After the resource was moved \u2014 e.g. assigned to a const , the previous reference to the resource is invalid and cannot be used anymore. To make the resource behavior clear and explicit, the prefix @ must be used in all type annotations dealing with resources, and the resource movement is denoted with the move operator \u2014 <- . // Resource definition. pub resource SomeResource { // The resource has a single field &#0151 the `value` integer. pub var value : Int // Define the resource initializer init ( value : Int ) { self . value = value } } // Resource is created using the `create` keyword. // Note that the const `a` has type of `@SomeResource`. let a : @ SomeResurce <- create SomeResource ( value : 2 ) // Resource is moved from const a to const b. // `a` can no longer be used to access the resource. let b <- a // Resource is destroyed. destroy b // Neither `a` or `b` can now be used to access the resource as it was destroyed. When a resource is returned from a function, the function caller has the responsibility to use the returned resource. // This function is invalid as it does not use the resource. pub fun invalid_use ( r : @ SomeResource ) { } // This function uses the resource by moving it to a new const. // This new const is used by being returned from the function. pub fun use ( res : @ SomeResource ) : @ SomeResource { let moved <- res // Note that the return call still uses the `move` operator. return <- moved } // Create a new resource. let a : @ SomeResource <- create SomeResource ( value : 3 ) // Move the function return value to a new const. // The const `a` can no longer be used to access the resource. let result <- use ( res : <- a ) // Destroy the resource. destroy result Resource can have a destructor, which is called when the resource is destroyed. pub resource SomeResource { destroy () { // Some logic here, e.g. decrement the variable indicating the number of resources in existence. } } Since the resource variables cannot be assigned to, there are two options to replace the values of resource variables \u2014 the swap operator ( <-> ) and the shift operator ( <- target <- ) pub resource SomeResource {} var x <- create SomeResource () var y <- create SomeResource () // Swap the resources. x < -> y // Alternatively, use the shift operator. // The shift operator moves the resource from `x` to `oldX`. // At the same time, `x` receives the value of the new resource. let oldX <- x <- create SomeResource () // oldX still needs to be used. Resources have an implicit unique identifier in the form of the predeclared public field uuid of type UInt64 . This field is incremented on each resource creation, and can never be the same for two resources, even if some of them were destroyed. Reference Comments Single-line comments in Cadence use // , multi-line comments use /* */ . // This is a comment on a single line. /* This is a comment which spans multiple lines. */ Documentation Comments Cadence has a specific way to create documentation comments. For single-line comments use /// , for multi-line comments use the syntax /** **/ . /// This is a single-line documentation comment /// You can keep them going. /** Multi-line Documentation Comment **/ Names Names can have letters, underscores and numbers, but can only start with any letter or underscore. By convention, variables, constants, and functions have lowercase names; and types have title-case names. // Valid: title-case // PersonID // Valid: with underscore // token_name // Valid: leading underscore and characters // _balance // Valid: leading underscore and numbers _8264 // Valid: characters and number // account2 // Invalid: leading number // 1something // Invalid: invalid character # _#1 // Invalid: various invalid characters // !@#$%^&* Types There are several built-in types for Cadence. Integer Cadence supports the following integer types: Int , Int8 , Int16 , Int32 , Int64 , Int128 , Int256 , UInt , UInt8 , UInt16 , UInt32 , UInt64 , UInt128 , UInt256 , Word8 , Word16 , Word32 , Word64 . The supported built-ins for this type family are: fun toString(): String fun toBigEndianBytes(): [Uint8] Uint8.max Uint8.min Fixed-Point Number Cadence supports the following fixed-point types: Fix64 , UFix64 . The supported built-ins for this type family are: fun toString(): String fun toBigEndianBytes(): [Uint8] Fix64.max Fix64.min Address The supported built-ins for this type family are: fun toString(): String fun toBytes(): [Uint8] String Strings are collections of characters. Strings can be used to work with text in a Unicode-compliant way. Strings are immutable. The supported built-ins for this type family are: let length: Int let utf8: [Uint8] fun concat(_ other: String): String fun slice(from: Int, upTo: Int): String fun decodeHex(): [Uint8] fun toLower(): String fun String.encodeHex(_ data: [Uin8]): String Arrays The supported built-ins for this type family are: let length: Int fun concat(_ array: T): T fun contains(_ element: T) Bool Specific to Variable size Array Functions: fun append(_ element: T): Void fun appendAll(_ array: T) Void fun insert(at index: Int, _ element: T): Void fun remove(at index: Int): T fun removeFirst(): T fun removeLast(): T Dictionary The supported built-ins for this type family are: let length: Int fun insert(key: K, _ value: V) V? fun remove(key: K): V? let keys: [K] let values: [V] fun containsKey(key: K): Bool Floating-Point There is no support for floating point numbers. These kinds of numbers, not natural/cardinal, are handled by the Fixed-Point type. AnyStruct and AnyResource AnyStruct is the base type of all non-resource types, i.e., all non-resource types are a subtype of it. ( Int8 , String , Bool , struct , not type specific) AnyResource is the base type of all resource types. Optionals Optionals are values which can represent the absence of a value. An optional type is declared using the ? suffix for another type. For example, Int is a non-optional integer, and Int? is an optional integer, i.e. either an integer, or nothing. The value representing the absence of a value is nil . Nil-Coalescing Operator The nil-coalescing operator ?? returns the value inside an optional if it contains a value, or returns an alternative value if the optional has no value. let a : Int ? = nil let b : Int = a ?? 42 // `b` is 42, as `a` is nil Force Unwrap The force-unwrap operator ! returns the value inside an optional if it contains a value, or panics and aborts the execution if the optional has no value. Never Never can be used as the return type for functions that never return normally. For example, it is the return type of the panic function. Array Types Arrays either have a fixed or variable size. Fixed-size array types have the form [T; N] , where T is the element type, and N is the size of the array. For example, a fixed-size array of 3 Int8 elements has the type [Int8; 3] . Variable-size array types have the form [T] , where T is the element type. For example, the type [Int16] specifies a variable-size array of elements that have type Int16 . Dictionary Types Dictionary keys must be hashable and equatable, i.e., must implement the Hashable and Equatable interfaces. Most of the built-in types, like booleans and integers, are hashable and equatable, so can be used as keys in dictionaries. Swapping The binary swap operator <-> can be used to exchange the values of two variables. It is only allowed in a statement and is not allowed in expressions. var a = 1 var b = 2 var c = 3 // Invalid: The swap operation cannot be used in an expression. a < -> b < -> c // Instead, the intended swap must be // written in multiple statements. b < -> c a < -> b Argument Passing Behavior Arguments are passed to functions by value. Therefore, values passed to a function are unchanged in the caller's scope when that function returns. Function Preconditions and Postconditions The functions preconditions and postconditions are blocks of expressions that check input and outputs. fun factorial ( _ n : Int ) : Int { pre { // Require the parameter `n` to be greater than or equal to zero. // n >= 0 : \"factorial is only defined for integers greater than or equal to zero\" } post { // Ensure the result will be greater than or equal to 1. // result >= 1 : \"the result must be greater than or equal to 1\" } if n < 1 { return 1 } return n * factorial ( n - 1 ) } factorial ( 5 ) // is `120` // Run-time error: The given argument does not satisfy // the precondition `n >= 0` of the function, the program aborts. // factorial ( - 2 ) Switches As opposed to some languages, Cadence does not feature \"fall through\" in switch statements. Composite Types Composite types can only be declared within a contract and nowhere else. There are two types: Structures They are copied. They are value types. Resources They are moved. They are linear types. Equatable Interface struct interface Equatable { pub fun equals ( _ other : { Equatable }) : Bool } Hashable Interface struct interface Hashable : Equatable { pub hashValue : Int } Restricted Types Resources and structs can be used with the restricted types. For example, a Resource that implements the interface Balance can be accessed by accessing only the functions and/or fields available in Balance . resource interface HasCount { pub let count : Int } pub resource Counter : HasCount { pub var count : Int init ( count : Int ) { self . count = count } pub fun increment () { self . count = self . count + 1 } } let counter : @ Counter <- create Counter ( count : 42 ) counter . count // is `42` counter . increment () counter . count // is `43` // Move the resource in variable `counter` to a new variable `restrictedCounter`, // but typed with the restricted type `Counter{HasCount}`: // The variable may hold any `Counter`, but only the functionality // defined in the given restriction, the interface `HasCount`, may be accessed // let restrictedCounter : @ Counter { HasCount } <- counter // Invalid: Only functionality of restriction `Count` is available, // i.e. the read-only field `count`, but not the function `increment` of `Counter` // restrictedCounter . increment () Events Events can only be declared within contracts and cannot use resources as parameters or outputs. // Invalid: An event cannot be declared globally // event GlobalEvent ( field : Int ) pub contract Events { // Event with explicit argument labels // event BarEvent ( labelA fieldA : Int , labelB fieldB : Int ) // Invalid: A resource type is not allowed to be used // because it would be moved and lost // event ResourceEvent ( resourceField : @ Vault ) } Emitting Events To emit an event, the keyword emit is used. For example, emitting an event of the type Test(field: Int) : emit Test(1) . Core Events There are some core events built-in in Cadence. pub event AccountCreated(address: Address) pub event AccountKeyAdded(address: Address, publicKey: [UInt8]) pub event AccountKeyRemoved(address: Address, publicKey: [UInt8]) pub event AccountContractAdded(address: Address, codeHash: [UInt8], contract: String) pub event AccountContractUpdated(address: Address, codeHash: [UInt8], contract: String) pub event AccountContractRemoved(address: Address, codeHash: [UInt8], contract: String) Contracts A contract in Cadence is a collection of definitions of interfaces, structs, resources, data (its state) and code (its functions). Contracts live in the contract storage area of an account, and they can be added, updated and removed. Composite types \u2014 structs, resources, events and interfaces for these types have to be defined in a contract. However, there is an exception to this rule, since there is a number of native event types that are not defined in a contract, but built into the Cadence runtime itself. These events are emitted on user account creation, account key addition or removal, and contract deployment, update or removal. Contracts themselves are types, similar to composite types, but cannot be used as values, copied or moved like resources or structs. Example of a simple contract is given below: // Name of this contract is HelloWorld. pub contract HelloWorld { // Public constants &#0151 state fields. pub let description : String pub let greeting : String // init function, called when a contract is created. init ( description : String ) { self . description = description self . greeting = \"Hello World!\" } // Public function hello(), returning a string. // This function can be called by anyone importing the contract. pub fun hello () : String { return self . greeting } } This contract can be imported by Cadence scripts, transactions or other contracts at the beginning of the script, transaction or contract definition. // 0x01 is the account address where the contract is deployed. import HelloWorld from 0x01 // Invoke the hello() function of the imported contract. log ( HelloWorld . hello ()) Any number of contracts can be present on an account, which could include an arbitrary amount of data. Each contract has an implicit field \u2014 let account: AuthAccount , which is the account in which the contract is deployed. Deploying a Contract A new contract can be deployed to an account using the add function. An example transaction deploying a HelloWorld contract is given below: transaction ( code : String ) { prepare ( signer : AuthAccount ) { signer . contracts . add ( name : \"HelloWorld\" , code : code . decodeHex (), description : \"This is a new contract on an existing account\" ) } } When executing a transaction with this Cadence script, given a string argument that is a hex-encoded text of the Cadence contract, a new contract gets deployed to the account that signed the transaction. The contract's text can then be found in the code.HelloWorld account register. An event of type flow.AccountContractAdded is emitted as a result of contract deployment. This and a number of other events are built into the Cadence standard library . Another register is created in the given account, prefixed contract , followed by the \\x1F character and the contract name. This register holds the state of the contract. Updating a Contract The contract's updates are currently experimental. Updates to existing contracts are done using the update__experimental function. Contract updates are very similar to the contract deployments, except the init() function of the contract is not invoked on update. transaction ( code : String ) { prepare ( signer : AuthAccount ) { signer . contracts . update__experimental ( name : \"HelloWorld\" , code : code . decodeHex () ) } } There is a number of limitations to the types of updates that can be made to a contract. For example, it is valid to remove a field or change its access modifier from public to private, but it is invalid to add a field, or change its type. This is done in order to ensure data consistency, since changing a contract changes how the program interprets the data, but does not change the actual stored data. After a successful contract update, an event of flow.AccountContractUpdated type is emitted. Removing a Contract An existing contract can be deleted from an account using the remove function. Example of a transaction removing a contract: transaction ( name : String ) { prepare ( signer : AuthAccount ) { // name is the name of the contract that should be removed signer . contracts . remove ( name : name , ) } } After a successful contract removal, an event of flow.AccountContractRemoved type is emitted. Flow Contracts There are core Cadence contracts that are deployed on the Flow network as soon as it is bootstrapped. These contracts are essential to the normal operation of the Flow network. FungibleToken The Flow Fungible Token standard is defined in 0xf233dcee88fe0abe for the mainnet. It defines the FungibleToken contract interface, to which all fungible token contracts need to conform to. The Vault resource is also defined in the same contract, which is the resource that each account needs to have in storage in order to own tokens. Resources and interfaces defined here allow sending and receiving of tokens peer-to-peer, by withdrawing tokens from one user's Vault and depositing them to another user's Vault. FlowToken Implementation of the FungibleToken interface for the FLOW token. FlowServiceAccount The Flow Service Account is an account like any other on Flow except it is responsible for managing core network operations. The Service Account can be referred to as the keeper of core network parameters which will be managed algorithmically over time but are currently set by a group of core contributors to ensure ease of updates to the network in this early stage of its development. FlowEpoch Top-level smart contract that manages the lifecycle of epochs . Epochs are the smallest unit of time during which the set of network operators is either static or can decrease due to nodes election or malicious node presence. Nodes may only leave or join the network at epoch boundaries. The list of participating nodes is kept in the Identity Table . Epochs have three phases \u2014 staking , setup and committed . Staking Phase During the staking phase (or staking Auction), nodes submit staking requests for the next epoch. At the end of this phase, the identity table for the next epoch is determined. Setup Phase During the setup phase , the participants are preparing for the next epoch. Collection nodes submit votes for their cluster's root quorum certificate and consensus nodes run the distributed key generation protocol (DKG) to set up the random beacon. Committed Phase When the committed phase begins, the network is fully prepared to move to the next epoch. Failure to enter this phase before transitioning to the next epoch would be a critical failure and would cause the chain to halt. FlowFees Contract that manages the fee Vault and deposits and withdrawal of fees to and from the fee vault. FlowStorageFees Storage capacity of an account determines how much storage on chain it can use. There is a set of minimum amount of tokens reserved for storage capacity that is paid during account creation, by the creator. If any transaction that results in account's storage use greater that the storage capacity, the transaction will fail. FlowClusterQC This contract manages the process of collecting votes for the root Quorum Certificate (QC) of the upcoming epoch for all Collection node clusters assigned for the next epoch. During the setup phase of the epoch, collection nodes in a specific cluster generate a root block for the cluster. The nodes then need to submit a vote for the root block. Once enough cluster nodes have voted with the same unique vote, the cluster is considered complete. Once all clusters are complete, the QC is complete. FlowDKG This contract manages the process of generating a group key with the participation of all consensus nodes for the upcoming epoch (DKG). FlowIDTableStaking The Flow ID Table and Staking contract manages the node operators' and delegators' information and Flow tokens that are staked as part of the protocol. Nodes submit their stakes during the staking phase of the epoch. When staking, nodes receive an object they can use to stake, unstake and withdraw rewards. Tokens are held in multiple token buckets depending on their status \u2014 staked, unstaking, unstaked or rewarded. This is also where the Delegator functionality, which manages delegation of FLOW to node operators, is specified. FlowStakingCollection This contract defines a collection for staking and delegating objects which allows users to stake and delegate to as many nodes as they want. LockedTokens This contract implements the functionality required to manage FLOW buyers locked tokens from the token sale. Each token holder gets two accounts. The first account is the locked token account, jointly controlled by the user and the token administrator. Token administrator cannot interact with the account without approval from the token holder except for depositing additional tokens, or to unlock existing tokens at the appropriate milestones. Second account is the unlocked user account, which is in full possession of the user. This account stores a capability which allows the account owner to withdraw tokens when they become unlocked, and also to perform staking operations with the locked tokens. StakingProxy This contract defines an interface for node stakers to use to be able to perform common staking actions. Contract Addresses FungibleToken Mainnet: 0xf233dcee88fe0abe NonFungibleToken Mainnet: 0x1d7e57aa55817448 DapperUtilityCoin Mainnet: 0xead892083b3e2c6c","title":"Cadence"},{"location":"flow/cadence/#cadence","text":"","title":"Cadence"},{"location":"flow/cadence/#introduction","text":"Cadence is a resource-oriented programming language specifically designed for smart-contract programming.","title":"Introduction"},{"location":"flow/cadence/#goals","text":"Cadence Language was designed with these three goals in mind: Safety and security Safety is the underlying reliability of any smart contract. Security is the prevention of attacks on the network or smart contracts. Clarity Code needs to be easy to read, and its meaning should be as unambiguous as possible. Simplicity Writing code and creating programs should be as approachable as possible.","title":"Goals"},{"location":"flow/cadence/#features","text":"Some features of the Cadence programming language are: type safety and a strong static type system resource-oriented programming resources are types that can only exist in one location at a time and cannot be copied, lost or stolen; Thus there is a concept of scarcity and ownership over these objects built-in pre-conditions and post-conditions for functions and transactions capability-based security access to objects is restricted only to the owner of the object and those who have a valid reference to it","title":"Features"},{"location":"flow/cadence/#terminology","text":"Invalid : The invalid program is not even be allowed to run. The program error is detected and reported statically by the type checker. Run-time error : The erroneous program can run, but bad behavior will result in the execution of the program being aborted.","title":"Terminology"},{"location":"flow/cadence/#account","text":"Accounts are address on the flow network, e.g. 0xf233dcee88fe0abe . Every account can be accessed through two types: PublicAccount and AuthAccount , each corresponding to a level of accessibility. Cadence transactions consist of four optional phases: prepare , precondition , execution and postconditions . Phases can be omitted, but, if present, they must appear in that order. Each account signing the transaction appears as an argument in the prepare() phase. These signers appear as arguments of AuthAccount type, and their number must match the number of signers of the transaction. The prepare phase is the only place where direct access to the signing accounts is possible. transaction { prepare ( account : AuthAccount , account2 : AuthAccount , ( .. .), accountN : AuthAccount ) { // ... } }","title":"Account"},{"location":"flow/cadence/#publicaccount","text":"PublicAccount represents the publicly available information about an account. This information can be retrieved about any account using the getAccount() function. struct PublicAccount { let address : Address // The FLOW balance of the default vault of this account let balance : UFix64 // The FLOW balance of the default vault of this account that is available to be moved let availableBalance : UFix64 // Amount of storage used by the account, in bytes let storageUsed : UInt64 // storage capacity of the account, in bytes let storageCapacity : UInt64 // Contracts deployed to the account let contracts : PublicAccount . Contracts // Keys assigned to the account let keys : PublicAccount . Keys // Storage operations fun getCapability < T > ( _ path : PublicPath ) : Capability < T > fun getLinkTarget ( _ path : CapabilityPath ) : Path ? struct Contracts { let names : [ String ] fun get ( name : String ) : DeployedContract ? } struct Keys { // Returns the key at the given index, if it exists. // Revoked keys are always returned, but they have `isRevoked` field set to true. fun get ( keyIndex : Int ) : AccountKey ? } }","title":"PublicAccount"},{"location":"flow/cadence/#authaccount","text":"On the other hand, AuthAccount represents an authorized account. Authorized accounts can only be encountered in the prepare function of a signed transaction. struct AuthAccount { let address : Address // The FLOW balance of the default vault of this account let balance : UFix64 // The FLOW balance of the default vault of this account that is available to be moved let availableBalance : UFix64 // Amount of storage used by the account, in bytes let storageUsed : UInt64 // storage capacity of the account, in bytes let storageCapacity : UInt64 // Contracts deployed to the account let contracts : AuthAccount . Contracts // Keys assigned to the account let keys : AuthAccount . Keys // Key management // Adds a public key to the account. // The public key must be encoded together with their signature algorithm, hashing algorithm and weight. // This method is currently deprecated and is available only for the backward compatibility. // `keys.add` method can be use instead. fun addPublicKey ( _ publicKey : [ UInt8 ]) // Revokes the key at the given index. // This method is currently deprecated and is available only for the backward compatibility. // `keys.revoke` method can be use instead. fun removePublicKey ( _ index : Int ) // Account storage API (see the section below for documentation) fun save < T > ( _ value : T , to : StoragePath ) fun load < T > ( from : StoragePath ) : T ? fun copy < T : AnyStruct > ( from : StoragePath ) : T ? fun borrow < T : & Any > ( from : StoragePath ) : T ? fun link < T : & Any > ( _ newCapabilityPath : CapabilityPath , target : Path ) : Capability < T >? fun getCapability < T > ( _ path : CapabilityPath ) : Capability < T > fun getLinkTarget ( _ path : CapabilityPath ) : Path ? fun unlink ( _ path : CapabilityPath ) struct Contracts { // The names of each contract deployed to the account let names : [ String ] fun add ( name : String , code : [ UInt8 ], .. . contractInitializerArguments ) : DeployedContract fun update__experimental ( name : String , code : [ UInt8 ]) : DeployedContract fun get ( name : String ) : DeployedContract ? fun remove ( name : String ) : DeployedContract ? } struct Keys { // Adds a new key with the given hashing algorithm and a weight, and returns the added key. fun add ( publicKey : PublicKey , hashAlgorithm : HashAlgorithm , weight : UFix64 ) : AccountKey // Returns the key at the given index, if it exists, or nil otherwise. // Revoked keys are always returned, but they have `isRevoked` field set to true. fun get ( keyIndex : Int ) : AccountKey ? // Marks the key at the given index revoked, but does not delete it. // Returns the revoked key if it exists, or nil otherwise. fun revoke ( keyIndex : Int ) : AccountKey ? } } struct DeployedContract { let name : String let code : [ UInt8 ] }","title":"AuthAccount"},{"location":"flow/cadence/#account-storage","text":"Each account has storage, and this is where resources and structs can be persisted. Authorized accounts have full access to the account storage. Objects in storage are stored under paths. Each storage path location corresponds to a single register. Paths correspond to the Key part of the register ID. Paths have a format of /<domain>/<identifier> . There are three valid domains \u2014 storage , public and private . Objects in storage are always stored in the storage domain \u2014 meaning this is where resources and any other data is kept. The public domain allows the account to let other accounts access the objects, links to storage , inside it.","title":"Account Storage"},{"location":"flow/cadence/#resources","text":"Resources are types that can exist only in one memory location at a time. At the end of a function which has resources in scope, resources must either be moved or destroyed . After it was destroyed , the resource can no longer be used. Moving a resource means either assigning it to a different constant or a variable, passing it as an argument to another function, or returning it from a function. After the resource was moved \u2014 e.g. assigned to a const , the previous reference to the resource is invalid and cannot be used anymore. To make the resource behavior clear and explicit, the prefix @ must be used in all type annotations dealing with resources, and the resource movement is denoted with the move operator \u2014 <- . // Resource definition. pub resource SomeResource { // The resource has a single field &#0151 the `value` integer. pub var value : Int // Define the resource initializer init ( value : Int ) { self . value = value } } // Resource is created using the `create` keyword. // Note that the const `a` has type of `@SomeResource`. let a : @ SomeResurce <- create SomeResource ( value : 2 ) // Resource is moved from const a to const b. // `a` can no longer be used to access the resource. let b <- a // Resource is destroyed. destroy b // Neither `a` or `b` can now be used to access the resource as it was destroyed. When a resource is returned from a function, the function caller has the responsibility to use the returned resource. // This function is invalid as it does not use the resource. pub fun invalid_use ( r : @ SomeResource ) { } // This function uses the resource by moving it to a new const. // This new const is used by being returned from the function. pub fun use ( res : @ SomeResource ) : @ SomeResource { let moved <- res // Note that the return call still uses the `move` operator. return <- moved } // Create a new resource. let a : @ SomeResource <- create SomeResource ( value : 3 ) // Move the function return value to a new const. // The const `a` can no longer be used to access the resource. let result <- use ( res : <- a ) // Destroy the resource. destroy result Resource can have a destructor, which is called when the resource is destroyed. pub resource SomeResource { destroy () { // Some logic here, e.g. decrement the variable indicating the number of resources in existence. } } Since the resource variables cannot be assigned to, there are two options to replace the values of resource variables \u2014 the swap operator ( <-> ) and the shift operator ( <- target <- ) pub resource SomeResource {} var x <- create SomeResource () var y <- create SomeResource () // Swap the resources. x < -> y // Alternatively, use the shift operator. // The shift operator moves the resource from `x` to `oldX`. // At the same time, `x` receives the value of the new resource. let oldX <- x <- create SomeResource () // oldX still needs to be used. Resources have an implicit unique identifier in the form of the predeclared public field uuid of type UInt64 . This field is incremented on each resource creation, and can never be the same for two resources, even if some of them were destroyed.","title":"Resources"},{"location":"flow/cadence/#reference","text":"","title":"Reference"},{"location":"flow/cadence/#comments","text":"Single-line comments in Cadence use // , multi-line comments use /* */ . // This is a comment on a single line. /* This is a comment which spans multiple lines. */","title":"Comments"},{"location":"flow/cadence/#documentation-comments","text":"Cadence has a specific way to create documentation comments. For single-line comments use /// , for multi-line comments use the syntax /** **/ . /// This is a single-line documentation comment /// You can keep them going. /** Multi-line Documentation Comment **/","title":"Documentation Comments"},{"location":"flow/cadence/#names","text":"Names can have letters, underscores and numbers, but can only start with any letter or underscore. By convention, variables, constants, and functions have lowercase names; and types have title-case names. // Valid: title-case // PersonID // Valid: with underscore // token_name // Valid: leading underscore and characters // _balance // Valid: leading underscore and numbers _8264 // Valid: characters and number // account2 // Invalid: leading number // 1something // Invalid: invalid character # _#1 // Invalid: various invalid characters // !@#$%^&*","title":"Names"},{"location":"flow/cadence/#types","text":"There are several built-in types for Cadence.","title":"Types"},{"location":"flow/cadence/#integer","text":"Cadence supports the following integer types: Int , Int8 , Int16 , Int32 , Int64 , Int128 , Int256 , UInt , UInt8 , UInt16 , UInt32 , UInt64 , UInt128 , UInt256 , Word8 , Word16 , Word32 , Word64 . The supported built-ins for this type family are: fun toString(): String fun toBigEndianBytes(): [Uint8] Uint8.max Uint8.min","title":"Integer"},{"location":"flow/cadence/#fixed-point-number","text":"Cadence supports the following fixed-point types: Fix64 , UFix64 . The supported built-ins for this type family are: fun toString(): String fun toBigEndianBytes(): [Uint8] Fix64.max Fix64.min","title":"Fixed-Point Number"},{"location":"flow/cadence/#address","text":"The supported built-ins for this type family are: fun toString(): String fun toBytes(): [Uint8]","title":"Address"},{"location":"flow/cadence/#string","text":"Strings are collections of characters. Strings can be used to work with text in a Unicode-compliant way. Strings are immutable. The supported built-ins for this type family are: let length: Int let utf8: [Uint8] fun concat(_ other: String): String fun slice(from: Int, upTo: Int): String fun decodeHex(): [Uint8] fun toLower(): String fun String.encodeHex(_ data: [Uin8]): String","title":"String"},{"location":"flow/cadence/#arrays","text":"The supported built-ins for this type family are: let length: Int fun concat(_ array: T): T fun contains(_ element: T) Bool Specific to Variable size Array Functions: fun append(_ element: T): Void fun appendAll(_ array: T) Void fun insert(at index: Int, _ element: T): Void fun remove(at index: Int): T fun removeFirst(): T fun removeLast(): T","title":"Arrays"},{"location":"flow/cadence/#dictionary","text":"The supported built-ins for this type family are: let length: Int fun insert(key: K, _ value: V) V? fun remove(key: K): V? let keys: [K] let values: [V] fun containsKey(key: K): Bool","title":"Dictionary"},{"location":"flow/cadence/#floating-point","text":"There is no support for floating point numbers. These kinds of numbers, not natural/cardinal, are handled by the Fixed-Point type.","title":"Floating-Point"},{"location":"flow/cadence/#anystruct-and-anyresource","text":"AnyStruct is the base type of all non-resource types, i.e., all non-resource types are a subtype of it. ( Int8 , String , Bool , struct , not type specific) AnyResource is the base type of all resource types.","title":"AnyStruct and AnyResource"},{"location":"flow/cadence/#optionals","text":"Optionals are values which can represent the absence of a value. An optional type is declared using the ? suffix for another type. For example, Int is a non-optional integer, and Int? is an optional integer, i.e. either an integer, or nothing. The value representing the absence of a value is nil .","title":"Optionals"},{"location":"flow/cadence/#nil-coalescing-operator","text":"The nil-coalescing operator ?? returns the value inside an optional if it contains a value, or returns an alternative value if the optional has no value. let a : Int ? = nil let b : Int = a ?? 42 // `b` is 42, as `a` is nil","title":"Nil-Coalescing Operator"},{"location":"flow/cadence/#force-unwrap","text":"The force-unwrap operator ! returns the value inside an optional if it contains a value, or panics and aborts the execution if the optional has no value.","title":"Force Unwrap"},{"location":"flow/cadence/#never","text":"Never can be used as the return type for functions that never return normally. For example, it is the return type of the panic function.","title":"Never"},{"location":"flow/cadence/#array-types","text":"Arrays either have a fixed or variable size. Fixed-size array types have the form [T; N] , where T is the element type, and N is the size of the array. For example, a fixed-size array of 3 Int8 elements has the type [Int8; 3] . Variable-size array types have the form [T] , where T is the element type. For example, the type [Int16] specifies a variable-size array of elements that have type Int16 .","title":"Array Types"},{"location":"flow/cadence/#dictionary-types","text":"Dictionary keys must be hashable and equatable, i.e., must implement the Hashable and Equatable interfaces. Most of the built-in types, like booleans and integers, are hashable and equatable, so can be used as keys in dictionaries.","title":"Dictionary Types"},{"location":"flow/cadence/#swapping","text":"The binary swap operator <-> can be used to exchange the values of two variables. It is only allowed in a statement and is not allowed in expressions. var a = 1 var b = 2 var c = 3 // Invalid: The swap operation cannot be used in an expression. a < -> b < -> c // Instead, the intended swap must be // written in multiple statements. b < -> c a < -> b","title":"Swapping"},{"location":"flow/cadence/#argument-passing-behavior","text":"Arguments are passed to functions by value. Therefore, values passed to a function are unchanged in the caller's scope when that function returns.","title":"Argument Passing Behavior"},{"location":"flow/cadence/#function-preconditions-and-postconditions","text":"The functions preconditions and postconditions are blocks of expressions that check input and outputs. fun factorial ( _ n : Int ) : Int { pre { // Require the parameter `n` to be greater than or equal to zero. // n >= 0 : \"factorial is only defined for integers greater than or equal to zero\" } post { // Ensure the result will be greater than or equal to 1. // result >= 1 : \"the result must be greater than or equal to 1\" } if n < 1 { return 1 } return n * factorial ( n - 1 ) } factorial ( 5 ) // is `120` // Run-time error: The given argument does not satisfy // the precondition `n >= 0` of the function, the program aborts. // factorial ( - 2 )","title":"Function Preconditions and Postconditions"},{"location":"flow/cadence/#switches","text":"As opposed to some languages, Cadence does not feature \"fall through\" in switch statements.","title":"Switches"},{"location":"flow/cadence/#composite-types","text":"Composite types can only be declared within a contract and nowhere else. There are two types: Structures They are copied. They are value types. Resources They are moved. They are linear types.","title":"Composite Types"},{"location":"flow/cadence/#equatable-interface","text":"struct interface Equatable { pub fun equals ( _ other : { Equatable }) : Bool }","title":"Equatable Interface"},{"location":"flow/cadence/#hashable-interface","text":"struct interface Hashable : Equatable { pub hashValue : Int }","title":"Hashable Interface"},{"location":"flow/cadence/#restricted-types","text":"Resources and structs can be used with the restricted types. For example, a Resource that implements the interface Balance can be accessed by accessing only the functions and/or fields available in Balance . resource interface HasCount { pub let count : Int } pub resource Counter : HasCount { pub var count : Int init ( count : Int ) { self . count = count } pub fun increment () { self . count = self . count + 1 } } let counter : @ Counter <- create Counter ( count : 42 ) counter . count // is `42` counter . increment () counter . count // is `43` // Move the resource in variable `counter` to a new variable `restrictedCounter`, // but typed with the restricted type `Counter{HasCount}`: // The variable may hold any `Counter`, but only the functionality // defined in the given restriction, the interface `HasCount`, may be accessed // let restrictedCounter : @ Counter { HasCount } <- counter // Invalid: Only functionality of restriction `Count` is available, // i.e. the read-only field `count`, but not the function `increment` of `Counter` // restrictedCounter . increment ()","title":"Restricted Types"},{"location":"flow/cadence/#events","text":"Events can only be declared within contracts and cannot use resources as parameters or outputs. // Invalid: An event cannot be declared globally // event GlobalEvent ( field : Int ) pub contract Events { // Event with explicit argument labels // event BarEvent ( labelA fieldA : Int , labelB fieldB : Int ) // Invalid: A resource type is not allowed to be used // because it would be moved and lost // event ResourceEvent ( resourceField : @ Vault ) }","title":"Events"},{"location":"flow/cadence/#emitting-events","text":"To emit an event, the keyword emit is used. For example, emitting an event of the type Test(field: Int) : emit Test(1) .","title":"Emitting Events"},{"location":"flow/cadence/#core-events","text":"There are some core events built-in in Cadence. pub event AccountCreated(address: Address) pub event AccountKeyAdded(address: Address, publicKey: [UInt8]) pub event AccountKeyRemoved(address: Address, publicKey: [UInt8]) pub event AccountContractAdded(address: Address, codeHash: [UInt8], contract: String) pub event AccountContractUpdated(address: Address, codeHash: [UInt8], contract: String) pub event AccountContractRemoved(address: Address, codeHash: [UInt8], contract: String)","title":"Core Events"},{"location":"flow/cadence/#contracts","text":"A contract in Cadence is a collection of definitions of interfaces, structs, resources, data (its state) and code (its functions). Contracts live in the contract storage area of an account, and they can be added, updated and removed. Composite types \u2014 structs, resources, events and interfaces for these types have to be defined in a contract. However, there is an exception to this rule, since there is a number of native event types that are not defined in a contract, but built into the Cadence runtime itself. These events are emitted on user account creation, account key addition or removal, and contract deployment, update or removal. Contracts themselves are types, similar to composite types, but cannot be used as values, copied or moved like resources or structs. Example of a simple contract is given below: // Name of this contract is HelloWorld. pub contract HelloWorld { // Public constants &#0151 state fields. pub let description : String pub let greeting : String // init function, called when a contract is created. init ( description : String ) { self . description = description self . greeting = \"Hello World!\" } // Public function hello(), returning a string. // This function can be called by anyone importing the contract. pub fun hello () : String { return self . greeting } } This contract can be imported by Cadence scripts, transactions or other contracts at the beginning of the script, transaction or contract definition. // 0x01 is the account address where the contract is deployed. import HelloWorld from 0x01 // Invoke the hello() function of the imported contract. log ( HelloWorld . hello ()) Any number of contracts can be present on an account, which could include an arbitrary amount of data. Each contract has an implicit field \u2014 let account: AuthAccount , which is the account in which the contract is deployed.","title":"Contracts"},{"location":"flow/cadence/#deploying-a-contract","text":"A new contract can be deployed to an account using the add function. An example transaction deploying a HelloWorld contract is given below: transaction ( code : String ) { prepare ( signer : AuthAccount ) { signer . contracts . add ( name : \"HelloWorld\" , code : code . decodeHex (), description : \"This is a new contract on an existing account\" ) } } When executing a transaction with this Cadence script, given a string argument that is a hex-encoded text of the Cadence contract, a new contract gets deployed to the account that signed the transaction. The contract's text can then be found in the code.HelloWorld account register. An event of type flow.AccountContractAdded is emitted as a result of contract deployment. This and a number of other events are built into the Cadence standard library . Another register is created in the given account, prefixed contract , followed by the \\x1F character and the contract name. This register holds the state of the contract.","title":"Deploying a Contract"},{"location":"flow/cadence/#updating-a-contract","text":"The contract's updates are currently experimental. Updates to existing contracts are done using the update__experimental function. Contract updates are very similar to the contract deployments, except the init() function of the contract is not invoked on update. transaction ( code : String ) { prepare ( signer : AuthAccount ) { signer . contracts . update__experimental ( name : \"HelloWorld\" , code : code . decodeHex () ) } } There is a number of limitations to the types of updates that can be made to a contract. For example, it is valid to remove a field or change its access modifier from public to private, but it is invalid to add a field, or change its type. This is done in order to ensure data consistency, since changing a contract changes how the program interprets the data, but does not change the actual stored data. After a successful contract update, an event of flow.AccountContractUpdated type is emitted.","title":"Updating a Contract"},{"location":"flow/cadence/#removing-a-contract","text":"An existing contract can be deleted from an account using the remove function. Example of a transaction removing a contract: transaction ( name : String ) { prepare ( signer : AuthAccount ) { // name is the name of the contract that should be removed signer . contracts . remove ( name : name , ) } } After a successful contract removal, an event of flow.AccountContractRemoved type is emitted.","title":"Removing a Contract"},{"location":"flow/cadence/#flow-contracts","text":"There are core Cadence contracts that are deployed on the Flow network as soon as it is bootstrapped. These contracts are essential to the normal operation of the Flow network.","title":"Flow Contracts"},{"location":"flow/cadence/#fungibletoken","text":"The Flow Fungible Token standard is defined in 0xf233dcee88fe0abe for the mainnet. It defines the FungibleToken contract interface, to which all fungible token contracts need to conform to. The Vault resource is also defined in the same contract, which is the resource that each account needs to have in storage in order to own tokens. Resources and interfaces defined here allow sending and receiving of tokens peer-to-peer, by withdrawing tokens from one user's Vault and depositing them to another user's Vault.","title":"FungibleToken"},{"location":"flow/cadence/#flowtoken","text":"Implementation of the FungibleToken interface for the FLOW token.","title":"FlowToken"},{"location":"flow/cadence/#flowserviceaccount","text":"The Flow Service Account is an account like any other on Flow except it is responsible for managing core network operations. The Service Account can be referred to as the keeper of core network parameters which will be managed algorithmically over time but are currently set by a group of core contributors to ensure ease of updates to the network in this early stage of its development.","title":"FlowServiceAccount"},{"location":"flow/cadence/#flowepoch","text":"Top-level smart contract that manages the lifecycle of epochs . Epochs are the smallest unit of time during which the set of network operators is either static or can decrease due to nodes election or malicious node presence. Nodes may only leave or join the network at epoch boundaries. The list of participating nodes is kept in the Identity Table . Epochs have three phases \u2014 staking , setup and committed .","title":"FlowEpoch"},{"location":"flow/cadence/#staking-phase","text":"During the staking phase (or staking Auction), nodes submit staking requests for the next epoch. At the end of this phase, the identity table for the next epoch is determined.","title":"Staking Phase"},{"location":"flow/cadence/#setup-phase","text":"During the setup phase , the participants are preparing for the next epoch. Collection nodes submit votes for their cluster's root quorum certificate and consensus nodes run the distributed key generation protocol (DKG) to set up the random beacon.","title":"Setup Phase"},{"location":"flow/cadence/#committed-phase","text":"When the committed phase begins, the network is fully prepared to move to the next epoch. Failure to enter this phase before transitioning to the next epoch would be a critical failure and would cause the chain to halt.","title":"Committed Phase"},{"location":"flow/cadence/#flowfees","text":"Contract that manages the fee Vault and deposits and withdrawal of fees to and from the fee vault.","title":"FlowFees"},{"location":"flow/cadence/#flowstoragefees","text":"Storage capacity of an account determines how much storage on chain it can use. There is a set of minimum amount of tokens reserved for storage capacity that is paid during account creation, by the creator. If any transaction that results in account's storage use greater that the storage capacity, the transaction will fail.","title":"FlowStorageFees"},{"location":"flow/cadence/#flowclusterqc","text":"This contract manages the process of collecting votes for the root Quorum Certificate (QC) of the upcoming epoch for all Collection node clusters assigned for the next epoch. During the setup phase of the epoch, collection nodes in a specific cluster generate a root block for the cluster. The nodes then need to submit a vote for the root block. Once enough cluster nodes have voted with the same unique vote, the cluster is considered complete. Once all clusters are complete, the QC is complete.","title":"FlowClusterQC"},{"location":"flow/cadence/#flowdkg","text":"This contract manages the process of generating a group key with the participation of all consensus nodes for the upcoming epoch (DKG).","title":"FlowDKG"},{"location":"flow/cadence/#flowidtablestaking","text":"The Flow ID Table and Staking contract manages the node operators' and delegators' information and Flow tokens that are staked as part of the protocol. Nodes submit their stakes during the staking phase of the epoch. When staking, nodes receive an object they can use to stake, unstake and withdraw rewards. Tokens are held in multiple token buckets depending on their status \u2014 staked, unstaking, unstaked or rewarded. This is also where the Delegator functionality, which manages delegation of FLOW to node operators, is specified.","title":"FlowIDTableStaking"},{"location":"flow/cadence/#flowstakingcollection","text":"This contract defines a collection for staking and delegating objects which allows users to stake and delegate to as many nodes as they want.","title":"FlowStakingCollection"},{"location":"flow/cadence/#lockedtokens","text":"This contract implements the functionality required to manage FLOW buyers locked tokens from the token sale. Each token holder gets two accounts. The first account is the locked token account, jointly controlled by the user and the token administrator. Token administrator cannot interact with the account without approval from the token holder except for depositing additional tokens, or to unlock existing tokens at the appropriate milestones. Second account is the unlocked user account, which is in full possession of the user. This account stores a capability which allows the account owner to withdraw tokens when they become unlocked, and also to perform staking operations with the locked tokens.","title":"LockedTokens"},{"location":"flow/cadence/#stakingproxy","text":"This contract defines an interface for node stakers to use to be able to perform common staking actions.","title":"StakingProxy"},{"location":"flow/cadence/#contract-addresses","text":"FungibleToken Mainnet: 0xf233dcee88fe0abe NonFungibleToken Mainnet: 0x1d7e57aa55817448 DapperUtilityCoin Mainnet: 0xead892083b3e2c6c","title":"Contract Addresses"},{"location":"flow/sdk/","text":"Flow Go SDK Flow Go SDK allows users to interact with the Flow Access API. This allows Go code applications to send requests directly to the network. The SDK has the ability to send transactions, scripts and get information about the state of the network. Sending a Transaction Flow SDK exposes the method SendTransaction to send transactions to the Flow Network. In this example the amount of 100 FLOW tokens will be sent between the service account and the RECEIVER_ACCOUNT . In the prepare phase of the transaction, the reference to the sender's vault is retrieved and the specified amount of tokens is withdrawn to a temporary vault. In the execute phase of the transaction, the FungibleToken.Receiver capability is used to get access to the deposit function of the receiver's vault. The tokens are then deposited to the receiver from the temporary vault. /// 0xFUNGIBLE_TOKEN and 0xFLOW_TOKEN are place for the actual addresses of the contracts that differ from network to network. import FungibleToken from 0xF UNGIBLE_TOKEN import FlowToken from 0xF LOW_TOKEN transaction () { let sentVault : @ FungibleToken . Vault prepare ( signer : AuthAccount ) { // Get a reference to the signer's stored vault let vaultRef = signer . borrow <& FlowToken . Vault > ( from : / storage / flowTokenVault ) ?? panic ( \"Could not borrow reference to the owner's Vault!\" ) // Withdraw tokens from the signer's stored vault self . sentVault <- vaultRef . withdraw ( amount : 100 ) } execute { // Get a reference to the recipient's Receiver let receiverRef = getAccount ( 0 xRECEIVER_ACCOUNT ) . getCapability ( / public / flowTokenReceiver ) . borrow <& { FungibleToken . Receiver } > () ?? panic ( \"Could not borrow receiver reference to the recipient's Vault\" ) // Deposit the withdrawn tokens in the recipient's receiver receiverRef . deposit ( from : <- self . sentVault ) } } Using the SDK the script connects to the access node using client.New(ADDRESS) , then gets the private key and the first account key of the service account and uses it to pay and sign the transaction. Finally, the transaction is sent. package main import ( \"context\" \"github.com/onflow/flow-go-sdk\" \"github.com/onflow/flow-go-sdk/crypto\" ) var ( cadenceScript = \"Cadence transaction code describing token transfer...\" serviceAccountAddress = \"0xADDRESS\" serviceAccountPrivateKey = \"privateKey\" ) func main () { ctx := context . Background () cli , err := client . New ( \"127.0.0.1\" , grpc . WithInsecure ()) if err != nil { panic ( err ) } privateKey , err := crypto . DecodePrivateKeyHex ( crypto . ECDSA_P256 , serviceAccountPrivateKey ) if err != nil { panic ( err ) } addr := flow . HexToAddress ( serviceAccountAddress ) acc , err := cli . GetAccount ( ctx , addr ) if err != nil { panic ( err ) } accountKey := acc . Keys [ 0 ] signer := crypto . NewInMemorySigner ( privateKey , accountKey . HashAlgo ) tx := flow . NewTransaction (). SetPayer ( addr ). SetProposalKey ( addr , accountKey . Index , accountKey . SequenceNumber ). SetScript ([] byte ( cadenceScript )) err := tx . SignEnvelope ( addr , accountKey . Index , signer ) if err != nil { panic ( err ) } err := cli . SendTransaction ( ctx , tx ) if err != nil { panic ( err ) } } Sending a Script Flow SDK exposes the method ExecuteScriptAtLatestBlock to execute a script on the latest block. It also offers two more functions to execute scripts on a block at a defined height or referenced by its id: ExecuteScriptAtBlockHeight and ExecuteScriptAtBlockID respectively. To get the current FLOW token balance from an account the following cadence script should be executed. In this script the ACCOUNT placeholder is the account address of the target balance. import FlowToken from 0xF LOW_TOKEN pub fun main () { let account = getAccount ( 0xACC OUNT ) let balanceRef = account . getCapability <& FlowToken . Vault { FlowToken . Balance } > ( / public / flowTokenBalance ) . borrow () ?? panic ( \"Could not borrow a reference to the account balance\" ) return balanceRef . balance } To execute the script from the Go SDK first it requires to create a new connection to an access node. This is done using client.New(ACCESS_NODE_ADDRESS) . With this connection the script can be executed using ExecuteScriptAtLatestBlock(CONTEXT, CADENCE_SCRIPT, ARGUMENTS) . package main import ( \"context\" \"fmt\" \"github.com/onflow/flow-go-sdk\" ) var ( cadenceScript = \"Cadence script code describing retrieving token balance...\" ) func main () { ctx := context . Background () cli , err := client . New ( \"127.0.0.1\" , grpc . WithInsecure ()) if err != nil { panic ( err ) } script := [] byte ( cadenceScript ) value , err := cli . ExecuteScriptAtLatestBlock ( ctx , script , nil ) if err != nil { panic ( err ) } fmt . Printf ( \"\\nValue: %s\" , value . String ()) }","title":"SDK"},{"location":"flow/sdk/#flow-go-sdk","text":"Flow Go SDK allows users to interact with the Flow Access API. This allows Go code applications to send requests directly to the network. The SDK has the ability to send transactions, scripts and get information about the state of the network.","title":"Flow Go SDK"},{"location":"flow/sdk/#sending-a-transaction","text":"Flow SDK exposes the method SendTransaction to send transactions to the Flow Network. In this example the amount of 100 FLOW tokens will be sent between the service account and the RECEIVER_ACCOUNT . In the prepare phase of the transaction, the reference to the sender's vault is retrieved and the specified amount of tokens is withdrawn to a temporary vault. In the execute phase of the transaction, the FungibleToken.Receiver capability is used to get access to the deposit function of the receiver's vault. The tokens are then deposited to the receiver from the temporary vault. /// 0xFUNGIBLE_TOKEN and 0xFLOW_TOKEN are place for the actual addresses of the contracts that differ from network to network. import FungibleToken from 0xF UNGIBLE_TOKEN import FlowToken from 0xF LOW_TOKEN transaction () { let sentVault : @ FungibleToken . Vault prepare ( signer : AuthAccount ) { // Get a reference to the signer's stored vault let vaultRef = signer . borrow <& FlowToken . Vault > ( from : / storage / flowTokenVault ) ?? panic ( \"Could not borrow reference to the owner's Vault!\" ) // Withdraw tokens from the signer's stored vault self . sentVault <- vaultRef . withdraw ( amount : 100 ) } execute { // Get a reference to the recipient's Receiver let receiverRef = getAccount ( 0 xRECEIVER_ACCOUNT ) . getCapability ( / public / flowTokenReceiver ) . borrow <& { FungibleToken . Receiver } > () ?? panic ( \"Could not borrow receiver reference to the recipient's Vault\" ) // Deposit the withdrawn tokens in the recipient's receiver receiverRef . deposit ( from : <- self . sentVault ) } } Using the SDK the script connects to the access node using client.New(ADDRESS) , then gets the private key and the first account key of the service account and uses it to pay and sign the transaction. Finally, the transaction is sent. package main import ( \"context\" \"github.com/onflow/flow-go-sdk\" \"github.com/onflow/flow-go-sdk/crypto\" ) var ( cadenceScript = \"Cadence transaction code describing token transfer...\" serviceAccountAddress = \"0xADDRESS\" serviceAccountPrivateKey = \"privateKey\" ) func main () { ctx := context . Background () cli , err := client . New ( \"127.0.0.1\" , grpc . WithInsecure ()) if err != nil { panic ( err ) } privateKey , err := crypto . DecodePrivateKeyHex ( crypto . ECDSA_P256 , serviceAccountPrivateKey ) if err != nil { panic ( err ) } addr := flow . HexToAddress ( serviceAccountAddress ) acc , err := cli . GetAccount ( ctx , addr ) if err != nil { panic ( err ) } accountKey := acc . Keys [ 0 ] signer := crypto . NewInMemorySigner ( privateKey , accountKey . HashAlgo ) tx := flow . NewTransaction (). SetPayer ( addr ). SetProposalKey ( addr , accountKey . Index , accountKey . SequenceNumber ). SetScript ([] byte ( cadenceScript )) err := tx . SignEnvelope ( addr , accountKey . Index , signer ) if err != nil { panic ( err ) } err := cli . SendTransaction ( ctx , tx ) if err != nil { panic ( err ) } }","title":"Sending a Transaction"},{"location":"flow/sdk/#sending-a-script","text":"Flow SDK exposes the method ExecuteScriptAtLatestBlock to execute a script on the latest block. It also offers two more functions to execute scripts on a block at a defined height or referenced by its id: ExecuteScriptAtBlockHeight and ExecuteScriptAtBlockID respectively. To get the current FLOW token balance from an account the following cadence script should be executed. In this script the ACCOUNT placeholder is the account address of the target balance. import FlowToken from 0xF LOW_TOKEN pub fun main () { let account = getAccount ( 0xACC OUNT ) let balanceRef = account . getCapability <& FlowToken . Vault { FlowToken . Balance } > ( / public / flowTokenBalance ) . borrow () ?? panic ( \"Could not borrow a reference to the account balance\" ) return balanceRef . balance } To execute the script from the Go SDK first it requires to create a new connection to an access node. This is done using client.New(ACCESS_NODE_ADDRESS) . With this connection the script can be executed using ExecuteScriptAtLatestBlock(CONTEXT, CADENCE_SCRIPT, ARGUMENTS) . package main import ( \"context\" \"fmt\" \"github.com/onflow/flow-go-sdk\" ) var ( cadenceScript = \"Cadence script code describing retrieving token balance...\" ) func main () { ctx := context . Background () cli , err := client . New ( \"127.0.0.1\" , grpc . WithInsecure ()) if err != nil { panic ( err ) } script := [] byte ( cadenceScript ) value , err := cli . ExecuteScriptAtLatestBlock ( ctx , script , nil ) if err != nil { panic ( err ) } fmt . Printf ( \"\\nValue: %s\" , value . String ()) }","title":"Sending a Script"},{"location":"flow/virtual-machine/","text":"Flow Registers The FVM interacts with the Flow execution state by running atomic operations against a ledger. What we call a ledger is a type of key/value storage. It holds an array of key-value pairs called registers. In Flow, the ledger implementation provides a number of functionalities and guarantees, including speed, memory-efficiency, crash resilience (via write-ahead logs and checkpoints), thread safety etc. It is also a stateful key/value storage, and every update to the ledger creates a new state. A limited number of recent states is kept, and updates can be applied to any one of those states. Each ledger register is referenced by an ID \u2014 the key and holds a value \u2014 binary data. Register ID to Ledger Path When referencing storage locations from Flow code, each register is uniquely identified by three components \u2014 owner , controller and key . Owner represents the account to which the register belongs to. Depending on the register, the controller field can be either empty or have the value of the owner field. The key field represents the specific storage location of the account. Definition of a register ID ( source ): type RegisterID struct { Owner string Controller string Key string } Register ID is converted to a ledger.Key by concatenating specific key parts ( source ). const ( KeyPartOwner = uint16 ( 0 ) KeyPartController = uint16 ( 1 ) KeyPartKey = uint16 ( 2 ) ) // ... func RegisterIDToKey ( reg flow . RegisterID ) ledger . Key { return ledger . NewKey ([] ledger . KeyPart { ledger . NewKeyPart ( KeyPartOwner , [] byte ( reg . Owner )), ledger . NewKeyPart ( KeyPartController , [] byte ( reg . Controller )), ledger . NewKeyPart ( KeyPartKey , [] byte ( reg . Key )), }) } In order to map a Ledger key to a ledger path, the key can be converted to a corresponding path using pathfinder.KeyToPath . Depending on the version of the ledger pathfinder version, ledger path is either SHA256 or SHA3-256 (current version) hash of the canonical form of the ledger key. The canonical form of the ledger key is the concatenation of the individual key parts, taking into account the key part type ( source ) func ( k * Key ) CanonicalForm () [] byte { ret := \"\" for _ , kp := range k . KeyParts { ret += fmt . Sprintf ( \"/%d/%v\" , kp . Type , string ( kp . Value )) } return [] byte ( ret ) } Effectively, this means that for a given register, the ledger path is the SHA3-256 hash of the /0/<owner>/1/<controller>/2/<key> , where <controller> is typically either the <owner> field or an empty string, depending on the register. Common Registers There's a number of registers that have specific meaning and are often encountered in Flow. The following registers can be typically found in a Flow account: exists - does the account exist or not frozen - is the account frozen or not contract_names - list of the names of contracts deployed to an account public_key_count - number of public keys an account has public_key_<N> - location of the N-th public key of an account storage_used - amount of storage used by the account, in bytes storage_index - used to keep track of a number of registers in the owner account code.<name> - register where the name Cadence contract is stored Flow Contracts There are core Cadence contracts that are deployed on the Flow network as soon as it is bootstrapped. These contracts are essential to the normal operation of the Flow network. These contracts are: FlowToken FungibleToken FlowServiceAccount FlowEpoch FlowFees FlowStorageFees FlowClusterQC FlowDKG FlowIDTableStaking FlowStakingCollection LockedTokens StakingProxy These contracts are described in more detail in the Cadence document . Flow Virtual Machine The Flow Virtual Machine (FVM) augments the Cadence runtime with the domain-specific functionality required by the Flow protocol. There are two types of operations that can be executed in the FVM - scripts and transactions. Scripts are operations that have read-only access to the ledger, while transactions can read or write to it. Cadence runtime.Interface defines a set of functions Cadence will use to query state from the FVM/ledger. The fvm package provides two execution environments that satisfy the runtime.Interface - the scriptEnv for scripts, and transactionEnv for transaction execution. These environments allow reading and writing values to the underlying storage (ledger), as expected by the Cadence interpreter. Case study - execution of a Cadence script This section is a detailed walkthrough for a simple scenario, in which a simple Cadence script is executed. It includes the creation of a Flow Virtual machine, Cadence interpreter and runtime, as well as the interaction with the underlying storage (ledger) provided by the ScriptEnv . // In this example 0x01 represents the address where the contract is located. import FungibleToken from 0x01 pub fun main () : UFix64 { return 1.0 } To execute this, we first need to create the FVM interpreter runtime in order to execute Cadence scripts and procedures. Default implementation of the Cadence interpreter runtime can be found in Cadence runtime/runtime.go . Then, the Flow Virtual Machine is created. runtime := fvm . NewInterpreterRuntime () vm := fvm . NewVirtualMachine ( runtime ) Now, we will create a fvm.ScriptProcedure . A Procedure in the context of the FVM is an operation that reads or updates the ledger state. Both Cadence scripts and transactions are procedures, via the ScriptProcedure and TransactionProcedure types. // Create a fvm.ScriptProcedure with the given script. // script argument is a byte slice with the Cadence script text. procedure := fvm . Script ( script ) // The fvm.ScriptProcedure is then run using the Flow Virtual Machine. err = vm . Run ( ctx , procedure , view , programs ) When using the FVM to run a procedure, it invokes the procedure's Run() method. During the preparation for running the script, first a new ScriptEnv is created, and then the ExecuteScript() method of the runtime is executed. func ( i ScriptInvocator ) Process ( vm * VirtualMachine , ctx Context , proc * ScriptProcedure , sth * state . StateHolder , programs * programs . Programs ) error { env := NewScriptEnvironment ( ctx , vm , sth , programs ) value , err := vm . Runtime . ExecuteScript ( // ... runtime . Context { Interface : env , // ... }, ) } The ScriptEnv created earlier is used as the runtime storage provider. func ( r * interpreterRuntime ) ExecuteScript ( script Script , context Context ) ( cadence . Value , error ) { context . InitializeCodesAndPrograms () runtimeStorage := newRuntimeStorage ( context . Interface ) // ... } This runtime storage handler is in charge of I/O operations when it comes to the ledger. The Cadence runtime then parses the provided script and check its correctness (via the parseAndCheckProgram() method of the runtime). One of the things that the check does is look at any imports found, and it uses the interpreter's GetAccountContractCode() method to load it. As the interpreter's storage provider was previously initialized to ScriptEnv , it's effectively invoking ScriptEnv s GetAccountContractCode() method. GetAccountContractCode() checks whether the account is frozen, and, if not, tries to read the contract code from the appropriate location. Skipping few levels down the call stack, GetAccountContractCode() will result in a call to fvm/state/accounts.go getValue() method. The code of getValue() shown below demonstrates two distinct solutions for resolving parameters for the register getter, the difference being the isController boolean flag. func ( a * StatefulAccounts ) getValue ( address flow . Address , isController bool , key string ) ( flow . RegisterValue , error ) { if isController { return a . stateHolder . State (). Get ( string ( address . Bytes ()), string ( address . Bytes ()), key ) } return a . stateHolder . State (). Get ( string ( address . Bytes ()), \"\" , key ) } The State s Get() method shown there can be simplified to this: func ( s * State ) Get ( owner , controller , key string ) ( flow . RegisterValue , error ) { return s . view . Get ( owner , controller , key ); } To get to the origin of the StateHolder / State objects, we need to look back at the FVM creation. For instance, in the Flow DPS invoker code, the state provider, named view below, is created like this: read := readRegister ( i . index , i . cache , height ) view := delta . NewView ( read ) vm . Run ( ctx , procedure , view , programs ) What's important to note is that the readRegister function returns a GetRegisterFunc , in charge of returning value for an appropriate register. In the Flow DPS ecosystem, it in essence means translating the owner , controller and key combination into a ledger path and reading it from the DPS index at the appropriate height. type GetRegisterFunc func ( owner , controller , key string ) ( flow . RegisterValue , error ) What's done with the ledger.Value / flow.RegisterValue later depends on the context. In the case of previous script example, the contents of the register /0/0x01/1/0x01/2/code.FungibleToken are shown as plaintext and can be imported to the Cadence program that is to be executed. The data in the register itself may be formatted differently based on the specific register, and it is up to the caller to unpack it correctly. For instance, the code.FungibleToken register contains a plaintext Cadence script. On the other hand, the contract_names register contains a CBOR-encoded array of strings. Registers can also contain complex types such as dictionaries, structs or arrays. Flow Localnet TODO Starting a Localnet git clone https://github.com/onflow/flow-go.git cd crypto ; go generate cd ../internal/localnet make init make start Indexing Localnet's past Sporks with Flow DPS TODO Using Flow Insights to inspect some Registers TODO","title":"Virtual Machine"},{"location":"flow/virtual-machine/#flow","text":"","title":"Flow"},{"location":"flow/virtual-machine/#registers","text":"The FVM interacts with the Flow execution state by running atomic operations against a ledger. What we call a ledger is a type of key/value storage. It holds an array of key-value pairs called registers. In Flow, the ledger implementation provides a number of functionalities and guarantees, including speed, memory-efficiency, crash resilience (via write-ahead logs and checkpoints), thread safety etc. It is also a stateful key/value storage, and every update to the ledger creates a new state. A limited number of recent states is kept, and updates can be applied to any one of those states. Each ledger register is referenced by an ID \u2014 the key and holds a value \u2014 binary data.","title":"Registers"},{"location":"flow/virtual-machine/#register-id-to-ledger-path","text":"When referencing storage locations from Flow code, each register is uniquely identified by three components \u2014 owner , controller and key . Owner represents the account to which the register belongs to. Depending on the register, the controller field can be either empty or have the value of the owner field. The key field represents the specific storage location of the account. Definition of a register ID ( source ): type RegisterID struct { Owner string Controller string Key string } Register ID is converted to a ledger.Key by concatenating specific key parts ( source ). const ( KeyPartOwner = uint16 ( 0 ) KeyPartController = uint16 ( 1 ) KeyPartKey = uint16 ( 2 ) ) // ... func RegisterIDToKey ( reg flow . RegisterID ) ledger . Key { return ledger . NewKey ([] ledger . KeyPart { ledger . NewKeyPart ( KeyPartOwner , [] byte ( reg . Owner )), ledger . NewKeyPart ( KeyPartController , [] byte ( reg . Controller )), ledger . NewKeyPart ( KeyPartKey , [] byte ( reg . Key )), }) } In order to map a Ledger key to a ledger path, the key can be converted to a corresponding path using pathfinder.KeyToPath . Depending on the version of the ledger pathfinder version, ledger path is either SHA256 or SHA3-256 (current version) hash of the canonical form of the ledger key. The canonical form of the ledger key is the concatenation of the individual key parts, taking into account the key part type ( source ) func ( k * Key ) CanonicalForm () [] byte { ret := \"\" for _ , kp := range k . KeyParts { ret += fmt . Sprintf ( \"/%d/%v\" , kp . Type , string ( kp . Value )) } return [] byte ( ret ) } Effectively, this means that for a given register, the ledger path is the SHA3-256 hash of the /0/<owner>/1/<controller>/2/<key> , where <controller> is typically either the <owner> field or an empty string, depending on the register.","title":"Register ID to Ledger Path"},{"location":"flow/virtual-machine/#common-registers","text":"There's a number of registers that have specific meaning and are often encountered in Flow. The following registers can be typically found in a Flow account: exists - does the account exist or not frozen - is the account frozen or not contract_names - list of the names of contracts deployed to an account public_key_count - number of public keys an account has public_key_<N> - location of the N-th public key of an account storage_used - amount of storage used by the account, in bytes storage_index - used to keep track of a number of registers in the owner account code.<name> - register where the name Cadence contract is stored","title":"Common Registers"},{"location":"flow/virtual-machine/#flow-contracts","text":"There are core Cadence contracts that are deployed on the Flow network as soon as it is bootstrapped. These contracts are essential to the normal operation of the Flow network. These contracts are: FlowToken FungibleToken FlowServiceAccount FlowEpoch FlowFees FlowStorageFees FlowClusterQC FlowDKG FlowIDTableStaking FlowStakingCollection LockedTokens StakingProxy These contracts are described in more detail in the Cadence document .","title":"Flow Contracts"},{"location":"flow/virtual-machine/#flow-virtual-machine","text":"The Flow Virtual Machine (FVM) augments the Cadence runtime with the domain-specific functionality required by the Flow protocol. There are two types of operations that can be executed in the FVM - scripts and transactions. Scripts are operations that have read-only access to the ledger, while transactions can read or write to it. Cadence runtime.Interface defines a set of functions Cadence will use to query state from the FVM/ledger. The fvm package provides two execution environments that satisfy the runtime.Interface - the scriptEnv for scripts, and transactionEnv for transaction execution. These environments allow reading and writing values to the underlying storage (ledger), as expected by the Cadence interpreter.","title":"Flow Virtual Machine"},{"location":"flow/virtual-machine/#case-study-execution-of-a-cadence-script","text":"This section is a detailed walkthrough for a simple scenario, in which a simple Cadence script is executed. It includes the creation of a Flow Virtual machine, Cadence interpreter and runtime, as well as the interaction with the underlying storage (ledger) provided by the ScriptEnv . // In this example 0x01 represents the address where the contract is located. import FungibleToken from 0x01 pub fun main () : UFix64 { return 1.0 } To execute this, we first need to create the FVM interpreter runtime in order to execute Cadence scripts and procedures. Default implementation of the Cadence interpreter runtime can be found in Cadence runtime/runtime.go . Then, the Flow Virtual Machine is created. runtime := fvm . NewInterpreterRuntime () vm := fvm . NewVirtualMachine ( runtime ) Now, we will create a fvm.ScriptProcedure . A Procedure in the context of the FVM is an operation that reads or updates the ledger state. Both Cadence scripts and transactions are procedures, via the ScriptProcedure and TransactionProcedure types. // Create a fvm.ScriptProcedure with the given script. // script argument is a byte slice with the Cadence script text. procedure := fvm . Script ( script ) // The fvm.ScriptProcedure is then run using the Flow Virtual Machine. err = vm . Run ( ctx , procedure , view , programs ) When using the FVM to run a procedure, it invokes the procedure's Run() method. During the preparation for running the script, first a new ScriptEnv is created, and then the ExecuteScript() method of the runtime is executed. func ( i ScriptInvocator ) Process ( vm * VirtualMachine , ctx Context , proc * ScriptProcedure , sth * state . StateHolder , programs * programs . Programs ) error { env := NewScriptEnvironment ( ctx , vm , sth , programs ) value , err := vm . Runtime . ExecuteScript ( // ... runtime . Context { Interface : env , // ... }, ) } The ScriptEnv created earlier is used as the runtime storage provider. func ( r * interpreterRuntime ) ExecuteScript ( script Script , context Context ) ( cadence . Value , error ) { context . InitializeCodesAndPrograms () runtimeStorage := newRuntimeStorage ( context . Interface ) // ... } This runtime storage handler is in charge of I/O operations when it comes to the ledger. The Cadence runtime then parses the provided script and check its correctness (via the parseAndCheckProgram() method of the runtime). One of the things that the check does is look at any imports found, and it uses the interpreter's GetAccountContractCode() method to load it. As the interpreter's storage provider was previously initialized to ScriptEnv , it's effectively invoking ScriptEnv s GetAccountContractCode() method. GetAccountContractCode() checks whether the account is frozen, and, if not, tries to read the contract code from the appropriate location. Skipping few levels down the call stack, GetAccountContractCode() will result in a call to fvm/state/accounts.go getValue() method. The code of getValue() shown below demonstrates two distinct solutions for resolving parameters for the register getter, the difference being the isController boolean flag. func ( a * StatefulAccounts ) getValue ( address flow . Address , isController bool , key string ) ( flow . RegisterValue , error ) { if isController { return a . stateHolder . State (). Get ( string ( address . Bytes ()), string ( address . Bytes ()), key ) } return a . stateHolder . State (). Get ( string ( address . Bytes ()), \"\" , key ) } The State s Get() method shown there can be simplified to this: func ( s * State ) Get ( owner , controller , key string ) ( flow . RegisterValue , error ) { return s . view . Get ( owner , controller , key ); } To get to the origin of the StateHolder / State objects, we need to look back at the FVM creation. For instance, in the Flow DPS invoker code, the state provider, named view below, is created like this: read := readRegister ( i . index , i . cache , height ) view := delta . NewView ( read ) vm . Run ( ctx , procedure , view , programs ) What's important to note is that the readRegister function returns a GetRegisterFunc , in charge of returning value for an appropriate register. In the Flow DPS ecosystem, it in essence means translating the owner , controller and key combination into a ledger path and reading it from the DPS index at the appropriate height. type GetRegisterFunc func ( owner , controller , key string ) ( flow . RegisterValue , error ) What's done with the ledger.Value / flow.RegisterValue later depends on the context. In the case of previous script example, the contents of the register /0/0x01/1/0x01/2/code.FungibleToken are shown as plaintext and can be imported to the Cadence program that is to be executed. The data in the register itself may be formatted differently based on the specific register, and it is up to the caller to unpack it correctly. For instance, the code.FungibleToken register contains a plaintext Cadence script. On the other hand, the contract_names register contains a CBOR-encoded array of strings. Registers can also contain complex types such as dictionaries, structs or arrays.","title":"Case study - execution of a Cadence script"},{"location":"flow/virtual-machine/#flow-localnet","text":"TODO","title":"Flow Localnet"},{"location":"flow/virtual-machine/#starting-a-localnet","text":"git clone https://github.com/onflow/flow-go.git cd crypto ; go generate cd ../internal/localnet make init make start","title":"Starting a Localnet"},{"location":"flow/virtual-machine/#indexing-localnets-past-sporks-with-flow-dps","text":"TODO","title":"Indexing Localnet's past Sporks with Flow DPS"},{"location":"flow/virtual-machine/#using-flow-insights-to-inspect-some-registers","text":"TODO","title":"Using Flow Insights to inspect some Registers"},{"location":"general/glossary/","text":"Glossary General Blockchain Concepts Blockchain Blockchain is a technology for implementing a stateful immutable distributed and decentralized ledger of records. Participants that update the ledger records represent the peers that run the blockchain network. Peers participate in the blockchain network collaborating with their resources in the same manner as in other types of distributed network architectures. Records in the ledger, called transactions, update the ledger state and represent any type of social exchangeable value to the participants of the network \u2014 token representing ownership, money, access to some resource or event, etc. Transactions in the blockchain represent transferring of value from one party to the other. The fundamental difference the blockchain provide to the participants is that their values are exchanged without the participation of any third parties. For cryptocurrencies this idea drastically changes the concept of money transfers. Ledger records - transactions, are created and recorded in the ledger using specific steps and types of encryption on different parts of them - hashing, asymmetric and symmetric encryption. Transactions can be grouped in blocks. Blocks are created by the peers based on transaction updates they know about and then distributed in the network. Depending on the consensus protocol that is run in the blockchain network, blocks are eventually accepted and inserted in to the ledger. Each block of transactions follows the previous block inserted and reference it by its ID. This creates an immutable chain of blocks of transactions that represent the state of the ledger. Depending on the peers' ability to participate in the blockchain network, blockchains are public and private . In public blockchains everyone is allowed to participate in the network. Peers can be of different types depending on what amount of data from the ledger they keep and how they update it, but in terms of functionality and resources they provide to the network - they are equally placed. This means that none of the peers, or a group of them, provide any specific functionality to the network that others do not. Consequently, when peers leave or arrive in the blockchain network \u2014 its running is not affected. Public blockchains do not require trust in other participants in the network. This is one of the core concepts behind the idea of distribution in the blockchain world. Private blockchains are a specific type of blockchains that have some type of authorization scheme used for identities that can enter the network and access its records. Private blockchains require trust in other participants of the network and some peers inside them have different levels of access compared to others. In this kind of blockchain networks, a random peer or a group of peers leaving (malfunctioning) or arriving in the network affects the stability of the whole network. Depending on the access to participate in the network \u2014 being granted or not, blockchains are permissioned and permissionless Generally this difference lie in providing access from some authority running the network, which concerns again the trust between participants. Permissioned blockchains are networks that require permission to enter it, usually used by organizations for managing their internal processes and data. Permissioned blockchains shifts from the core feature of decentralization in the blockchain world and from the initial idea of blockchains in general. A permissioned blockchain can also be a public network that only allows participation based on different access levels. The participants are usually known by a permissioned blockchain network operator, while the transaction history is not publicly accessible. A permissionless blockchain network does not require permissions to arrive as a peer in it, but can be public or private depending on the level or data access for the participants (and in the consensus trust levels between participants). In both permissionless and permissioned blockchains peers and groups of peers can have different roles and their presence or absence can affect the network functioning. Initial idea of blockchain networks, as described in Nakamoto consensus description , aims to reach the highest possible levels of transparency, decentralization, anonymity and immutability at the same time. During time, different types of blockchains emerged and today, they advance in some characteristics but lack in others (for example anonymity). Core concepts of blockchain according to the initial Nakamoto Consensus descriptions are: Transparency. Every full node in the network has a copy of the whole chain of blocks. This means that every transaction is available to each member, making the transactions traceable (not available in private and/or permissioned blockchains today). Immutability. Transactions included in the ledger become immutable records (in the general case). Immutability is present for all types of blockchains today. Decentralization. This is one of the biggest advantages of the blockchain, being a decentralized system allows for the lack of a central authority to control the transactions. Every full node has a copy of the chain, which they can update with new information, and every SPV node can update their records requesting them from a full node. Decentralization at its fullest is not present in permissioned and private blockchains today as described above. Once a new block is created and inserted on the chain, the new block will have a link to the previous block, creating a chain. They all include the hash of the previous block except for the first one, which is called the genesis block and has a zero hash value. Staking Staking is putting some native cryptocurrency as collateral to become a validating entity in the network. Consensus TODO: add Gossip Algorithms TODO: add Liveness Threshold The number of malicious participants that can be tolerated before the consensus protocol stops functioning. Nakamoto Type of Consensus TODO: introduce (for BTC) A consensus algorithm that provides a probabilistic safety guarantee; its decisions may revert with some probability \u03b5 . A permissionless protocol allowing each node to join or leave the system at any time. TODO: finish Safety TODO: add Transaction Transactions are data structures that encode the transfer of value between participants in the blockchain system. A transaction consumes previously recorded unspent transaction outputs and creates new transaction output(s) that can be consumed by future transactions. This is the way in which chunks of value move forward from one owner to the next one in the blockchain network. The two main parts of the transaction that represent the chain between available value and spent value are outputs and inputs. Transaction input Transaction input is a part of the transaction structure, and it identifies (often by reference) which UTXO will be consumed. It provides a proof of ownership using an unblocking script. First part of the input is usually a pointer/reference to the UTXO, and the second part is the unlocking script (usually constructed by the wallet) that satisfies the spending condition set in the UTXO(s) that the transaction is going to consume. Input data structure can have arbitrary fields inside it depending on the blockchain network implementation, but they generally are: txid \u2014 referencing the transaction id that contains the UTXO index \u2014 referencing which UTXO from that transaction is going to be spent scriptSig \u2014 a script signature that satisfies the spending conditions; known as \"unlocking script\" a sequence number An input may or may not reference any nominal value or other context depending on the organization. What it must contain is the unlocking script (witness part) that unlocks the UTXO. Transaction Output The transaction output is the fundamental building block of a blockchain transaction. Outputs are indivisible chunks of value denominated in the blockchain network exchanged value/currency, recorded on the blockchain and recognized as valid by the entire network. An output can contain an arbitrary value, but once created it is indivisible \u2014 it can be consumed only in its entirety by the transactions. Outputs are discrete and indivisible units of value. Transaction outputs consist generally of two parts: amount of the UTXO denominated value a cryptographic script or a public key that determines the conditions on which the UTXO can be spent; also known as the \"locking script\". The locking script is a hash representation of a public key hash (in the simplest situation) or it is a hash of an unlocking script. In both cases - the public key along with the signature or the unlocking script need to be presented at the time of consuming the output in order the values to be transferred. UTXO Set This is the set of all unspent transaction outputs that are available to the whole network. The UTXO set grows when a new transaction output is created and shrinks when UTXO is consumed. At any moment of time the set represents the UTXO that are available to be spent by the network participants. Avalanche Concepts Ancestor Set TODO: add Chit A transaction receives a chit (boolean value) when the majority (quorum) of queried nodes respond with an approval for that transaction. Confidence The sum of transaction/vertex's descendants chits and its own chit. The chit value is a result of a one-time query for its associated transaction and becomes immutable afterwards. Because chit values can be 0 or 1 \u2014 c \u2208 {0,1} confidence values are monotonic. Consecutive Success Number The number of times a transaction or its descendant received an approval from majority (quorum) of queried nodes. TODO: add more thorough description Progeny TODO: add Slush TODO: add Flow Concepts Quorum Certificate A process by which nodes using the HotStuff consensus algorithm submit signed messages in order to generate a certificate for bootstrapping HotStuff. Each collector cluster runs a mini-version of HotStuff, and since clusters are randomized each epoch, a new quorum certificate is required for each cluster each epoch. Other Graph TODO: introduce the idea in one sentence without technicals A graph is pair of sets G = (V, E) . V is a set of elements called vertices (singular: vertex) and E is a set of paired vertices and its elements are called edges. An edge {x, y} contains vertices x and y , and they are its endpoints. A vertex may belong to no edge and in that case it is not joined to any other vertex. The set of vertices in a graph is a discrete value, meaning the set of edges is also a discrete value. The number of vertices \u2014 |V| is the \"order\" of the graph and the number of edges \u2014 |E| is the \"size\" of the graph. Directed Graph A directed graph consists of set of vertices connected by \"directed\" edges \u2014 also called arcs. A directed graph is represented as a pair of sets G = (V, A) , where: V is a set of elements (vertices, nodes) A is a set of ordered pairs of vertices, called arcs Directed Acyclic Graph (DAG) A Directed Acyclic Graph contains no directed cycles \u2014 each edge is directed from one vertex to another such that following those directions never forms a closed loop. A directed graph is a DAG if and only if it can be topologically ordered by arranging the vertices as a linear ordering that is consistent with all edge directions. A topological ordering of a directed graph is an ordering of its vertices into a sequence, such that for every edge the start vertex of the edge occurs earlier in the sequence than the ending vertex of the edge. A graph that has a topological ordering does not have any cycles, consequently every graph with a topological ordering is acyclic. Every acyclic directed graph has at least one topological ordering. Distributed System A system of computers connected by a network \u2014 local, hub-based or wide, that work together as one single computer. Applications built with distributed systems appear to users and other client applications as if they are running on a single machine. Distributed systems take advantage of each node's physical and virtual resources (horizontal scalability), minimize overload of the whole system and can be designed to achieve a certain performance level, speed or other characteristic levels needed for the application running to overcome. Partition of a Set A set's partition is a group of its elements into non-empty subsets, in such a way that every element is included in exactly one subset. Transitive Relation For a set T with elements a , b , c , d A{a, b, c, d} A relation R on the elements of the set that relates a to b , b to c and c to d , but also relates a to c , a to d and b to d is called transitive. a -> b, b -> c, c -> d AND a -> c, a -> d, b -> d Example of a Transitive Relation R A -> ancestor of B B -> ancestor of C => A -> ancestor of C A \u2500\u2500> B \u2500\u2500> C => A \u2500\u2500> C Example of a Non-Transitive Relation R A -> ancestor of B B -> ancestor of C A -> ancestor of D => D is NOT an ancestor of C A \u251c\u2500\u2500> B \u2500\u2500> C \u2514\u2500\u2500> D The inverse of a transitive relation is always transitive. See also Homogeneous Relation . Transitive Closure of a Graph A transitive closure of a directed graph G is a directed graph G' with an edge (i, j) corresponding to each directed path from i to j in G . The resultant directed graph G' representation in the form of the adjacency matrix is called the connectivity matrix .","title":"Glossary"},{"location":"general/glossary/#glossary","text":"","title":"Glossary"},{"location":"general/glossary/#general-blockchain-concepts","text":"","title":"General Blockchain Concepts"},{"location":"general/glossary/#blockchain","text":"Blockchain is a technology for implementing a stateful immutable distributed and decentralized ledger of records. Participants that update the ledger records represent the peers that run the blockchain network. Peers participate in the blockchain network collaborating with their resources in the same manner as in other types of distributed network architectures. Records in the ledger, called transactions, update the ledger state and represent any type of social exchangeable value to the participants of the network \u2014 token representing ownership, money, access to some resource or event, etc. Transactions in the blockchain represent transferring of value from one party to the other. The fundamental difference the blockchain provide to the participants is that their values are exchanged without the participation of any third parties. For cryptocurrencies this idea drastically changes the concept of money transfers. Ledger records - transactions, are created and recorded in the ledger using specific steps and types of encryption on different parts of them - hashing, asymmetric and symmetric encryption. Transactions can be grouped in blocks. Blocks are created by the peers based on transaction updates they know about and then distributed in the network. Depending on the consensus protocol that is run in the blockchain network, blocks are eventually accepted and inserted in to the ledger. Each block of transactions follows the previous block inserted and reference it by its ID. This creates an immutable chain of blocks of transactions that represent the state of the ledger. Depending on the peers' ability to participate in the blockchain network, blockchains are public and private . In public blockchains everyone is allowed to participate in the network. Peers can be of different types depending on what amount of data from the ledger they keep and how they update it, but in terms of functionality and resources they provide to the network - they are equally placed. This means that none of the peers, or a group of them, provide any specific functionality to the network that others do not. Consequently, when peers leave or arrive in the blockchain network \u2014 its running is not affected. Public blockchains do not require trust in other participants in the network. This is one of the core concepts behind the idea of distribution in the blockchain world. Private blockchains are a specific type of blockchains that have some type of authorization scheme used for identities that can enter the network and access its records. Private blockchains require trust in other participants of the network and some peers inside them have different levels of access compared to others. In this kind of blockchain networks, a random peer or a group of peers leaving (malfunctioning) or arriving in the network affects the stability of the whole network. Depending on the access to participate in the network \u2014 being granted or not, blockchains are permissioned and permissionless Generally this difference lie in providing access from some authority running the network, which concerns again the trust between participants. Permissioned blockchains are networks that require permission to enter it, usually used by organizations for managing their internal processes and data. Permissioned blockchains shifts from the core feature of decentralization in the blockchain world and from the initial idea of blockchains in general. A permissioned blockchain can also be a public network that only allows participation based on different access levels. The participants are usually known by a permissioned blockchain network operator, while the transaction history is not publicly accessible. A permissionless blockchain network does not require permissions to arrive as a peer in it, but can be public or private depending on the level or data access for the participants (and in the consensus trust levels between participants). In both permissionless and permissioned blockchains peers and groups of peers can have different roles and their presence or absence can affect the network functioning. Initial idea of blockchain networks, as described in Nakamoto consensus description , aims to reach the highest possible levels of transparency, decentralization, anonymity and immutability at the same time. During time, different types of blockchains emerged and today, they advance in some characteristics but lack in others (for example anonymity). Core concepts of blockchain according to the initial Nakamoto Consensus descriptions are: Transparency. Every full node in the network has a copy of the whole chain of blocks. This means that every transaction is available to each member, making the transactions traceable (not available in private and/or permissioned blockchains today). Immutability. Transactions included in the ledger become immutable records (in the general case). Immutability is present for all types of blockchains today. Decentralization. This is one of the biggest advantages of the blockchain, being a decentralized system allows for the lack of a central authority to control the transactions. Every full node has a copy of the chain, which they can update with new information, and every SPV node can update their records requesting them from a full node. Decentralization at its fullest is not present in permissioned and private blockchains today as described above. Once a new block is created and inserted on the chain, the new block will have a link to the previous block, creating a chain. They all include the hash of the previous block except for the first one, which is called the genesis block and has a zero hash value.","title":"Blockchain"},{"location":"general/glossary/#staking","text":"Staking is putting some native cryptocurrency as collateral to become a validating entity in the network.","title":"Staking"},{"location":"general/glossary/#consensus","text":"TODO: add","title":"Consensus"},{"location":"general/glossary/#gossip-algorithms","text":"TODO: add","title":"Gossip Algorithms"},{"location":"general/glossary/#liveness-threshold","text":"The number of malicious participants that can be tolerated before the consensus protocol stops functioning.","title":"Liveness Threshold"},{"location":"general/glossary/#nakamoto-type-of-consensus","text":"TODO: introduce (for BTC) A consensus algorithm that provides a probabilistic safety guarantee; its decisions may revert with some probability \u03b5 . A permissionless protocol allowing each node to join or leave the system at any time. TODO: finish","title":"Nakamoto Type of Consensus"},{"location":"general/glossary/#safety","text":"TODO: add","title":"Safety"},{"location":"general/glossary/#transaction","text":"Transactions are data structures that encode the transfer of value between participants in the blockchain system. A transaction consumes previously recorded unspent transaction outputs and creates new transaction output(s) that can be consumed by future transactions. This is the way in which chunks of value move forward from one owner to the next one in the blockchain network. The two main parts of the transaction that represent the chain between available value and spent value are outputs and inputs.","title":"Transaction"},{"location":"general/glossary/#transaction-input","text":"Transaction input is a part of the transaction structure, and it identifies (often by reference) which UTXO will be consumed. It provides a proof of ownership using an unblocking script. First part of the input is usually a pointer/reference to the UTXO, and the second part is the unlocking script (usually constructed by the wallet) that satisfies the spending condition set in the UTXO(s) that the transaction is going to consume. Input data structure can have arbitrary fields inside it depending on the blockchain network implementation, but they generally are: txid \u2014 referencing the transaction id that contains the UTXO index \u2014 referencing which UTXO from that transaction is going to be spent scriptSig \u2014 a script signature that satisfies the spending conditions; known as \"unlocking script\" a sequence number An input may or may not reference any nominal value or other context depending on the organization. What it must contain is the unlocking script (witness part) that unlocks the UTXO.","title":"Transaction input"},{"location":"general/glossary/#transaction-output","text":"The transaction output is the fundamental building block of a blockchain transaction. Outputs are indivisible chunks of value denominated in the blockchain network exchanged value/currency, recorded on the blockchain and recognized as valid by the entire network. An output can contain an arbitrary value, but once created it is indivisible \u2014 it can be consumed only in its entirety by the transactions. Outputs are discrete and indivisible units of value. Transaction outputs consist generally of two parts: amount of the UTXO denominated value a cryptographic script or a public key that determines the conditions on which the UTXO can be spent; also known as the \"locking script\". The locking script is a hash representation of a public key hash (in the simplest situation) or it is a hash of an unlocking script. In both cases - the public key along with the signature or the unlocking script need to be presented at the time of consuming the output in order the values to be transferred.","title":"Transaction Output"},{"location":"general/glossary/#utxo-set","text":"This is the set of all unspent transaction outputs that are available to the whole network. The UTXO set grows when a new transaction output is created and shrinks when UTXO is consumed. At any moment of time the set represents the UTXO that are available to be spent by the network participants.","title":"UTXO Set"},{"location":"general/glossary/#avalanche-concepts","text":"","title":"Avalanche Concepts"},{"location":"general/glossary/#ancestor-set","text":"TODO: add","title":"Ancestor Set"},{"location":"general/glossary/#chit","text":"A transaction receives a chit (boolean value) when the majority (quorum) of queried nodes respond with an approval for that transaction.","title":"Chit"},{"location":"general/glossary/#confidence","text":"The sum of transaction/vertex's descendants chits and its own chit. The chit value is a result of a one-time query for its associated transaction and becomes immutable afterwards. Because chit values can be 0 or 1 \u2014 c \u2208 {0,1} confidence values are monotonic.","title":"Confidence"},{"location":"general/glossary/#consecutive-success-number","text":"The number of times a transaction or its descendant received an approval from majority (quorum) of queried nodes. TODO: add more thorough description","title":"Consecutive Success Number"},{"location":"general/glossary/#progeny","text":"TODO: add","title":"Progeny"},{"location":"general/glossary/#slush","text":"TODO: add","title":"Slush"},{"location":"general/glossary/#flow-concepts","text":"","title":"Flow Concepts"},{"location":"general/glossary/#quorum-certificate","text":"A process by which nodes using the HotStuff consensus algorithm submit signed messages in order to generate a certificate for bootstrapping HotStuff. Each collector cluster runs a mini-version of HotStuff, and since clusters are randomized each epoch, a new quorum certificate is required for each cluster each epoch.","title":"Quorum Certificate"},{"location":"general/glossary/#other","text":"","title":"Other"},{"location":"general/glossary/#graph","text":"TODO: introduce the idea in one sentence without technicals A graph is pair of sets G = (V, E) . V is a set of elements called vertices (singular: vertex) and E is a set of paired vertices and its elements are called edges. An edge {x, y} contains vertices x and y , and they are its endpoints. A vertex may belong to no edge and in that case it is not joined to any other vertex. The set of vertices in a graph is a discrete value, meaning the set of edges is also a discrete value. The number of vertices \u2014 |V| is the \"order\" of the graph and the number of edges \u2014 |E| is the \"size\" of the graph.","title":"Graph"},{"location":"general/glossary/#directed-graph","text":"A directed graph consists of set of vertices connected by \"directed\" edges \u2014 also called arcs. A directed graph is represented as a pair of sets G = (V, A) , where: V is a set of elements (vertices, nodes) A is a set of ordered pairs of vertices, called arcs","title":"Directed Graph"},{"location":"general/glossary/#directed-acyclic-graph-dag","text":"A Directed Acyclic Graph contains no directed cycles \u2014 each edge is directed from one vertex to another such that following those directions never forms a closed loop. A directed graph is a DAG if and only if it can be topologically ordered by arranging the vertices as a linear ordering that is consistent with all edge directions. A topological ordering of a directed graph is an ordering of its vertices into a sequence, such that for every edge the start vertex of the edge occurs earlier in the sequence than the ending vertex of the edge. A graph that has a topological ordering does not have any cycles, consequently every graph with a topological ordering is acyclic. Every acyclic directed graph has at least one topological ordering.","title":"Directed Acyclic Graph (DAG)"},{"location":"general/glossary/#distributed-system","text":"A system of computers connected by a network \u2014 local, hub-based or wide, that work together as one single computer. Applications built with distributed systems appear to users and other client applications as if they are running on a single machine. Distributed systems take advantage of each node's physical and virtual resources (horizontal scalability), minimize overload of the whole system and can be designed to achieve a certain performance level, speed or other characteristic levels needed for the application running to overcome.","title":"Distributed System"},{"location":"general/glossary/#partition-of-a-set","text":"A set's partition is a group of its elements into non-empty subsets, in such a way that every element is included in exactly one subset.","title":"Partition of a Set"},{"location":"general/glossary/#transitive-relation","text":"For a set T with elements a , b , c , d A{a, b, c, d} A relation R on the elements of the set that relates a to b , b to c and c to d , but also relates a to c , a to d and b to d is called transitive. a -> b, b -> c, c -> d AND a -> c, a -> d, b -> d","title":"Transitive Relation"},{"location":"general/glossary/#example-of-a-transitive-relation-r","text":"A -> ancestor of B B -> ancestor of C => A -> ancestor of C A \u2500\u2500> B \u2500\u2500> C => A \u2500\u2500> C","title":"Example of a Transitive Relation R"},{"location":"general/glossary/#example-of-a-non-transitive-relation-r","text":"A -> ancestor of B B -> ancestor of C A -> ancestor of D => D is NOT an ancestor of C A \u251c\u2500\u2500> B \u2500\u2500> C \u2514\u2500\u2500> D The inverse of a transitive relation is always transitive. See also Homogeneous Relation .","title":"Example of a Non-Transitive Relation R"},{"location":"general/glossary/#transitive-closure-of-a-graph","text":"A transitive closure of a directed graph G is a directed graph G' with an edge (i, j) corresponding to each directed path from i to j in G . The resultant directed graph G' representation in the form of the adjacency matrix is called the connectivity matrix .","title":"Transitive Closure of a Graph"},{"location":"process/git-flow/","text":"Git This guide assumes that you are already familiar with the basic functionalities of git : Cloning repositories Checking branches out Committing changes Pushing Pulling And it will walk you through slightly more advanced concepts that will come in handy when working on Optakt projects. Rebasing The most important thing you'll likely need to master is rebasing . Whenever the base branch one of your pull requests is against gets updated with changes that conflict with yours, you will need to rebase your branch before it can be properly reviewed and merged. git merge We do not use git merge at Optakt, because it would result in a messier commit history. Rebasing, and squashing commits before merging pull requests ensures that when looking at master, each commit corresponds to a single pull request. Fetch the latest version of the base branch you need to rebase against. In most cases this will be master . Run git rebase <base branch> from your local branch. This will make git rewind history and put your HEAD back to the first commit that diverges between the two branches. One by one, commits will be automatically applied to your local branch until one with conflicts is reached. You will now need to resolve the conflicts This can be done with a GUI using most development environments, or manually by running git status to see which files have conflicts that need to be resolved and modifying them manually. Now, run git rebase --continue to continue rebasing. Once the final commit is reached, you can run git push --force to overwrite the previous version of your branch with the new rebased one. If you made a mistake If you made a mistake and overwrote your remote and local branches with one that does not work, no worries! You can always use git reflog to get back to the previous state of your branch, before you started rebasing. For more details on git rebase , please read the git manual . Alternative to Rebasing In some cases, if the base branch was modified by force and now has tons of conflicts that are not related to your own changes, you might be better off not rebasing, so that you do not have to deal with resolving many conflicts you are unsure of. In that situation, you can follow these simple steps: git log on your branch to know the hashes of your commits git checkout <base branch> git pull git checkout -b tmp git cherry-pick <commit hash> for each of your commits git branch -D <your branch> to remove your local branch git checkout -b <your branch> to use the contents of the now clean tmp branch as your new local branch git push --set-upstream=<remote> <your branch> --force to overwrite the remote version of your branch with the clean one If there was a pull request that used your branch, it will now automatically be updated and no longer have any conflicts. If you made a mistake If you made a mistake and overwrote your remote and local branches with one that does not work, no worries! You can always use git reflog to get back to the previous state of your branch, before you deleted anything. Git Reflog Reference logs, or \"reflogs\", record when the tips of branches and other references were updated in the local repository. This is very useful for us, in order to revert changes that were made by mistake and to get back to the previous state of a modified branch. You can use git reflog show to look at your reference logs and find the hashes of the references you are interested in, and then simply git checkout <hash> to get back to their state. From there, you can use git checkout -b <branch name> to create a branch from that point in history. Git Restore git restore is a relatively new command in git which allows you to restore files from any point in history and even from other branches. This can be very useful for example if you want a specific file from a specific commit on another branch, you don't need to cherry-pick the whole commit, but instead of you run git restore --source=<other branch> -- /path/to/file . Clean Commits Ideally, we want to keep the commit history of our projects clean, in order to make it easy to navigate, to track and understand changes. On the master branch, each commit should correspond to a single PR, and within each PR, ideally each commit should correspond to a single change, in a way that reviewing a PR commit-by-commit should make sense. This is not simple however, since it means that if you go ahead and code huge amounts at once, you will then need to either: Create multiple commits from these changes, by interactively adding the specific parts that belong together into one commit for each feature/component of the changes. Create one huge commit for now, and split it later on by using git rebase and amending the commit history. Interactive Add When using git add -i , you have access to an interface that lets you go through the changes on your local branch and choose whether to add them to staging or not, block-by-block. This means you do not need to add a whole file to your next commit, but instead can add only the parts of it that are relevant to that commit. Amending Commit History The other solution is to use git rebase -i <base branch> to amend the commit history. In order to split a commit into multiple parts, you need to specify the edit directive for that commit. Once you get to that point in the rebasing process, running git status will show you all the changes from that commit. You can then use git restore --staged <file names> to remove them from staging. You are now free to run git add -i or use your IDE to create the relevant commits from those changes, and to create new commits. When you are done, simply run git rebase --continue to continue the rebasing process where we left it.","title":"Git Flow"},{"location":"process/git-flow/#git","text":"This guide assumes that you are already familiar with the basic functionalities of git : Cloning repositories Checking branches out Committing changes Pushing Pulling And it will walk you through slightly more advanced concepts that will come in handy when working on Optakt projects.","title":"Git"},{"location":"process/git-flow/#rebasing","text":"The most important thing you'll likely need to master is rebasing . Whenever the base branch one of your pull requests is against gets updated with changes that conflict with yours, you will need to rebase your branch before it can be properly reviewed and merged. git merge We do not use git merge at Optakt, because it would result in a messier commit history. Rebasing, and squashing commits before merging pull requests ensures that when looking at master, each commit corresponds to a single pull request. Fetch the latest version of the base branch you need to rebase against. In most cases this will be master . Run git rebase <base branch> from your local branch. This will make git rewind history and put your HEAD back to the first commit that diverges between the two branches. One by one, commits will be automatically applied to your local branch until one with conflicts is reached. You will now need to resolve the conflicts This can be done with a GUI using most development environments, or manually by running git status to see which files have conflicts that need to be resolved and modifying them manually. Now, run git rebase --continue to continue rebasing. Once the final commit is reached, you can run git push --force to overwrite the previous version of your branch with the new rebased one. If you made a mistake If you made a mistake and overwrote your remote and local branches with one that does not work, no worries! You can always use git reflog to get back to the previous state of your branch, before you started rebasing. For more details on git rebase , please read the git manual .","title":"Rebasing"},{"location":"process/git-flow/#alternative-to-rebasing","text":"In some cases, if the base branch was modified by force and now has tons of conflicts that are not related to your own changes, you might be better off not rebasing, so that you do not have to deal with resolving many conflicts you are unsure of. In that situation, you can follow these simple steps: git log on your branch to know the hashes of your commits git checkout <base branch> git pull git checkout -b tmp git cherry-pick <commit hash> for each of your commits git branch -D <your branch> to remove your local branch git checkout -b <your branch> to use the contents of the now clean tmp branch as your new local branch git push --set-upstream=<remote> <your branch> --force to overwrite the remote version of your branch with the clean one If there was a pull request that used your branch, it will now automatically be updated and no longer have any conflicts. If you made a mistake If you made a mistake and overwrote your remote and local branches with one that does not work, no worries! You can always use git reflog to get back to the previous state of your branch, before you deleted anything.","title":"Alternative to Rebasing"},{"location":"process/git-flow/#git-reflog","text":"Reference logs, or \"reflogs\", record when the tips of branches and other references were updated in the local repository. This is very useful for us, in order to revert changes that were made by mistake and to get back to the previous state of a modified branch. You can use git reflog show to look at your reference logs and find the hashes of the references you are interested in, and then simply git checkout <hash> to get back to their state. From there, you can use git checkout -b <branch name> to create a branch from that point in history.","title":"Git Reflog"},{"location":"process/git-flow/#git-restore","text":"git restore is a relatively new command in git which allows you to restore files from any point in history and even from other branches. This can be very useful for example if you want a specific file from a specific commit on another branch, you don't need to cherry-pick the whole commit, but instead of you run git restore --source=<other branch> -- /path/to/file .","title":"Git Restore"},{"location":"process/git-flow/#clean-commits","text":"Ideally, we want to keep the commit history of our projects clean, in order to make it easy to navigate, to track and understand changes. On the master branch, each commit should correspond to a single PR, and within each PR, ideally each commit should correspond to a single change, in a way that reviewing a PR commit-by-commit should make sense. This is not simple however, since it means that if you go ahead and code huge amounts at once, you will then need to either: Create multiple commits from these changes, by interactively adding the specific parts that belong together into one commit for each feature/component of the changes. Create one huge commit for now, and split it later on by using git rebase and amending the commit history.","title":"Clean Commits"},{"location":"process/git-flow/#interactive-add","text":"When using git add -i , you have access to an interface that lets you go through the changes on your local branch and choose whether to add them to staging or not, block-by-block. This means you do not need to add a whole file to your next commit, but instead can add only the parts of it that are relevant to that commit.","title":"Interactive Add"},{"location":"process/git-flow/#amending-commit-history","text":"The other solution is to use git rebase -i <base branch> to amend the commit history. In order to split a commit into multiple parts, you need to specify the edit directive for that commit. Once you get to that point in the rebasing process, running git status will show you all the changes from that commit. You can then use git restore --staged <file names> to remove them from staging. You are now free to run git add -i or use your IDE to create the relevant commits from those changes, and to create new commits. When you are done, simply run git rebase --continue to continue the rebasing process where we left it.","title":"Amending Commit History"},{"location":"process/meetings/","text":"Team Process Since we like to describe ourselves as a tactical team of engineers, we try to keep our processes as minimal as possible. Meetings The only recurring meeting that we maintain at the moment is the daily standup meeting, where the whole team meets to discuss what they did within the last day, what their blockers are and what they plan on doing during the next day. When the needs arise, the team can also decide to organize a backlog refinement to go through the issues of a project, prioritize them, refine them, remove outdated ones and create missing ones. Communication There are currently two channels of communication, each with its specific purpose. GitHub Issues and Pull Requests should contain all discussions about design where we want to reach a decision. This makes those discussions searchable and directly a part of the repository itself. All other written communication should be on Slack, where each project has a dedicated channel, and there is a general channel for matters related to Optakt itself.","title":"Meetings"},{"location":"process/meetings/#team-process","text":"Since we like to describe ourselves as a tactical team of engineers, we try to keep our processes as minimal as possible.","title":"Team Process"},{"location":"process/meetings/#meetings","text":"The only recurring meeting that we maintain at the moment is the daily standup meeting, where the whole team meets to discuss what they did within the last day, what their blockers are and what they plan on doing during the next day. When the needs arise, the team can also decide to organize a backlog refinement to go through the issues of a project, prioritize them, refine them, remove outdated ones and create missing ones.","title":"Meetings"},{"location":"process/meetings/#communication","text":"There are currently two channels of communication, each with its specific purpose. GitHub Issues and Pull Requests should contain all discussions about design where we want to reach a decision. This makes those discussions searchable and directly a part of the repository itself. All other written communication should be on Slack, where each project has a dedicated channel, and there is a general channel for matters related to Optakt itself.","title":"Communication"},{"location":"process/repositories/","text":"Projects This document describes the organization and workflow for Optakt projects, and acts as a centralized table of contents for project documentation. Organization Any Optakt project should have at least the following: Continuous Integration (CI) using GitHub Actions Automatic Releases when a tag is pushed (using goreleaser and a GitHub Action) A Pull Request (PR) template that guides contributors into the requirements for a successful PR Proper documentation License at the root of the repository README that describes the project, links to Godoc, shows build status and clearly enumerates the project dependencies Each executable and API within the project should have their own dedicated documentation For executable, document purpose, flags and give example uses For APIs, document purpose and describe endpoints with expected inputs and outputs Document on getting started/contributing Architecture document This document should explain clearly how this project will interact with other components For complex projects, this should also include a model of how the internal components of the project interact Continuous Integration We use GitHub Actions for our CI, since it allows us not to depend on any external services besides GitHub, while being a free and simple solution. Projects should have one action for verifying the integrity of incoming PRs. This can include but is not limited to: Verifying that formatting is correct Verifying that there are no linter warnings Verifying that tests are passing Verifying that calling go generate does not produce any change Verifying that calling go mod tidy does not produce any change Verifying that the project builds successfully Another common action between all repositories is one that upon a version of the project being tagged, releases it and generates a changelog for it. This can be done using goreleaser . Architecture Documents When writing architecture documents, using diagrams is often the most efficient way to express a mental model. For this purpose, we use a simple language to generate consistent diagrams in SVG formats, called nomnoml . Using the official website for the language , diagrams can be written, previewed live and exported to SVG. When using nomnoml diagrams, make sure to always include their sources within the repository: . \u251c\u2500\u2500 architecture.md \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 mydiagram.nomnoml \u2514\u2500\u2500 svg \u2514\u2500\u2500 mydiagram.svg #title: api #direction: bottom #background: #262626 #stroke: #fff #fill: #888 [<flow> Flow Network] [<external> Flow DPS Live] [<external> Flow DPS Indexer] [<indexer> Index] [<dps> DPS API] [<access> DPS Access API] [<rosetta> Rosetta API] [<label> SubmitTransaction] [Flow DPS Indexer]-->[Index] [Flow DPS Live]-->[Index] [Flow DPS Live]<--[Flow Network] [Index]->[DPS API] [DPS API]->[DPS Access API] [DPS API]->[Rosetta API] [SubmitTransaction]-[Rosetta API] [Flow Network]<-[SubmitTransaction] #.flow: fill=#262626 stroke=#00bff3 visual=ellipse dashed title=bold #.dps: fill=#262626 stroke=#fbb363 title=bold visual=receiver #.rosetta: fill=#262626 stroke=#fbb363 title=bold visual=transceiver #.access: fill=#262626 stroke=#fbb363 title=bold visual=transceiver #.indexer: fill=#262626 stroke=#fbb363 visual=database title=bold #.external: dashed Git Workflow Branches Currently, we use a branch-based git workflow, where internal contributors have the permissions to create any number of branches on the repository. In most cases, a feature can be implemented on a single branch and a PR is then opened against master to merge it. For large features however, we use feature branches, against which we create multiple smaller PRs. This makes the review process more digestible and forces us to split large features into smaller chunks. Pull Requests Each PR has to be related to a specific GitHub Issue, and the issue should be included in the body of the PR, by writing Fixes #issueNumber . This allows GitHub to automatically close the issue once the PR gets merged against master. For a Pull Request to be considered ready, it needs to fulfill some requirements. It should: Have at least one reviewer Be labelled appropriately Successfully run the CI Have an up-to-date branch with the PR's target Update any tests, documentation etc. that is impacted by the PR's contents Not contain any leftover FIXME notes, and if TODO notes are added, those should be linked to a GitHub issue Reviewers should only block a PR by Requesting Changes if one or more elements from the Pull Request must be changed before the PR can get merged. If a reviewer only gives feedback for nitpicks and style, for example, they should most likely just leave comments but not request changes. This keeps the reviewing process smoother. Once a PR has been reviewed and approved by at least one reviewer, it can be merged. The only way we allow merging is by squashing all the PR's commits into a single one, as this keeps the master branch and the changelog clean and easy to follow. Closing Pull Requests When closing a Pull Request, make sure to always specify the reason for doing so by adding a comment to the PR. This will ensure that collaborators can follow the process and understand the reasons for the PR being closed, and that the repository has a well-documented history. If you have issues with conflicts and/or your branch is messy, NEVER close the PR and delete the remote branch. Instead, simply create a new clean branch on which you cherry-pick your commits, clean up, and overwrite the original remote branch by using git push --force . This will update the PR and the previous history will be removed while preserving the original suggestion threads and PR metadata. Doing this also avoids duplicating information and having a confusing git history for the repository. Labels Task prioritization is currently handled using labels: must means the task is of the highest priority and must be implemented as soon as possible. should means the task should be done at some point, but there is no urgency at the moment. could means the task could be done if we want to, but it is not required. Each project then has its own custom labels for different types of issues and areas of the code. go generate Anything that gets generated should ideally be generated by running go generate ./... at the root of the repository. For example, in Flow DPS, the protobuf files are currently generated this way. In the Flow Rosetta repository however, go generate is used to generate constants that are used by the API to return version information. Releases Optakt projects use a GitHub Action that, whenever a new tag is pushed on master, compiles the binaries of the project for a given set of operating systems and architectures, computes a checksum and generates a changelog automatically. When the action runs successfully, it automatically transforms the tag into a GitHub release, sets the description of the release as the generated changelog, and adds the precompiled binaries and checksums to the release as downloadable files. Documentation Flow DPS Introduction Architecture Database API Snapshots Binaries flow-dps-client flow-dps-server flow-dps-live flow-dps-indexer dictionary-generator create-index-snapshot restore-index-snapshot Flow DPS Rosetta API Flow DPS Access","title":"Repositories"},{"location":"process/repositories/#projects","text":"This document describes the organization and workflow for Optakt projects, and acts as a centralized table of contents for project documentation.","title":"Projects"},{"location":"process/repositories/#organization","text":"Any Optakt project should have at least the following: Continuous Integration (CI) using GitHub Actions Automatic Releases when a tag is pushed (using goreleaser and a GitHub Action) A Pull Request (PR) template that guides contributors into the requirements for a successful PR Proper documentation License at the root of the repository README that describes the project, links to Godoc, shows build status and clearly enumerates the project dependencies Each executable and API within the project should have their own dedicated documentation For executable, document purpose, flags and give example uses For APIs, document purpose and describe endpoints with expected inputs and outputs Document on getting started/contributing Architecture document This document should explain clearly how this project will interact with other components For complex projects, this should also include a model of how the internal components of the project interact","title":"Organization"},{"location":"process/repositories/#continuous-integration","text":"We use GitHub Actions for our CI, since it allows us not to depend on any external services besides GitHub, while being a free and simple solution. Projects should have one action for verifying the integrity of incoming PRs. This can include but is not limited to: Verifying that formatting is correct Verifying that there are no linter warnings Verifying that tests are passing Verifying that calling go generate does not produce any change Verifying that calling go mod tidy does not produce any change Verifying that the project builds successfully Another common action between all repositories is one that upon a version of the project being tagged, releases it and generates a changelog for it. This can be done using goreleaser .","title":"Continuous Integration"},{"location":"process/repositories/#architecture-documents","text":"When writing architecture documents, using diagrams is often the most efficient way to express a mental model. For this purpose, we use a simple language to generate consistent diagrams in SVG formats, called nomnoml . Using the official website for the language , diagrams can be written, previewed live and exported to SVG. When using nomnoml diagrams, make sure to always include their sources within the repository: . \u251c\u2500\u2500 architecture.md \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 mydiagram.nomnoml \u2514\u2500\u2500 svg \u2514\u2500\u2500 mydiagram.svg #title: api #direction: bottom #background: #262626 #stroke: #fff #fill: #888 [<flow> Flow Network] [<external> Flow DPS Live] [<external> Flow DPS Indexer] [<indexer> Index] [<dps> DPS API] [<access> DPS Access API] [<rosetta> Rosetta API] [<label> SubmitTransaction] [Flow DPS Indexer]-->[Index] [Flow DPS Live]-->[Index] [Flow DPS Live]<--[Flow Network] [Index]->[DPS API] [DPS API]->[DPS Access API] [DPS API]->[Rosetta API] [SubmitTransaction]-[Rosetta API] [Flow Network]<-[SubmitTransaction] #.flow: fill=#262626 stroke=#00bff3 visual=ellipse dashed title=bold #.dps: fill=#262626 stroke=#fbb363 title=bold visual=receiver #.rosetta: fill=#262626 stroke=#fbb363 title=bold visual=transceiver #.access: fill=#262626 stroke=#fbb363 title=bold visual=transceiver #.indexer: fill=#262626 stroke=#fbb363 visual=database title=bold #.external: dashed","title":"Architecture Documents"},{"location":"process/repositories/#git-workflow","text":"","title":"Git Workflow"},{"location":"process/repositories/#branches","text":"Currently, we use a branch-based git workflow, where internal contributors have the permissions to create any number of branches on the repository. In most cases, a feature can be implemented on a single branch and a PR is then opened against master to merge it. For large features however, we use feature branches, against which we create multiple smaller PRs. This makes the review process more digestible and forces us to split large features into smaller chunks.","title":"Branches"},{"location":"process/repositories/#pull-requests","text":"Each PR has to be related to a specific GitHub Issue, and the issue should be included in the body of the PR, by writing Fixes #issueNumber . This allows GitHub to automatically close the issue once the PR gets merged against master. For a Pull Request to be considered ready, it needs to fulfill some requirements. It should: Have at least one reviewer Be labelled appropriately Successfully run the CI Have an up-to-date branch with the PR's target Update any tests, documentation etc. that is impacted by the PR's contents Not contain any leftover FIXME notes, and if TODO notes are added, those should be linked to a GitHub issue Reviewers should only block a PR by Requesting Changes if one or more elements from the Pull Request must be changed before the PR can get merged. If a reviewer only gives feedback for nitpicks and style, for example, they should most likely just leave comments but not request changes. This keeps the reviewing process smoother. Once a PR has been reviewed and approved by at least one reviewer, it can be merged. The only way we allow merging is by squashing all the PR's commits into a single one, as this keeps the master branch and the changelog clean and easy to follow. Closing Pull Requests When closing a Pull Request, make sure to always specify the reason for doing so by adding a comment to the PR. This will ensure that collaborators can follow the process and understand the reasons for the PR being closed, and that the repository has a well-documented history. If you have issues with conflicts and/or your branch is messy, NEVER close the PR and delete the remote branch. Instead, simply create a new clean branch on which you cherry-pick your commits, clean up, and overwrite the original remote branch by using git push --force . This will update the PR and the previous history will be removed while preserving the original suggestion threads and PR metadata. Doing this also avoids duplicating information and having a confusing git history for the repository.","title":"Pull Requests"},{"location":"process/repositories/#labels","text":"Task prioritization is currently handled using labels: must means the task is of the highest priority and must be implemented as soon as possible. should means the task should be done at some point, but there is no urgency at the moment. could means the task could be done if we want to, but it is not required. Each project then has its own custom labels for different types of issues and areas of the code.","title":"Labels"},{"location":"process/repositories/#go-generate","text":"Anything that gets generated should ideally be generated by running go generate ./... at the root of the repository. For example, in Flow DPS, the protobuf files are currently generated this way. In the Flow Rosetta repository however, go generate is used to generate constants that are used by the API to return version information.","title":"go generate"},{"location":"process/repositories/#releases","text":"Optakt projects use a GitHub Action that, whenever a new tag is pushed on master, compiles the binaries of the project for a given set of operating systems and architectures, computes a checksum and generates a changelog automatically. When the action runs successfully, it automatically transforms the tag into a GitHub release, sets the description of the release as the generated changelog, and adds the precompiled binaries and checksums to the release as downloadable files.","title":"Releases"},{"location":"process/repositories/#documentation","text":"Flow DPS Introduction Architecture Database API Snapshots Binaries flow-dps-client flow-dps-server flow-dps-live flow-dps-indexer dictionary-generator create-index-snapshot restore-index-snapshot Flow DPS Rosetta API Flow DPS Access","title":"Documentation"}]}