{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction This site is meant to act as a guide for new developers that join the Optakt team or contribute to Optakt projects. Optakt Optakt is a tactical team of engineers working on projects which reshape the landscape of blockchain technology. Given the enormous scope and the potential economic throughput of blockchain networks, maintaining high-quality standards for software is paramount. Our team delivers on this through a single-minded focus on good software design, rather than utilizing the brute-force efforts of larger teams.","title":"Introduction"},{"location":"#introduction","text":"This site is meant to act as a guide for new developers that join the Optakt team or contribute to Optakt projects.","title":"Introduction"},{"location":"#optakt","text":"Optakt is a tactical team of engineers working on projects which reshape the landscape of blockchain technology. Given the enormous scope and the potential economic throughput of blockchain networks, maintaining high-quality standards for software is paramount. Our team delivers on this through a single-minded focus on good software design, rather than utilizing the brute-force efforts of larger teams.","title":"Optakt"},{"location":"design/","text":"Design Principles The design principles mentioned in this document are at the core of the software we create at Optakt, as they provide helpful guidelines for engineers to build software in well-thought-out ways. They provide means to build complex software effectively so that it is testable, maintainable, expandable and clear. SOLID The SOLID acronym represents five design principles: Single-responsibility principle Open-closed principle Liskov Substitution principle Interface segregation principle Dependency inversion principle Go is a perfect language for integrating all of those principles within your software design, and this is best explained by Dave Cheney in his article on the matter .","title":"Design Principles"},{"location":"design/#design-principles","text":"The design principles mentioned in this document are at the core of the software we create at Optakt, as they provide helpful guidelines for engineers to build software in well-thought-out ways. They provide means to build complex software effectively so that it is testable, maintainable, expandable and clear.","title":"Design Principles"},{"location":"design/#solid","text":"The SOLID acronym represents five design principles: Single-responsibility principle Open-closed principle Liskov Substitution principle Interface segregation principle Dependency inversion principle Go is a perfect language for integrating all of those principles within your software design, and this is best explained by Dave Cheney in his article on the matter .","title":"SOLID"},{"location":"process/","text":"Team Process Since we like to describe ourselves as a tactical team of engineers, we try to keep our processes as minimal as possible. Meetings The only recurring meeting that we maintain at the moment is the daily standup meeting, where the whole team meets to discuss what they did within the last day, what their blockers are and what they plan on doing during the next day. When the needs arise, the team can also decide to organize a backlog refinement to go through the issues of a project, prioritize them, refine them, remove outdated ones and create missing ones. Communication There are currently two channels of communication, each with its specific purpose. GitHub Issues and Pull Requests should contain all discussions about design where we want to reach a decision. This makes those discussions searchable and directly a part of the repository itself. All other written communication should be on Slack, where each project has a dedicated channel, and there is a general channel for matters related to Optakt itself.","title":"Team Process"},{"location":"process/#team-process","text":"Since we like to describe ourselves as a tactical team of engineers, we try to keep our processes as minimal as possible.","title":"Team Process"},{"location":"process/#meetings","text":"The only recurring meeting that we maintain at the moment is the daily standup meeting, where the whole team meets to discuss what they did within the last day, what their blockers are and what they plan on doing during the next day. When the needs arise, the team can also decide to organize a backlog refinement to go through the issues of a project, prioritize them, refine them, remove outdated ones and create missing ones.","title":"Meetings"},{"location":"process/#communication","text":"There are currently two channels of communication, each with its specific purpose. GitHub Issues and Pull Requests should contain all discussions about design where we want to reach a decision. This makes those discussions searchable and directly a part of the repository itself. All other written communication should be on Slack, where each project has a dedicated channel, and there is a general channel for matters related to Optakt itself.","title":"Communication"},{"location":"projects/","text":"Projects This document describes the organization and workflow for Optakt projects, and acts as a centralized table of contents for project documentation. Organization Any Optakt project should have at least the following: Continuous Integration (CI) using GitHub Actions Automatic Releases when a tag is pushed (using goreleaser and a GitHub Action) A Pull Request (PR) template that guides contributors into the requirements for a successful PR Proper documentation License at the root of the repository README that describes the project, links to Godoc, shows build status and clearly enumerates the project dependencies Each executable and API within the project should have their own dedicated documentation For executable, document purpose, flags and give example uses For APIs, document purpose and describe endpoints with expected inputs and outputs Document on getting started/contributing Architecture document This document should explain clearly how this project will interact with other components For complex projects, this should also include a model of how the internal components of the project interact Continuous Integration We use GitHub Actions for our CI, since it allows us not to depend on any external services besides GitHub, while being a free and simple solution. Projects should have one action for verifying the integrity of incoming PRs. This can include but is not limited to: Verifying that formatting is correct Verifying that there are no linter warnings Verifying that tests are passing Verifying that calling go generate does not produce any change Verifying that calling go mod tidy does not produce any change Verifying that the project builds successfully Another common action between all repositories is one that upon a version of the project being tagged, releases it and generates a changelog for it. This can be done using goreleaser . Architecture Documents When writing architecture documents, using diagrams is often the most efficient way to express a mental model. For this purpose, we use a simple language to generate consistent diagrams in SVG formats, called nomnoml . Using the official website for the language , diagrams can be written, previewed live and exported to SVG. When using nomnoml diagrams, make sure to always include their sources within the repository: . \u251c\u2500\u2500 architecture.md \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 mydiagram.nomnoml \u2514\u2500\u2500 svg \u2514\u2500\u2500 mydiagram.svg #title: api #direction: bottom #background: #262626 #stroke: #fff #fill: #888 [<flow> Flow Network] [<external> Flow DPS Live] [<external> Flow DPS Indexer] [<indexer> Index] [<dps> DPS API] [<access> DPS Access API] [<rosetta> Rosetta API] [<label> SubmitTransaction] [Flow DPS Indexer]-->[Index] [Flow DPS Live]-->[Index] [Flow DPS Live]<--[Flow Network] [Index]->[DPS API] [DPS API]->[DPS Access API] [DPS API]->[Rosetta API] [SubmitTransaction]-[Rosetta API] [Flow Network]<-[SubmitTransaction] #.flow: fill=#262626 stroke=#00bff3 visual=ellipse dashed title=bold #.dps: fill=#262626 stroke=#fbb363 title=bold visual=receiver #.rosetta: fill=#262626 stroke=#fbb363 title=bold visual=transceiver #.access: fill=#262626 stroke=#fbb363 title=bold visual=transceiver #.indexer: fill=#262626 stroke=#fbb363 visual=database title=bold #.external: dashed Git Workflow Branches Currently, we use a branch-based git workflow, where internal contributors have the permissions to create any number of branches on the repository. In most cases, a feature can be implemented on a single branch and a PR is then opened against master to merge it. For large features however, we use feature branches, against which we create multiple smaller PRs. This makes the review process more digestible and forces us to split large features into smaller chunks. Pull Requests Each PR has to be related to a specific GitHub Issue, and the issue should be included in the body of the PR, by writing Fixes #issueNumber . This allows GitHub to automatically close the issue once the PR gets merged against master. For a Pull Request to be considered ready, it needs to fulfill some requirements. It should: Have at least one reviewer Be labelled appropriately Successfully run the CI Have an up-to-date branch with the PR's target Update any tests, documentation etc. that is impacted by the PR's contents Not contain any leftover FIXME notes, and if TODO notes are added, those should be linked to a GitHub issue Reviewers should only block a PR by Requesting Changes if one or more elements from the Pull Request must be changed before the PR can get merged. If a reviewer only gives feedback for nitpicks and style, for example, they should most likely just leave comments but not request changes. This keeps the reviewing process smoother. Once a PR has been reviewed and approved by at least one reviewer, it can be merged. The only way we allow merging is by squashing all the PR's commits into a single one, as this keeps the master branch and the changelog clean and easy to follow. Labels Task prioritization is currently handled using labels: must means the task is of the highest priority and must be implemented as soon as possible. should means the task should be done at some point, but there is no urgency at the moment. could means the task could be done if we want to, but it is not required. Each project then has its own custom labels for different types of issues and areas of the code. go generate Anything that gets generated should ideally be generated by running go generate ./... at the root of the repository. For example, in Flow DPS, the protobuf files are currently generated this way. In the Flow Rosetta repository however, go generate is used to generate constants that are used by the API to return version information. Releases Optakt projects use a GitHub Action that, whenever a new tag is pushed on master, compiles the binaries of the project for a given set of operating systems and architectures, computes a checksum and generates a changelog automatically. When the action runs successfully, it automatically transforms the tag into a GitHub release, sets the description of the release as the generated changelog, and adds the precompiled binaries and checksums to the release as downloadable files. Documentation Flow DPS Introduction Architecture Database API Snapshots Binaries flow-dps-client flow-dps-server flow-dps-live flow-dps-indexer dictionary-generator create-index-snapshot restore-index-snapshot Flow DPS Rosetta API Flow DPS Access","title":"Projects"},{"location":"projects/#projects","text":"This document describes the organization and workflow for Optakt projects, and acts as a centralized table of contents for project documentation.","title":"Projects"},{"location":"projects/#organization","text":"Any Optakt project should have at least the following: Continuous Integration (CI) using GitHub Actions Automatic Releases when a tag is pushed (using goreleaser and a GitHub Action) A Pull Request (PR) template that guides contributors into the requirements for a successful PR Proper documentation License at the root of the repository README that describes the project, links to Godoc, shows build status and clearly enumerates the project dependencies Each executable and API within the project should have their own dedicated documentation For executable, document purpose, flags and give example uses For APIs, document purpose and describe endpoints with expected inputs and outputs Document on getting started/contributing Architecture document This document should explain clearly how this project will interact with other components For complex projects, this should also include a model of how the internal components of the project interact","title":"Organization"},{"location":"projects/#continuous-integration","text":"We use GitHub Actions for our CI, since it allows us not to depend on any external services besides GitHub, while being a free and simple solution. Projects should have one action for verifying the integrity of incoming PRs. This can include but is not limited to: Verifying that formatting is correct Verifying that there are no linter warnings Verifying that tests are passing Verifying that calling go generate does not produce any change Verifying that calling go mod tidy does not produce any change Verifying that the project builds successfully Another common action between all repositories is one that upon a version of the project being tagged, releases it and generates a changelog for it. This can be done using goreleaser .","title":"Continuous Integration"},{"location":"projects/#architecture-documents","text":"When writing architecture documents, using diagrams is often the most efficient way to express a mental model. For this purpose, we use a simple language to generate consistent diagrams in SVG formats, called nomnoml . Using the official website for the language , diagrams can be written, previewed live and exported to SVG. When using nomnoml diagrams, make sure to always include their sources within the repository: . \u251c\u2500\u2500 architecture.md \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 mydiagram.nomnoml \u2514\u2500\u2500 svg \u2514\u2500\u2500 mydiagram.svg #title: api #direction: bottom #background: #262626 #stroke: #fff #fill: #888 [<flow> Flow Network] [<external> Flow DPS Live] [<external> Flow DPS Indexer] [<indexer> Index] [<dps> DPS API] [<access> DPS Access API] [<rosetta> Rosetta API] [<label> SubmitTransaction] [Flow DPS Indexer]-->[Index] [Flow DPS Live]-->[Index] [Flow DPS Live]<--[Flow Network] [Index]->[DPS API] [DPS API]->[DPS Access API] [DPS API]->[Rosetta API] [SubmitTransaction]-[Rosetta API] [Flow Network]<-[SubmitTransaction] #.flow: fill=#262626 stroke=#00bff3 visual=ellipse dashed title=bold #.dps: fill=#262626 stroke=#fbb363 title=bold visual=receiver #.rosetta: fill=#262626 stroke=#fbb363 title=bold visual=transceiver #.access: fill=#262626 stroke=#fbb363 title=bold visual=transceiver #.indexer: fill=#262626 stroke=#fbb363 visual=database title=bold #.external: dashed","title":"Architecture Documents"},{"location":"projects/#git-workflow","text":"","title":"Git Workflow"},{"location":"projects/#branches","text":"Currently, we use a branch-based git workflow, where internal contributors have the permissions to create any number of branches on the repository. In most cases, a feature can be implemented on a single branch and a PR is then opened against master to merge it. For large features however, we use feature branches, against which we create multiple smaller PRs. This makes the review process more digestible and forces us to split large features into smaller chunks.","title":"Branches"},{"location":"projects/#pull-requests","text":"Each PR has to be related to a specific GitHub Issue, and the issue should be included in the body of the PR, by writing Fixes #issueNumber . This allows GitHub to automatically close the issue once the PR gets merged against master. For a Pull Request to be considered ready, it needs to fulfill some requirements. It should: Have at least one reviewer Be labelled appropriately Successfully run the CI Have an up-to-date branch with the PR's target Update any tests, documentation etc. that is impacted by the PR's contents Not contain any leftover FIXME notes, and if TODO notes are added, those should be linked to a GitHub issue Reviewers should only block a PR by Requesting Changes if one or more elements from the Pull Request must be changed before the PR can get merged. If a reviewer only gives feedback for nitpicks and style, for example, they should most likely just leave comments but not request changes. This keeps the reviewing process smoother. Once a PR has been reviewed and approved by at least one reviewer, it can be merged. The only way we allow merging is by squashing all the PR's commits into a single one, as this keeps the master branch and the changelog clean and easy to follow.","title":"Pull Requests"},{"location":"projects/#labels","text":"Task prioritization is currently handled using labels: must means the task is of the highest priority and must be implemented as soon as possible. should means the task should be done at some point, but there is no urgency at the moment. could means the task could be done if we want to, but it is not required. Each project then has its own custom labels for different types of issues and areas of the code.","title":"Labels"},{"location":"projects/#go-generate","text":"Anything that gets generated should ideally be generated by running go generate ./... at the root of the repository. For example, in Flow DPS, the protobuf files are currently generated this way. In the Flow Rosetta repository however, go generate is used to generate constants that are used by the API to return version information.","title":"go generate"},{"location":"projects/#releases","text":"Optakt projects use a GitHub Action that, whenever a new tag is pushed on master, compiles the binaries of the project for a given set of operating systems and architectures, computes a checksum and generates a changelog automatically. When the action runs successfully, it automatically transforms the tag into a GitHub release, sets the description of the release as the generated changelog, and adds the precompiled binaries and checksums to the release as downloadable files.","title":"Releases"},{"location":"projects/#documentation","text":"Flow DPS Introduction Architecture Database API Snapshots Binaries flow-dps-client flow-dps-server flow-dps-live flow-dps-indexer dictionary-generator create-index-snapshot restore-index-snapshot Flow DPS Rosetta API Flow DPS Access","title":"Documentation"},{"location":"testing/","text":"Testing Guide At Optakt, we consistently write tests to ensure a reliable engineering environment where quality is paramount. Over the course of the product development life cycle, testing saves time and money, and helps developers write better code, more efficiently. Untested code is fragile, difficult to maintain and becomes questionable as soon as changes are made. This guide assumes that you are already familiar with Go testing. Unit Tests This section outlines a few of the rules we try to follow when it comes to Go unit tests. Naming Conventions Unit tests should have consistent names. The best way to go about it is to follow the official guidelines of the Go testing package , which states that: The naming convention to declare tests for the package, a function F , a type T and method M on type T are: func Test () { ... } func TestF () { ... } func TestT () { ... } func TestT_M () { ... } When it comes to subtests, the names of individual subtests should be lowercased and concise. The tests usually start with a subtest called nominal case which verifies that the tested component behaves as expected in a baseline situation, where no failures occur and no edge cases are handled. Internal Unit Tests In most cases, packages can be tested using external tests only. When writing tests for a package called xyz , the external tests should be in the same folder, but in a package called xyz_test . This case is handled by Go natively and will therefore not result in complaints about there being two different packages within the same directory. Of course, there are exceptions. If you need to test some internal logic, those tests must be in a file suffixed with _internal_test.go . Mocks When it comes to mocking dependencies for tests, we prefer to use simple hand-made mocks rather than to use testing frameworks to generate them. The Go language makes it easy to do so elegantly by creating structures that implement the interfaces for dependencies of the tested code and exposing functions that match the interface's signature as attributes that can be overridden externally. package mocks import ( \"testing\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" ) type Loader struct { TrieFunc func () ( * trie . MTrie , error ) } func BaselineLoader ( t * testing . T ) * Loader { t . Helper () l := Loader { TrieFunc : func () ( * trie . MTrie , error ) { return GenericTrie , nil }, } return & l } func ( l * Loader ) Trie () ( * trie . MTrie , error ) { return l . TrieFunc () } Using those mocks is as simple as instantiating a baseline version of the mock and setting its attributes to the desired functions: // ... t . Run ( \"handles failure to load checkpoint\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusEmpty ) load := mocks . BaselineLoader ( t ) load . CheckpointFunc = func () ( * trie . MTrie , error ) { return nil , mocks . GenericError } tr . load = load err := tr . BootstrapState ( st ) assert . Error ( t , err ) }) // ... Pseudorandom Generic Values When using test data for unit tests, it is always a good idea to use random generated data as the inputs. This avoids the bias where a test passes because it is given a valid set of inputs while some other inputs might have highlighted a flaw in the logic, by using an unconstrained data set. In order for the tests to be repeatable and for results to be consistent though, the given inputs should not be completely random, but instead they should be pseudorandom, with the same initial seed, to ensure the same sequence of \"random\" tests. Here is an example of such a value being generated. func GenericAddresses ( number int ) [] flow . Address { // Ensure consistent deterministic results. random := rand . New ( rand . NewSource ( 5 )) var addresses [] flow . Address for i := 0 ; i < number ; i ++ { var address flow . Address binary . BigEndian . PutUint64 ( address [ 0 :], random . Uint64 ()) addresses = append ( addresses , address ) } return addresses } func GenericAddress ( index int ) flow . Address { return GenericAddresses ( index + 1 )[ index ] } Warning While randomly generating valid inputs makes sense, randomly generating invalid inputs does not. In the case of invalid inputs, it is much better to have an exhaustive list of all types of cases that are expected to be invalid and always test each one of them. Parallelization Since the version 1.7 of Go, tests can be run in parallel. This can be done by calling t.Parallel in each subtest. Calling this function signals that the test is to be run in parallel with (and only with) other parallel tests, and the amount of tests running in parallel is limited by the value of runtime.GOMAXPROCS . There are multiple advantages to parallelizing tests: It ensures that regardless of the order in which inputs are given, components behave as expected. It maximizes performance, which in turns results in a faster CI and a faster workflow for everyone, which allows us to write more tests and therefore produce more actionable data to find bugs as well as improve tests cases and coverage. It makes it possible to ensure that the components you test are concurrency-safe Parallelizing table-driven tests When it comes to table-driven tests, a common pitfall developers fall into is to call t.Parallel in their subtest without capturing the loop variable with their test case. Here is an example of how it should be done: func TestGroupedParallel ( t * testing . T ) { for _ , tc := range tests { tc := tc // capture range variable t . Run ( tc . Name , func ( t * testing . T ) { t . Parallel () ... }) } } Standard testing Package The standard testing package is very powerful, and does not require additional frameworks to be used efficiently. The only exception we make to that are the stretchr/testify/assert and stretchr/testify/require packages which we use only for convenience, as they expose assertion functions that produce consistent outputs and make tests easy to understand. Subtests and Sub-benchmarks The testing package exposes a Run method on the T type which makes it possible to nest tests within tests. This can be very useful, as it enables creating a hierarchical structure within a test. index_internal_test.go func TestIndex ( t * testing . T ) { // ... t . Run ( \"collections\" , func ( t * testing . T ) { t . Parallel () collections := mocks . GenericCollections ( 4 ) reader , writer , db := setupIndex ( t ) defer db . Close () assert . NoError ( t , writer . Collections ( mocks . GenericHeight , collections )) // Close the writer to make it commit its transactions. require . NoError ( t , writer . Close ()) // NOTE: The following subtests should NOT be run in parallel, because of the deferral // to close the database above. t . Run ( \"retrieve collection by ID\" , func ( t * testing . T ) { got , err := reader . Collection ( collections [ 0 ]. ID ()) require . NoError ( t , err ) assert . Equal ( t , collections [ 0 ], got ) }) t . Run ( \"retrieve collections by height\" , func ( t * testing . T ) { got , err := reader . CollectionsByHeight ( mocks . GenericHeight ) require . NoError ( t , err ) assert . ElementsMatch ( t , mocks . GenericCollectionIDs ( 4 ), got ) }) t . Run ( \"retrieve transactions from collection\" , func ( t * testing . T ) { // For now this index is not used. }) }) // ... } Table-Driven Tests It makes a lot of sense to use subtests when testing the behavior of complex components, but it is better to use table-driven tests when testing a simple function with an expected output for a given input, and that there are many cases to cover. For such cases, table-driven tests massively improve clarity and readability. They should not be used blindly for all tests, however. In cases where the tested component is complex and that testing its methods cannot be simplified to a common setup, call to a function and assertion of the output, trying to use table-driven tests at all costs might lead to messy code, where the subtest which runs the test case is full of conditions to try to handle each separate setup. This is usually a sign that using simple tests and subtests would be a better approach. Case Study: The Flow DPS Mapper Sometimes, a core piece of software might seem impossible to test. That was the case for the mapper component in Flow DPS at some point, where its main function consisted of a 453-lines-long loop which orchestrated the use of all the other components of the application. mapper_old.go package mapper import ( \"bytes\" \"context\" \"errors\" \"fmt\" \"os\" \"sort\" \"sync\" \"github.com/gammazero/deque\" \"github.com/rs/zerolog\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/flattener\" \"github.com/onflow/flow-go/ledger/complete/mtrie/node\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/ledger/complete/wal\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" \"github.com/optakt/flow-dps/models/index\" ) type Mapper struct { log zerolog . Logger cfg Config chain Chain feed Feeder index index . Writer wg * sync . WaitGroup stop chan struct {} } // New creates a new mapper that uses chain data to map trie updates to blocks // and then passes on the details to the indexer for indexing. func New ( log zerolog . Logger , chain Chain , feed Feeder , index index . Writer , options ... func ( * Config )) ( * Mapper , error ) { // We don't use a checkpoint by default. The options can set one, in which // case we will add the checkpoint as a finalized state commitment in our // trie registry. cfg := Config { CheckpointFile : \"\" , PostProcessing : PostNoop , } for _ , option := range options { option ( & cfg ) } // Check if the checkpoint file exists. if cfg . CheckpointFile != \"\" { stat , err := os . Stat ( cfg . CheckpointFile ) if err != nil { return nil , fmt . Errorf ( \"invalid checkpoint file: %w\" , err ) } if stat . IsDir () { return nil , fmt . Errorf ( \"invalid checkpoint file: directory\" ) } } i := Mapper { log : log , chain : chain , feed : feed , index : index , cfg : cfg , wg : & sync . WaitGroup {}, stop : make ( chan struct {}), } return & i , nil } func ( m * Mapper ) Stop ( ctx context . Context ) error { close ( m . stop ) done := make ( chan struct {}) go func () { m . wg . Wait () close ( done ) }() select { case <- ctx . Done (): return ctx . Err () case <- done : return nil } } // NOTE: We might want to move height and tree (checkpoint) to parameters of the // run function; that would make it quite easy to resume from an arbitrary // point in the LedgerWAL and get rid of the related struct fields. func ( m * Mapper ) Run () error { m . wg . Add ( 1 ) defer m . wg . Done () // We start trying to map at the root height. height , err := m . chain . Root () if err != nil { return fmt . Errorf ( \"could not get root height: %w\" , err ) } // We always initialize an empty state trie to refer to the first step // before the checkpoint. If there is no checkpoint, then the step after the // checkpoint will also just be the empty trie. Otherwise, the second trie // will load the checkpoint trie. empty := trie . NewEmptyMTrie () var tree * trie . MTrie if m . cfg . CheckpointFile == \"\" { tree = empty } else { m . log . Info (). Msg ( \"checkpoint rebuild started\" ) file , err := os . Open ( m . cfg . CheckpointFile ) if err != nil { return fmt . Errorf ( \"could not open checkpoint file: %w\" , err ) } checkpoint , err := wal . ReadCheckpoint ( file ) if err != nil { return fmt . Errorf ( \"could not read checkpoint: %w\" , err ) } trees , err := flattener . RebuildTries ( checkpoint ) if err != nil { return fmt . Errorf ( \"could not rebuild tries: %w\" , err ) } if len ( trees ) != 1 { return fmt . Errorf ( \"should only have one trie in root checkpoint (tries: %d)\" , len ( trees )) } tree = trees [ 0 ] m . log . Info (). Msg ( \"checkpoint rebuild finished\" ) } m . log . Info (). Msg ( \"path collection started\" ) // We have to index all of the paths from the checkpoint; otherwise, we will // miss every single one of the bootstrapped registers. paths := make ([] ledger . Path , 0 , len ( tree . AllPayloads ())) queue := deque . New () root := tree . RootNode () if root != nil { queue . PushBack ( root ) } for queue . Len () > 0 { node := queue . PopBack ().( * node . Node ) if node . IsLeaf () { path := node . Path () paths = append ( paths , * path ) continue } if node . LeftChild () != nil { queue . PushBack ( node . LeftChild ()) } if node . RightChild () != nil { queue . PushBack ( node . RightChild ()) } } m . log . Info (). Int ( \"paths\" , len ( paths )). Msg ( \"path collection finished\" ) m . log . Info (). Msg ( \"path sorting started\" ) sort . Slice ( paths , func ( i int , j int ) bool { return bytes . Compare ( paths [ i ][:], paths [ j ][:]) < 0 }) m . log . Info (). Msg ( \"path sorting finished\" ) // When trying to go from one finalized block to the next, we keep a list // of intermediary tries until the full set of transitions have been // identified. We keep track of these transitions as steps in this map. steps := make ( map [ flow . StateCommitment ] * Step ) // We start at an \"imaginary\" step that refers to an empty trie, has no // paths and no previous commit. We consider this step already done, so it // will never be indexed; it's merely used as the sentinel value for // stopping when we index the first block. It also makes sure that we don't // return a `nil` trie if we abort indexing before the first block is done. emptyCommit := flow . DummyStateCommitment steps [ emptyCommit ] = & Step { Commit : flow . StateCommitment {}, Paths : nil , Tree : empty , } // We then add a second step that refers to the first step that is already // done, which uses the commit of the initial state trie after the // checkpoint has been loaded, and contains all of the paths found in the // initial checkpoint state trie. This will make sure that we index all the // data from the checkpoint as part of the first block. rootCommit := flow . StateCommitment ( tree . RootHash ()) steps [ rootCommit ] = & Step { Commit : emptyCommit , Paths : paths , Tree : tree , } // This is how we let the indexing loop know that the first \"imaginary\" step // was already indexed. The `commitPrev` value is used as a sentinel value // for when to stop going backwards through the steps when indexing a block. // This means the value is always set to the last already indexed step. commitPrev := emptyCommit m . log . Info (). Msg ( \"state indexing started\" ) // Next, we launch into the loop that is responsible for mapping all // incoming trie updates to a block. The loop itself has no concept of what // the next state commitment is that we should look at. It will simply try // to find a previous step for _any_ trie update that comes in. This means // that the first trie update needs to either apply to the empty trie or to // the trie after the checkpoint in order to be processed. once := & sync . Once {} Outer : for { // We want to check in this tight loop if we want to quit, just in case // we get stuck on a timed out network connection. select { case <- m . stop : break Outer default : // keep going } log := m . log . With (). Uint64 ( \"height\" , height ). Hex ( \"commit_prev\" , commitPrev [:]). Logger () // As a first step, we retrieve the state commitment of the finalized // block at the current height; we start at the root height and then // increase it each time we are done indexing a block. Once an applied // trie update gives us a state trie with the same root hash as // `commitNext`, we have reached the end state of the next finalized // block and can index all steps in-between for that block height. commitNext , err := m . chain . Commit ( height ) // If the retrieval times out, it's possible that we are on a live chain // and the next block has not been finalized yet. We should thus simply // retry until we have a new block. if errors . Is ( err , dps . ErrTimeout ) { log . Warn (). Msg ( \"commit retrieval timed out, retrying\" ) continue Outer } // If we have reached the end of the finalized blocks, we are probably // on a historical chain and there are no more finalized blocks for the // related spork. We can exit without error. if errors . Is ( err , dps . ErrFinished ) { log . Debug (). Msg ( \"reached end of finalized chain\" ) break Outer } // Any other error should not happen and should crash explicitly. if err != nil { return fmt . Errorf ( \"could not retrieve next commit (height: %d): %w\" , height , err ) } log = log . With (). Hex ( \"commit_next\" , commitNext [:]). Logger () Inner : for { // We want to check in this tight loop if we want to quit, just in case // we get stuck on a timed out network connection. select { case <- m . stop : break Outer default : // keep going } // When we have the state commitment of the next finalized block, we // check to see if we find a trie for it in our steps. If we do, it // means that we have steps from the last finalized block to the // finalized block at the current height. This condition will // trigger immediately for every empty block. _ , ok := steps [ commitNext ] if ok { break Inner } // If we don't find a trie for the current state commitment, we need // to keep applying trie updates to state tries until one of them // does have the correct commit. We simply feed the next trie update // here. update , err := m . feed . Update () // Once more, we might be on a live spork and the next delta might not // be available yet. In that case, keep trying. if errors . Is ( err , dps . ErrTimeout ) { log . Warn (). Msg ( \"delta retrieval timed out, retrying\" ) continue Inner } // Similarly, if no more deltas are available, we reached the end of // the WAL and we are done reconstructing the execution state. if errors . Is ( err , dps . ErrFinished ) { log . Debug (). Msg ( \"reached end of delta log\" ) break Outer } // Other errors should fail execution as they should not happen. if err != nil { return fmt . Errorf ( \"could not retrieve next delta: %w\" , err ) } // NOTE: We used to require a copy of the `RootHash` here, when it // was still a byte slice, as the underlying slice was being reused. // It was changed to a value type that is always copied now. commitBefore := flow . StateCommitment ( update . RootHash ) log := log . With (). Hex ( \"commit_before\" , commitBefore [:]). Logger () // Once we have our new update and know which trie it should be // applied to, we check to see if we have such a trie in our current // steps. If not, we can simply skip it; this can happen, for // example, when there is an execution fork and the trie update // applies to an obsolete part of the blockchain history. step , ok := steps [ commitBefore ] if ! ok { log . Debug (). Msg ( \"skipping trie update without matching trie\" ) continue Inner } // We de-duplicate the paths and payloads here. This replicates some // code that is part of the execution node and has moved between // different layers of the architecture. We keep it to be safe for // all versions of the Flow dependencies. // NOTE: Past versions of this code required paths to be copied, // because the underlying slice was being re-used. In contrary, // deep-copying payloads was a bad idea, because they were already // being copied by the trie insertion code, and it would have led to // twice the memory usage. paths = make ([] ledger . Path , 0 , len ( update . Paths )) lookup := make ( map [ ledger . Path ] * ledger . Payload ) for i , path := range update . Paths { _ , ok := lookup [ path ] if ! ok { paths = append ( paths , path ) } lookup [ path ] = update . Payloads [ i ] } sort . Slice ( paths , func ( i , j int ) bool { return bytes . Compare ( paths [ i ][:], paths [ j ][:]) < 0 }) payloads := make ([] ledger . Payload , 0 , len ( paths )) for _ , path := range paths { payloads = append ( payloads , * lookup [ path ]) } // We can now apply the trie update to the state trie as it was at // the previous step. This is where the trie code will deep-copy the // payloads. // NOTE: It's important that we don't shadow the variable here, // otherwise the root trie will never go out of scope and we will // never garbage collect any of the root trie payloads that have // been replaced by subsequent trie updates. tree , err = trie . NewTrieWithUpdatedRegisters ( step . Tree , paths , payloads ) if err != nil { return fmt . Errorf ( \"could not update trie: %w\" , err ) } // We then store the new trie along with the state commitment of its // parent and the paths that were changed. This will make it // available for subsequent trie updates to be applied to it, and it // will also allow us to reconstruct the payloads changed in this // step by retrieving them directly from the trie with the given // paths. commitAfter := flow . StateCommitment ( tree . RootHash ()) step = & Step { Commit : commitBefore , Paths : paths , Tree : tree , } steps [ commitAfter ] = step log . Debug (). Hex ( \"commit_after\" , commitAfter [:]). Msg ( \"trie update applied\" ) } // At this point we have identified a step that has lead to the state // commitment of the finalized block at the current height. We can // retrieve some additional indexing data, such as the block header and // the events that resulted from transactions in the block. header , err := m . chain . Header ( height ) if err != nil { return fmt . Errorf ( \"could not retrieve header: %w (height: %d)\" , err , height ) } events , err := m . chain . Events ( height ) if err != nil { return fmt . Errorf ( \"could not retrieve events: %w (height: %d)\" , err , height ) } transactions , err := m . chain . Transactions ( height ) if err != nil { return fmt . Errorf ( \"could not retrieve transactions: %w (height: %d)\" , err , height ) } collections , err := m . chain . Collections ( height ) if err != nil { return fmt . Errorf ( \"could not retrieve collections: %w (height: %d)\" , err , height ) } blockID := header . ID () // TODO: Refactor the mapper in https://github.com/optakt/flow-dps/issues/128 // and replace naive if statements around indexing. // We then index the data for the finalized block at the current height. if m . cfg . IndexHeaders { err = m . index . Header ( height , header ) if err != nil { return fmt . Errorf ( \"could not index header: %w\" , err ) } } if m . cfg . IndexCommit { err = m . index . Commit ( height , commitNext ) if err != nil { return fmt . Errorf ( \"could not index commit: %w\" , err ) } } if m . cfg . IndexEvents { err = m . index . Events ( height , events ) if err != nil { return fmt . Errorf ( \"could not index events: %w\" , err ) } } if m . cfg . IndexBlocks { err = m . index . Height ( blockID , height ) if err != nil { return fmt . Errorf ( \"could not index block heights: %w\" , err ) } } if m . cfg . IndexTransactions { err = m . index . Transactions ( blockID , collections , transactions ) if err != nil { return fmt . Errorf ( \"could not index transactions: %w\" , err ) } } // In order to index the payloads, we step back from the state // commitment of the finalized block at the current height to the state // commitment of the last finalized block that was indexed. For each // step, we collect all the payloads by using the paths for the step and // index them as we go. // NOTE: We keep track of the paths for which we already indexed // payloads, so we can skip them in earlier steps. One inherent benefit // of stepping from the last step to the first step is that this will // automatically use only the latest update of a register, which is // exactly what we want. commit := commitNext updated := make ( map [ ledger . Path ] struct {}) for commit != commitPrev { // In the first part, we get the step we are currently at and filter // out any paths that have already been updated. step := steps [ commit ] paths := make ([] ledger . Path , 0 , len ( step . Paths )) for _ , path := range step . Paths { _ , ok := updated [ path ] if ok { continue } paths = append ( paths , path ) updated [ path ] = struct {}{} } if ! m . cfg . IndexPayloads { commit = step . Commit continue } // We then divide the remaining paths into chunks of 1000. For each // batch, we retrieve the payloads from the state trie as it was at // the end of this block and index them. count := 0 n := 1000 total := ( len ( paths ) + n - 1 ) / n log . Debug (). Int ( \"num_paths\" , len ( paths )). Int ( \"num_batches\" , total ). Msg ( \"path batching executed\" ) for start := 0 ; start < len ( paths ); start += n { // This loop may take a while, especially for the root checkpoint // updates, so check if we should quit. select { case <- m . stop : break Outer default : // keep going } end := start + n if end > len ( paths ) { end = len ( paths ) } batch := paths [ start : end ] payloads := step . Tree . UnsafeRead ( batch ) err = m . index . Payloads ( height , batch , payloads ) if err != nil { return fmt . Errorf ( \"could not index payloads: %w\" , err ) } count ++ log . Debug (). Int ( \"batch\" , count ). Int ( \"start\" , start ). Int ( \"end\" , end ). Msg ( \"path batch indexed\" ) } // Finally, we forward the commit to the previous trie update and // repeat until we have stepped all the way back to the last indexed // commit. commit = step . Commit } // At this point, we can delete any trie that does not correspond to // the state that we have just reached. This will allow the garbage // collector to free up any payload that has been changed and which is // no longer part of the state trie at the newly indexed finalized // block. for key := range steps { if key != commitNext { delete ( steps , key ) } } // Last but not least, we take care of properly indexing the height of // the first indexed block and the height of the last indexed block. once . Do ( func () { err = m . index . First ( height ) }) if err != nil { return fmt . Errorf ( \"could not index first height: %w\" , err ) } err = m . index . Last ( height ) if err != nil { return fmt . Errorf ( \"could not index last height: %w\" , err ) } // We have now successfully indexed all state trie changes and other // data at the current height. We set the last indexed step to the last // step from our current height, and then increase the height to start // the indexing of the next block. commitPrev = commitNext height ++ log . Info (). Hex ( \"block\" , blockID [:]). Int ( \"num_changes\" , len ( updated )). Int ( \"num_events\" , len ( events )). Msg ( \"block data indexed\" ) } m . log . Info (). Msg ( \"state indexing finished\" ) step := steps [ commitPrev ] m . cfg . PostProcessing ( step . Tree ) return nil } As it was, this code was untestable. Covering each possible case from this huge piece of logic would have required immense, complex, unreadable tests, that would break whenever a piece of this logic would change, and this would require a huge amount of maintenance effort. To solve that massive problem, we refactored our original mapper into a finite-state machine which replicates the same computation logic by applying transitions to a state. mapper_new.go package mapper import ( \"errors\" \"fmt\" \"sync\" \"time\" \"github.com/rs/zerolog\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" ) // TransitionFunc is a function that is applied onto the state machine's // state. type TransitionFunc func ( * State ) error // Transitions is what applies transitions to the state of an FSM. type Transitions struct { cfg Config log zerolog . Logger load Loader chain dps . Chain feed Feeder read dps . Reader write dps . Writer once * sync . Once } // NewTransitions returns a Transitions component using the given dependencies and using the given options func NewTransitions ( log zerolog . Logger , load Loader , chain dps . Chain , feed Feeder , read dps . Reader , write dps . Writer , options ... Option ) * Transitions { cfg := DefaultConfig for _ , option := range options { option ( & cfg ) } t := Transitions { log : log . With (). Str ( \"component\" , \"mapper_transitions\" ). Logger (), cfg : cfg , load : load , chain : chain , feed : feed , read : read , write : write , once : & sync . Once {}, } return & t } // InitializeMapper initializes the mapper by either going into bootstrapping or // into resuming, depending on the configuration. func ( t * Transitions ) InitializeMapper ( s * State ) error { if s . status != StatusInitialize { return fmt . Errorf ( \"invalid status for initializing mapper (%s)\" , s . status ) } if t . cfg . BootstrapState { s . status = StatusBootstrap return nil } s . status = StatusResume return nil } // BootstrapState bootstraps the state by loading the checkpoint if there is one // and initializing the elements subsequently used by the FSM. func ( t * Transitions ) BootstrapState ( s * State ) error { if s . status != StatusBootstrap { return fmt . Errorf ( \"invalid status for bootstrapping state (%s)\" , s . status ) } // We always need at least one step in our forest, which is used as the // stopping point when indexing the payloads since the last finalized // block. We thus introduce an empty tree, with no paths and an // irrelevant previous commit. empty := trie . NewEmptyMTrie () s . forest . Save ( empty , nil , flow . DummyStateCommitment ) // The chain indexing will forward last to next and next to current height, // which will be the one for the checkpoint. first := flow . StateCommitment ( empty . RootHash ()) s . last = flow . DummyStateCommitment s . next = first t . log . Info (). Hex ( \"commit\" , first [:]). Msg ( \"added empty tree to forest\" ) // Then, we can load the root height and apply it to the state. That // will allow us to load the root blockchain data in the next step. height , err := t . chain . Root () if err != nil { return fmt . Errorf ( \"could not get root height: %w\" , err ) } s . height = height // When bootstrapping, the loader injected into the mapper loads the root // checkpoint. tree , err := t . load . Trie () if err != nil { return fmt . Errorf ( \"could not load root trie: %w\" , err ) } paths := allPaths ( tree ) s . forest . Save ( tree , paths , first ) second := tree . RootHash () t . log . Info (). Uint64 ( \"height\" , s . height ). Hex ( \"commit\" , second [:]). Int ( \"registers\" , len ( paths )). Msg ( \"added checkpoint tree to forest\" ) // We have successfully bootstrapped. However, no chain data for the root // block has been indexed yet. This is why we \"pretend\" that we just // forwarded the state to this height, so we go straight to the chain data // indexing. s . status = StatusIndex return nil } // ResumeIndexing resumes indexing the data from a previous run. func ( t * Transitions ) ResumeIndexing ( s * State ) error { if s . status != StatusResume { return fmt . Errorf ( \"invalid status for resuming indexing (%s)\" , s . status ) } // When resuming, we want to avoid overwriting the `first` height in the // index with the height we are resuming from. Theoretically, all that would // be needed would be to execute a no-op on `once`, which would subsequently // be skipped in the height forwarding code. However, this bug was already // released, so we have databases where `first` was incorrectly set to the // height we resume from. In order to fix them, we explicitly write the // correct `first` height here again, while at the same time using `once` to // disable any subsequent attempts to write it. first , err := t . chain . Root () if err != nil { return fmt . Errorf ( \"could not get root height: %w\" , err ) } t . once . Do ( func () { err = t . write . First ( first ) }) if err != nil { return fmt . Errorf ( \"could not write first: %w\" , err ) } // We need to know what the last indexed height was at the point we stopped // indexing. last , err := t . read . Last () if err != nil { return fmt . Errorf ( \"could not get last height: %w\" , err ) } // When resuming, the loader injected into the mapper rebuilds the trie from // the paths and payloads stored in the index database. tree , err := t . load . Trie () if err != nil { return fmt . Errorf ( \"could not restore index trie: %w\" , err ) } // After loading the trie, we should do a sanity check on its hash against // the commit we indexed for it. hash := flow . StateCommitment ( tree . RootHash ()) commit , err := t . read . Commit ( last ) if err != nil { return fmt . Errorf ( \"could not get last commit: %w\" , err ) } if hash != commit { return fmt . Errorf ( \"restored trie hash does not match last commit (hash: %x, commit: %x)\" , hash , commit ) } // At this point, we can store the restored trie in our forest, as the trie // for the last finalized block. We do not need to care about the parent // state commitment or the paths, as they should not be used. s . last = flow . DummyStateCommitment s . next = commit s . forest . Save ( tree , nil , flow . DummyStateCommitment ) // Lastly, we just need to point to the next height. The chain indexing will // then proceed with the first non-indexed block and forward the state // commitments accordingly. s . height = last + 1 // At this point, we should be able to start indexing the chain data for // the next height. s . status = StatusIndex return nil } // IndexChain indexes chain data for the current height. func ( t * Transitions ) IndexChain ( s * State ) error { if s . status != StatusIndex { return fmt . Errorf ( \"invalid status for indexing chain (%s)\" , s . status ) } log := t . log . With (). Uint64 ( \"height\" , s . height ). Logger () // We try to retrieve the next header until it becomes available, which // means all data coming from the protocol state is available after this // point. header , err := t . chain . Header ( s . height ) if errors . Is ( err , dps . ErrUnavailable ) { log . Debug (). Msg ( \"waiting for next header\" ) time . Sleep ( t . cfg . WaitInterval ) return nil } if err != nil { return fmt . Errorf ( \"could not get header: %w\" , err ) } // At this point, we can retrieve the data from the consensus state. This is // a slight optimization for the live indexer, as it allows us to process // some data before the full execution data becomes available. guarantees , err := t . chain . Guarantees ( s . height ) if err != nil { return fmt . Errorf ( \"could not get guarantees: %w\" , err ) } seals , err := t . chain . Seals ( s . height ) if err != nil { return fmt . Errorf ( \"could not get seals: %w\" , err ) } // We can also proceed to already indexing the data related to the consensus // state, before dealing with anything related to execution data, which // might go into the wait state. blockID := header . ID () err = t . write . Height ( blockID , s . height ) if err != nil { return fmt . Errorf ( \"could not index height: %w\" , err ) } err = t . write . Header ( s . height , header ) if err != nil { return fmt . Errorf ( \"could not index header: %w\" , err ) } err = t . write . Guarantees ( s . height , guarantees ) if err != nil { return fmt . Errorf ( \"could not index guarantees: %w\" , err ) } err = t . write . Seals ( s . height , seals ) if err != nil { return fmt . Errorf ( \"could not index seals: %w\" , err ) } // Next, we try to retrieve the next commit until it becomes available, // at which point all the data coming from the execution data should be // available. commit , err := t . chain . Commit ( s . height ) if errors . Is ( err , dps . ErrUnavailable ) { log . Debug (). Msg ( \"waiting for next state commitment\" ) time . Sleep ( t . cfg . WaitInterval ) return nil } if err != nil { return fmt . Errorf ( \"could not get commit: %w\" , err ) } collections , err := t . chain . Collections ( s . height ) if err != nil { return fmt . Errorf ( \"could not get collections: %w\" , err ) } transactions , err := t . chain . Transactions ( s . height ) if err != nil { return fmt . Errorf ( \"could not get transactions: %w\" , err ) } results , err := t . chain . Results ( s . height ) if err != nil { return fmt . Errorf ( \"could not get transaction results: %w\" , err ) } events , err := t . chain . Events ( s . height ) if err != nil { return fmt . Errorf ( \"could not get events: %w\" , err ) } // Next, all we need to do is index the remaining data and we have fully // processed indexing for this block height. err = t . write . Commit ( s . height , commit ) if err != nil { return fmt . Errorf ( \"could not index commit: %w\" , err ) } err = t . write . Collections ( s . height , collections ) if err != nil { return fmt . Errorf ( \"could not index collections: %w\" , err ) } err = t . write . Transactions ( s . height , transactions ) if err != nil { return fmt . Errorf ( \"could not index transactions: %w\" , err ) } err = t . write . Results ( results ) if err != nil { return fmt . Errorf ( \"could not index transaction results: %w\" , err ) } err = t . write . Events ( s . height , events ) if err != nil { return fmt . Errorf ( \"could not index events: %w\" , err ) } // At this point, we need to forward the `last` state commitment to // `next`, so we know what the state commitment was at the last finalized // block we processed. This will allow us to know when to stop when // walking back through the forest to collect trie updates. s . last = s . next // Last but not least, we need to update `next` to point to the commit we // have just retrieved for the new block height. This is the sentinel that // tells us when we have collected enough trie updates for the forest to // have reached the next finalized block. s . next = commit log . Info (). Msg ( \"indexed blockchain data for finalized block\" ) // After indexing the blockchain data, we can go back to updating the state // tree until we find the commit of the finalized block. This will allow us // to index the payloads then. s . status = StatusUpdate return nil } // UpdateTree updates the state's tree. If the state's forest already matches with the next block's state commitment, // it immediately returns and sets the state's status to StatusMatched. func ( t * Transitions ) UpdateTree ( s * State ) error { if s . status != StatusUpdate { return fmt . Errorf ( \"invalid status for updating tree (%s)\" , s . status ) } log := t . log . With (). Uint64 ( \"height\" , s . height ). Hex ( \"last\" , s . last [:]). Hex ( \"next\" , s . next [:]). Logger () // If the forest contains a tree for the commit of the next finalized block, // we have reached our goal, and we can go to the next step in order to // collect the register payloads we want to index for that block. ok := s . forest . Has ( s . next ) if ok { log . Info (). Hex ( \"commit\" , s . next [:]). Msg ( \"matched commit of finalized block\" ) s . status = StatusCollect return nil } // First, we get the next tree update from the feeder. We can skip it if // it doesn't have any updated paths, or if we can't find the tree to apply // it to in the forest. This usually means that it was meant for a pruned // branch of the execution forest. update , err := t . feed . Update () if errors . Is ( err , dps . ErrUnavailable ) { time . Sleep ( t . cfg . WaitInterval ) log . Debug (). Msg ( \"waiting for next trie update\" ) return nil } if err != nil { return fmt . Errorf ( \"could not feed update: %w\" , err ) } parent := flow . StateCommitment ( update . RootHash ) tree , ok := s . forest . Tree ( parent ) if ! ok { log . Warn (). Msg ( \"state commitment mismatch, retrieving next trie update\" ) return nil } // We then apply the update to the relevant tree, as retrieved from the // forest, and save the updated tree in the forest. If the tree is not new, // we should error, as that should not happen. paths , payloads := pathsPayloads ( update ) tree , err = trie . NewTrieWithUpdatedRegisters ( tree , paths , payloads ) if err != nil { return fmt . Errorf ( \"could not update tree: %w\" , err ) } s . forest . Save ( tree , paths , parent ) hash := tree . RootHash () log . Info (). Hex ( \"commit\" , hash [:]). Int ( \"registers\" , len ( paths )). Msg ( \"updated tree with register payloads\" ) return nil } // CollectRegisters reads the payloads for the next block to be indexed from the state's forest, unless payload // indexing is disabled. func ( t * Transitions ) CollectRegisters ( s * State ) error { log := t . log . With (). Uint64 ( \"height\" , s . height ). Hex ( \"commit\" , s . next [:]). Logger () if s . status != StatusCollect { return fmt . Errorf ( \"invalid status for collecting registers (%s)\" , s . status ) } // If indexing payloads is disabled, we can bypass collection and indexing // of payloads and just go straight to forwarding the height to the next // finalized block. if t . cfg . SkipRegisters { s . status = StatusForward return nil } // If we index payloads, we are basically stepping back from (and including) // the tree that corresponds to the next finalized block all the way up to // (and excluding) the tree for the last finalized block we indexed. To do // so, we will use the parent state commit to retrieve the parent trees from // the forest, and we use the paths we recorded changes on to retrieve the // changed payloads at each step. commit := s . next for commit != s . last { // We do this check only once, so that we don't need to do it for // each item we retrieve. The tree should always be there, but we // should check just to not fail silently. ok := s . forest . Has ( commit ) if ! ok { return fmt . Errorf ( \"could not load tree (commit: %x)\" , commit ) } // For each path, we retrieve the payload and add it to the registers we // will index later. If we already have a payload for the path, it is // more recent as we iterate backwards in time, so we can skip the // outdated payload. // NOTE: We read from the tree one by one here, as the performance // overhead is minimal compared to the disk i/o for badger, and it // allows us to ignore sorting of paths. tree , _ := s . forest . Tree ( commit ) paths , _ := s . forest . Paths ( commit ) for _ , path := range paths { _ , ok := s . registers [ path ] if ok { continue } payloads := tree . UnsafeRead ([] ledger . Path { path }) s . registers [ path ] = payloads [ 0 ] } log . Debug (). Int ( \"batch\" , len ( paths )). Msg ( \"collected register batch for finalized block\" ) // We now step back to the parent of the current state trie. parent , _ := s . forest . Parent ( commit ) commit = parent } log . Info (). Int ( \"registers\" , len ( s . registers )). Msg ( \"collected all registers for finalized block\" ) // At this point, we have collected all the payloads, so we go to the next // step, where we will index them. s . status = StatusMap return nil } // MapRegisters maps the collected registers to the current block. func ( t * Transitions ) MapRegisters ( s * State ) error { if s . status != StatusMap { return fmt . Errorf ( \"invalid status for indexing registers (%s)\" , s . status ) } log := t . log . With (). Uint64 ( \"height\" , s . height ). Hex ( \"commit\" , s . next [:]). Logger () // If there are no registers left to be indexed, we can go to the next step, // which is about forwarding the height to the next finalized block. if len ( s . registers ) == 0 { log . Info (). Msg ( \"indexed all registers for finalized block\" ) s . status = StatusForward return nil } // We will now collect and index 1000 registers at a time. This gives the // FSM the chance to exit the loop between every 1000 payloads we index. It // doesn't really matter for badger if they are in random order, so this // way of iterating should be fine. n := 1000 paths := make ([] ledger . Path , 0 , n ) payloads := make ([] * ledger . Payload , 0 , n ) for path , payload := range s . registers { paths = append ( paths , path ) payloads = append ( payloads , payload ) delete ( s . registers , path ) if len ( paths ) >= n { break } } // Then we store the (maximum) 1000 paths and payloads. err := t . write . Payloads ( s . height , paths , payloads ) if err != nil { return fmt . Errorf ( \"could not index registers: %w\" , err ) } log . Debug (). Int ( \"batch\" , len ( paths )). Int ( \"remaining\" , len ( s . registers )). Msg ( \"indexed register batch for finalized block\" ) return nil } // ForwardHeight increments the height at which the mapping operates, and updates the last indexed height. func ( t * Transitions ) ForwardHeight ( s * State ) error { if s . status != StatusForward { return fmt . Errorf ( \"invalid status for forwarding height (%s)\" , s . status ) } // After finishing the indexing of the payloads for a finalized block, or // skipping it, we should document the last indexed height. On the first // pass, we will also index the first indexed height here. var err error t . once . Do ( func () { err = t . write . First ( s . height ) }) if err != nil { return fmt . Errorf ( \"could not index first height: %w\" , err ) } err = t . write . Last ( s . height ) if err != nil { return fmt . Errorf ( \"could not index last height: %w\" , err ) } // Now that we have indexed the heights, we can forward to the next height, // and reset the forest to free up memory. s . height ++ s . forest . Reset ( s . next ) t . log . Info (). Uint64 ( \"height\" , s . height ). Msg ( \"forwarded finalized block to next height\" ) // Once the height is forwarded, we can set the status so that we index // the blockchain data next. s . status = StatusIndex return nil } This refactoring effort allowed us to write simple and concise tests that call a transition function upon the state machine and make assertions upon the resulting state. mapper_new_internal_test.go package mapper import ( \"sync\" \"testing\" \"github.com/stretchr/testify/assert\" \"github.com/stretchr/testify/require\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" \"github.com/optakt/flow-dps/testing/mocks\" ) func TestNewTransitions ( t * testing . T ) { t . Run ( \"nominal case, without options\" , func ( t * testing . T ) { load := mocks . BaselineLoader ( t ) chain := mocks . BaselineChain ( t ) feed := mocks . BaselineFeeder ( t ) read := mocks . BaselineReader ( t ) write := mocks . BaselineWriter ( t ) tr := NewTransitions ( mocks . NoopLogger , load , chain , feed , read , write ) assert . NotNil ( t , tr ) assert . Equal ( t , chain , tr . chain ) assert . Equal ( t , feed , tr . feed ) assert . Equal ( t , write , tr . write ) assert . NotNil ( t , tr . once ) assert . Equal ( t , DefaultConfig , tr . cfg ) }) t . Run ( \"nominal case, with option\" , func ( t * testing . T ) { load := mocks . BaselineLoader ( t ) chain := mocks . BaselineChain ( t ) feed := mocks . BaselineFeeder ( t ) read := mocks . BaselineReader ( t ) write := mocks . BaselineWriter ( t ) skip := true tr := NewTransitions ( mocks . NoopLogger , load , chain , feed , read , write , WithSkipRegisters ( skip ), ) assert . NotNil ( t , tr ) assert . Equal ( t , chain , tr . chain ) assert . Equal ( t , feed , tr . feed ) assert . Equal ( t , write , tr . write ) assert . NotNil ( t , tr . once ) assert . NotEqual ( t , DefaultConfig , tr . cfg ) assert . Equal ( t , skip , tr . cfg . SkipRegisters ) assert . Equal ( t , DefaultConfig . WaitInterval , tr . cfg . WaitInterval ) }) } func TestTransitions_BootstrapState ( t * testing . T ) { t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) // Copy state in local scope so that we can override its SaveFunc without impacting other // tests running in parallel. var saveCalled bool forest := mocks . BaselineForest ( t , true ) forest . SaveFunc = func ( tree * trie . MTrie , paths [] ledger . Path , parent flow . StateCommitment ) { if ! saveCalled { assert . True ( t , tree . IsEmpty ()) assert . Nil ( t , paths ) assert . Zero ( t , parent ) saveCalled = true return } assert . False ( t , tree . IsEmpty ()) assert . Len ( t , tree . AllPayloads (), len ( paths )) assert . Len ( t , paths , 3 ) // Expect the three paths from leaves. assert . NotZero ( t , parent ) } err := tr . BootstrapState ( st ) assert . NoError ( t , err ) }) t . Run ( \"invalid state\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusForward ) err := tr . BootstrapState ( st ) assert . Error ( t , err ) }) t . Run ( \"handles failure to get root height\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return 0 , mocks . GenericError } tr . chain = chain err := tr . BootstrapState ( st ) assert . Error ( t , err ) }) } func TestTransitions_IndexChain ( t * testing . T ) { t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . HeaderFunc = func ( height uint64 ) ( * flow . Header , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericHeader , nil } chain . CommitFunc = func ( height uint64 ) ( flow . StateCommitment , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericCommit ( 0 ), nil } chain . CollectionsFunc = func ( height uint64 ) ([] * flow . LightCollection , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericCollections ( 2 ), nil } chain . GuaranteesFunc = func ( height uint64 ) ([] * flow . CollectionGuarantee , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericGuarantees ( 2 ), nil } chain . TransactionsFunc = func ( height uint64 ) ([] * flow . TransactionBody , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericTransactions ( 4 ), nil } chain . ResultsFunc = func ( height uint64 ) ([] * flow . TransactionResult , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericResults ( 4 ), nil } chain . EventsFunc = func ( height uint64 ) ([] flow . Event , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericEvents ( 8 ), nil } chain . SealsFunc = func ( height uint64 ) ([] * flow . Seal , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericSeals ( 4 ), nil } write := mocks . BaselineWriter ( t ) write . HeaderFunc = func ( height uint64 , header * flow . Header ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericHeader , header ) return nil } write . CommitFunc = func ( height uint64 , commit flow . StateCommitment ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericCommit ( 0 ), commit ) return nil } write . HeightFunc = func ( blockID flow . Identifier , height uint64 ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericHeader . ID (), blockID ) return nil } write . CollectionsFunc = func ( height uint64 , collections [] * flow . LightCollection ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericCollections ( 2 ), collections ) return nil } write . GuaranteesFunc = func ( height uint64 , guarantees [] * flow . CollectionGuarantee ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericGuarantees ( 2 ), guarantees ) return nil } write . TransactionsFunc = func ( height uint64 , transactions [] * flow . TransactionBody ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericTransactions ( 4 ), transactions ) return nil } write . ResultsFunc = func ( results [] * flow . TransactionResult ) error { assert . Equal ( t , mocks . GenericResults ( 4 ), results ) return nil } write . EventsFunc = func ( height uint64 , events [] flow . Event ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericEvents ( 8 ), events ) return nil } write . SealsFunc = func ( height uint64 , seals [] * flow . Seal ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericSeals ( 4 ), seals ) return nil } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain tr . write = write err := tr . IndexChain ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusUpdate , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve commit\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return flow . DummyStateCommitment , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index commit\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . CommitFunc = func ( uint64 , flow . StateCommitment ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve header\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . HeaderFunc = func ( uint64 ) ( * flow . Header , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index header\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . HeaderFunc = func ( uint64 , * flow . Header ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve transactions\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . TransactionsFunc = func ( uint64 ) ([] * flow . TransactionBody , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve transaction results\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . ResultsFunc = func ( uint64 ) ([] * flow . TransactionResult , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index transactions\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . ResultsFunc = func ([] * flow . TransactionResult ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve collections\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . CollectionsFunc = func ( uint64 ) ([] * flow . LightCollection , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index collections\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . CollectionsFunc = func ( uint64 , [] * flow . LightCollection ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve guarantees\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . GuaranteesFunc = func ( uint64 ) ([] * flow . CollectionGuarantee , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index guarantees\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . GuaranteesFunc = func ( uint64 , [] * flow . CollectionGuarantee ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve events\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . EventsFunc = func ( uint64 ) ([] flow . Event , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index events\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . EventsFunc = func ( uint64 , [] flow . Event ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve seals\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . SealsFunc = func ( uint64 ) ([] * flow . Seal , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index seals\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . SealsFunc = func ( uint64 , [] * flow . Seal ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) } func TestTransitions_UpdateTree ( t * testing . T ) { update := mocks . GenericTrieUpdate ( 0 ) tree := mocks . GenericTrie t . Run ( \"nominal case without match\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusUpdate ) forest := mocks . BaselineForest ( t , false ) forest . SaveFunc = func ( tree * trie . MTrie , paths [] ledger . Path , parent flow . StateCommitment ) { // Parent is RootHash of the mocks.GenericTrie. assert . Equal ( t , update . RootHash [:], parent [:]) assert . ElementsMatch ( t , paths , update . Paths ) assert . NotZero ( t , tree ) } forest . TreeFunc = func ( commit flow . StateCommitment ) ( * trie . MTrie , bool ) { assert . Equal ( t , update . RootHash [:], commit [:]) return tree , true } st . forest = forest err := tr . UpdateTree ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusUpdate , st . status ) }) t . Run ( \"nominal case with no available update temporarily\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusUpdate ) // Set up the mock feeder to return an unavailable error on the first call and return successfully // to subsequent calls. var updateCalled bool feeder := mocks . BaselineFeeder ( t ) feeder . UpdateFunc = func () ( * ledger . TrieUpdate , error ) { if ! updateCalled { updateCalled = true return nil , dps . ErrUnavailable } return mocks . GenericTrieUpdate ( 0 ), nil } tr . feed = feeder forest := mocks . BaselineForest ( t , true ) forest . HasFunc = func ( flow . StateCommitment ) bool { return updateCalled } st . forest = forest // The first call should not error but should not change the status of the FSM to updating. It should // instead remain Updating until a match is found. err := tr . UpdateTree ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusUpdate , st . status ) // The second call is now successful and matches. err = tr . UpdateTree ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusCollect , st . status ) }) t . Run ( \"nominal case with match\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusUpdate ) err := tr . UpdateTree ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusCollect , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) err := tr . UpdateTree ( st ) assert . Error ( t , err ) }) t . Run ( \"handles feeder update failure\" , func ( t * testing . T ) { t . Parallel () feed := mocks . BaselineFeeder ( t ) feed . UpdateFunc = func () ( * ledger . TrieUpdate , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusUpdate ) st . forest = mocks . BaselineForest ( t , false ) tr . feed = feed err := tr . UpdateTree ( st ) assert . Error ( t , err ) }) t . Run ( \"handles forest parent tree not found\" , func ( t * testing . T ) { t . Parallel () forest := mocks . BaselineForest ( t , false ) forest . TreeFunc = func ( _ flow . StateCommitment ) ( * trie . MTrie , bool ) { return nil , false } tr , st := baselineFSM ( t , StatusUpdate ) st . forest = forest err := tr . UpdateTree ( st ) assert . NoError ( t , err ) }) } func TestTransitions_CollectRegisters ( t * testing . T ) { t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () forest := mocks . BaselineForest ( t , true ) forest . ParentFunc = func ( commit flow . StateCommitment ) ( flow . StateCommitment , bool ) { assert . Equal ( t , mocks . GenericCommit ( 0 ), commit ) return mocks . GenericCommit ( 1 ), true } tr , st := baselineFSM ( t , StatusCollect ) st . forest = forest err := tr . CollectRegisters ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusMap , st . status ) for _ , wantPath := range mocks . GenericLedgerPaths ( 6 ) { assert . Contains ( t , st . registers , wantPath ) } }) t . Run ( \"indexing payloads disabled\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusCollect ) tr . cfg . SkipRegisters = true err := tr . CollectRegisters ( st ) require . NoError ( t , err ) assert . Empty ( t , st . registers ) assert . Equal ( t , StatusForward , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) err := tr . CollectRegisters ( st ) assert . Error ( t , err ) assert . Empty ( t , st . registers ) }) t . Run ( \"handles missing tree for commit\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusCollect ) st . forest = mocks . BaselineForest ( t , false ) err := tr . CollectRegisters ( st ) assert . Error ( t , err ) assert . Empty ( t , st . registers ) }) } func TestTransitions_MapRegisters ( t * testing . T ) { t . Run ( \"nominal case with registers to write\" , func ( t * testing . T ) { t . Parallel () // Path 2 and 4 are the same so the map effectively contains 5 entries. testRegisters := map [ ledger . Path ] * ledger . Payload { mocks . GenericLedgerPath ( 0 ): mocks . GenericLedgerPayload ( 0 ), mocks . GenericLedgerPath ( 1 ): mocks . GenericLedgerPayload ( 1 ), mocks . GenericLedgerPath ( 2 ): mocks . GenericLedgerPayload ( 2 ), mocks . GenericLedgerPath ( 1 ): mocks . GenericLedgerPayload ( 3 ), mocks . GenericLedgerPath ( 4 ): mocks . GenericLedgerPayload ( 4 ), mocks . GenericLedgerPath ( 5 ): mocks . GenericLedgerPayload ( 5 ), } write := mocks . BaselineWriter ( t ) write . PayloadsFunc = func ( height uint64 , paths [] ledger . Path , value [] * ledger . Payload ) error { assert . Equal ( t , mocks . GenericHeight , height ) // Expect the 5 entries from the map. assert . Len ( t , paths , 5 ) assert . Len ( t , value , 5 ) return nil } tr , st := baselineFSM ( t , StatusMap ) tr . write = write st . registers = testRegisters err := tr . MapRegisters ( st ) require . NoError ( t , err ) // Should not be StateIndexed because registers map was not empty. assert . Empty ( t , st . registers ) assert . Equal ( t , StatusMap , st . status ) }) t . Run ( \"nominal case no more registers left to write\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusMap ) err := tr . MapRegisters ( st ) assert . NoError ( t , err ) assert . Equal ( t , StatusForward , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () testRegisters := map [ ledger . Path ] * ledger . Payload { mocks . GenericLedgerPath ( 0 ): mocks . GenericLedgerPayload ( 0 ), mocks . GenericLedgerPath ( 1 ): mocks . GenericLedgerPayload ( 1 ), mocks . GenericLedgerPath ( 2 ): mocks . GenericLedgerPayload ( 2 ), mocks . GenericLedgerPath ( 3 ): mocks . GenericLedgerPayload ( 3 ), mocks . GenericLedgerPath ( 4 ): mocks . GenericLedgerPayload ( 4 ), mocks . GenericLedgerPath ( 5 ): mocks . GenericLedgerPayload ( 5 ), } tr , st := baselineFSM ( t , StatusBootstrap ) st . registers = testRegisters err := tr . MapRegisters ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure\" , func ( t * testing . T ) { t . Parallel () testRegisters := map [ ledger . Path ] * ledger . Payload { mocks . GenericLedgerPath ( 0 ): mocks . GenericLedgerPayload ( 0 ), mocks . GenericLedgerPath ( 1 ): mocks . GenericLedgerPayload ( 1 ), mocks . GenericLedgerPath ( 2 ): mocks . GenericLedgerPayload ( 2 ), mocks . GenericLedgerPath ( 3 ): mocks . GenericLedgerPayload ( 3 ), mocks . GenericLedgerPath ( 4 ): mocks . GenericLedgerPayload ( 4 ), mocks . GenericLedgerPath ( 5 ): mocks . GenericLedgerPayload ( 5 ), } write := mocks . BaselineWriter ( t ) write . PayloadsFunc = func ( uint64 , [] ledger . Path , [] * ledger . Payload ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusMap ) tr . write = write st . registers = testRegisters err := tr . MapRegisters ( st ) assert . Error ( t , err ) }) } func TestTransitions_ForwardHeight ( t * testing . T ) { t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () var ( firstCalled int lastCalled int ) write := mocks . BaselineWriter ( t ) write . FirstFunc = func ( height uint64 ) error { assert . Equal ( t , mocks . GenericHeight , height ) firstCalled ++ return nil } write . LastFunc = func ( height uint64 ) error { assert . Equal ( t , mocks . GenericHeight + uint64 ( lastCalled ), height ) lastCalled ++ return nil } forest := mocks . BaselineForest ( t , true ) forest . ResetFunc = func ( finalized flow . StateCommitment ) { assert . Equal ( t , mocks . GenericCommit ( 0 ), finalized ) } tr , st := baselineFSM ( t , StatusForward ) st . forest = forest tr . write = write err := tr . ForwardHeight ( st ) assert . NoError ( t , err ) assert . Equal ( t , StatusIndex , st . status ) assert . Equal ( t , mocks . GenericHeight + 1 , st . height ) // Reset status to allow next call. st . status = StatusForward err = tr . ForwardHeight ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusIndex , st . status ) assert . Equal ( t , mocks . GenericHeight + 2 , st . height ) // First should have been called only once. assert . Equal ( t , 1 , firstCalled ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) err := tr . ForwardHeight ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer error on first\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . FirstFunc = func ( uint64 ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusForward ) tr . write = write err := tr . ForwardHeight ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer error on last\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . LastFunc = func ( uint64 ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusForward ) tr . write = write err := tr . ForwardHeight ( st ) assert . Error ( t , err ) }) } func TestTransitions_InitializeMapper ( t * testing . T ) { t . Run ( \"switches state to BootstrapState if configured to do so\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusInitialize ) tr . cfg . BootstrapState = true err := tr . InitializeMapper ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusBootstrap , st . status ) }) t . Run ( \"switches state to StatusResume if no bootstrapping configured\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusInitialize ) tr . cfg . BootstrapState = false err := tr . InitializeMapper ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusResume , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusForward ) err := tr . InitializeMapper ( st ) require . Error ( t , err ) }) } func TestTransitions_ResumeIndexing ( t * testing . T ) { header := mocks . GenericHeader tree := mocks . GenericTrie commit := flow . StateCommitment ( tree . RootHash ()) differentCommit := mocks . GenericCommit ( 0 ) t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } writer := mocks . BaselineWriter ( t ) writer . FirstFunc = func ( height uint64 ) error { assert . Equal ( t , header . Height , height ) return nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( height uint64 ) ( flow . StateCommitment , error ) { assert . Equal ( t , header . Height , height ) return commit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withWriter ( writer ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusIndex , st . status ) assert . Equal ( t , header . Height + 1 , st . height ) assert . Equal ( t , flow . DummyStateCommitment , st . last ) assert . Equal ( t , commit , st . next ) }) t . Run ( \"handles chain failure on Root\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return 0 , mocks . GenericError } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure on First\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } writer := mocks . BaselineWriter ( t ) writer . FirstFunc = func ( uint64 ) error { return mocks . GenericError } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusResume , withWriter ( writer ), withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles reader failure on Last\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return 0 , mocks . GenericError } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles reader failure on Commit\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return flow . DummyStateCommitment , mocks . GenericError } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles loader failure on Trie\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return nil , mocks . GenericError } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles mismatch between tree root hash and indexed commit\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return differentCommit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusForward , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) } func baselineFSM ( t * testing . T , status Status , opts ... func ( tr * Transitions )) ( * Transitions , * State ) { t . Helper () load := mocks . BaselineLoader ( t ) chain := mocks . BaselineChain ( t ) feeder := mocks . BaselineFeeder ( t ) read := mocks . BaselineReader ( t ) write := mocks . BaselineWriter ( t ) forest := mocks . BaselineForest ( t , true ) once := & sync . Once {} doneCh := make ( chan struct {}) tr := Transitions { cfg : Config { BootstrapState : false , SkipRegisters : false , WaitInterval : 0 , }, log : mocks . NoopLogger , load : load , chain : chain , feed : feeder , read : read , write : write , once : once , } for _ , opt := range opts { opt ( & tr ) } st := State { forest : forest , status : status , height : mocks . GenericHeight , last : mocks . GenericCommit ( 1 ), next : mocks . GenericCommit ( 0 ), registers : make ( map [ ledger . Path ] * ledger . Payload ), done : doneCh , } return & tr , & st } func withLoader ( load Loader ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . load = load } } func withChain ( chain dps . Chain ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . chain = chain } } func withFeeder ( feed Feeder ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . feed = feed } } func withReader ( read dps . Reader ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . read = read } } func withWriter ( write dps . Writer ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . write = write } } Integration Tests Integration tests are essential to ensure that components work together as expected. Those tests are usually much heavier and slower than unit tests, since they use real components instead of simple mocks, and often might run filesystem or network operations, wait for things to happen, or even run heavy computational tasks. Integration tests should always be specified in a separate test package and never run internally within the tested package. Build Tag Because integration tests are inherently slower than unit tests, they are placed in specific files that are suffixed with _integration_test.go and those files start with a build tag directive which prevents them from running unless the go test command is called with the integration tag. Both syntaxes should be specified, the <go1.17 one which is +build <tag> as well as the >=go1.17 one which is go:build <tag> . The former will be dropped when we feel like it is no longer relevant to support go 1.16 and prior. //go:build integration // +build integration package dps_test Examples In Go, good package documentation includes not only comments for each public type and method, but also runnable examples and benchmarks in some cases. Godoc allows defining examples which are verified by running them as tests and can be manually launched by readers of the documentation on the package's Godoc webpage. As for typical tests, examples are functions that reside in a package's _test.go files. Unlike normal test functions, though, example functions take no arguments and begin with the word Example instead of Test . In order to specify what is the expected output of a given example, a comment has to be written at the end of the Example function, in the form of // Output: <expected output> . If this is missing, examples will not be executed and therefore not included in the documentation. Benchmarks When a package exposes a performance-critical piece of code, it should be benchmarked, and benchmark tests must be available for anyone to reproduce the benchmark using their hardware. Writing benchmark results in a markdown file without providing a way to reproduce them is irrelevant.","title":"Testing Guide"},{"location":"testing/#testing-guide","text":"At Optakt, we consistently write tests to ensure a reliable engineering environment where quality is paramount. Over the course of the product development life cycle, testing saves time and money, and helps developers write better code, more efficiently. Untested code is fragile, difficult to maintain and becomes questionable as soon as changes are made. This guide assumes that you are already familiar with Go testing.","title":"Testing Guide"},{"location":"testing/#unit-tests","text":"This section outlines a few of the rules we try to follow when it comes to Go unit tests.","title":"Unit Tests"},{"location":"testing/#naming-conventions","text":"Unit tests should have consistent names. The best way to go about it is to follow the official guidelines of the Go testing package , which states that: The naming convention to declare tests for the package, a function F , a type T and method M on type T are: func Test () { ... } func TestF () { ... } func TestT () { ... } func TestT_M () { ... } When it comes to subtests, the names of individual subtests should be lowercased and concise. The tests usually start with a subtest called nominal case which verifies that the tested component behaves as expected in a baseline situation, where no failures occur and no edge cases are handled.","title":"Naming Conventions"},{"location":"testing/#internal-unit-tests","text":"In most cases, packages can be tested using external tests only. When writing tests for a package called xyz , the external tests should be in the same folder, but in a package called xyz_test . This case is handled by Go natively and will therefore not result in complaints about there being two different packages within the same directory. Of course, there are exceptions. If you need to test some internal logic, those tests must be in a file suffixed with _internal_test.go .","title":"Internal Unit Tests"},{"location":"testing/#mocks","text":"When it comes to mocking dependencies for tests, we prefer to use simple hand-made mocks rather than to use testing frameworks to generate them. The Go language makes it easy to do so elegantly by creating structures that implement the interfaces for dependencies of the tested code and exposing functions that match the interface's signature as attributes that can be overridden externally. package mocks import ( \"testing\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" ) type Loader struct { TrieFunc func () ( * trie . MTrie , error ) } func BaselineLoader ( t * testing . T ) * Loader { t . Helper () l := Loader { TrieFunc : func () ( * trie . MTrie , error ) { return GenericTrie , nil }, } return & l } func ( l * Loader ) Trie () ( * trie . MTrie , error ) { return l . TrieFunc () } Using those mocks is as simple as instantiating a baseline version of the mock and setting its attributes to the desired functions: // ... t . Run ( \"handles failure to load checkpoint\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusEmpty ) load := mocks . BaselineLoader ( t ) load . CheckpointFunc = func () ( * trie . MTrie , error ) { return nil , mocks . GenericError } tr . load = load err := tr . BootstrapState ( st ) assert . Error ( t , err ) }) // ...","title":"Mocks"},{"location":"testing/#pseudorandom-generic-values","text":"When using test data for unit tests, it is always a good idea to use random generated data as the inputs. This avoids the bias where a test passes because it is given a valid set of inputs while some other inputs might have highlighted a flaw in the logic, by using an unconstrained data set. In order for the tests to be repeatable and for results to be consistent though, the given inputs should not be completely random, but instead they should be pseudorandom, with the same initial seed, to ensure the same sequence of \"random\" tests. Here is an example of such a value being generated. func GenericAddresses ( number int ) [] flow . Address { // Ensure consistent deterministic results. random := rand . New ( rand . NewSource ( 5 )) var addresses [] flow . Address for i := 0 ; i < number ; i ++ { var address flow . Address binary . BigEndian . PutUint64 ( address [ 0 :], random . Uint64 ()) addresses = append ( addresses , address ) } return addresses } func GenericAddress ( index int ) flow . Address { return GenericAddresses ( index + 1 )[ index ] } Warning While randomly generating valid inputs makes sense, randomly generating invalid inputs does not. In the case of invalid inputs, it is much better to have an exhaustive list of all types of cases that are expected to be invalid and always test each one of them.","title":"Pseudorandom Generic Values"},{"location":"testing/#parallelization","text":"Since the version 1.7 of Go, tests can be run in parallel. This can be done by calling t.Parallel in each subtest. Calling this function signals that the test is to be run in parallel with (and only with) other parallel tests, and the amount of tests running in parallel is limited by the value of runtime.GOMAXPROCS . There are multiple advantages to parallelizing tests: It ensures that regardless of the order in which inputs are given, components behave as expected. It maximizes performance, which in turns results in a faster CI and a faster workflow for everyone, which allows us to write more tests and therefore produce more actionable data to find bugs as well as improve tests cases and coverage. It makes it possible to ensure that the components you test are concurrency-safe Parallelizing table-driven tests When it comes to table-driven tests, a common pitfall developers fall into is to call t.Parallel in their subtest without capturing the loop variable with their test case. Here is an example of how it should be done: func TestGroupedParallel ( t * testing . T ) { for _ , tc := range tests { tc := tc // capture range variable t . Run ( tc . Name , func ( t * testing . T ) { t . Parallel () ... }) } }","title":"Parallelization"},{"location":"testing/#standard-testing-package","text":"The standard testing package is very powerful, and does not require additional frameworks to be used efficiently. The only exception we make to that are the stretchr/testify/assert and stretchr/testify/require packages which we use only for convenience, as they expose assertion functions that produce consistent outputs and make tests easy to understand.","title":"Standard testing Package"},{"location":"testing/#subtests-and-sub-benchmarks","text":"The testing package exposes a Run method on the T type which makes it possible to nest tests within tests. This can be very useful, as it enables creating a hierarchical structure within a test. index_internal_test.go func TestIndex ( t * testing . T ) { // ... t . Run ( \"collections\" , func ( t * testing . T ) { t . Parallel () collections := mocks . GenericCollections ( 4 ) reader , writer , db := setupIndex ( t ) defer db . Close () assert . NoError ( t , writer . Collections ( mocks . GenericHeight , collections )) // Close the writer to make it commit its transactions. require . NoError ( t , writer . Close ()) // NOTE: The following subtests should NOT be run in parallel, because of the deferral // to close the database above. t . Run ( \"retrieve collection by ID\" , func ( t * testing . T ) { got , err := reader . Collection ( collections [ 0 ]. ID ()) require . NoError ( t , err ) assert . Equal ( t , collections [ 0 ], got ) }) t . Run ( \"retrieve collections by height\" , func ( t * testing . T ) { got , err := reader . CollectionsByHeight ( mocks . GenericHeight ) require . NoError ( t , err ) assert . ElementsMatch ( t , mocks . GenericCollectionIDs ( 4 ), got ) }) t . Run ( \"retrieve transactions from collection\" , func ( t * testing . T ) { // For now this index is not used. }) }) // ... }","title":"Subtests and Sub-benchmarks"},{"location":"testing/#table-driven-tests","text":"It makes a lot of sense to use subtests when testing the behavior of complex components, but it is better to use table-driven tests when testing a simple function with an expected output for a given input, and that there are many cases to cover. For such cases, table-driven tests massively improve clarity and readability. They should not be used blindly for all tests, however. In cases where the tested component is complex and that testing its methods cannot be simplified to a common setup, call to a function and assertion of the output, trying to use table-driven tests at all costs might lead to messy code, where the subtest which runs the test case is full of conditions to try to handle each separate setup. This is usually a sign that using simple tests and subtests would be a better approach.","title":"Table-Driven Tests"},{"location":"testing/#case-study-the-flow-dps-mapper","text":"Sometimes, a core piece of software might seem impossible to test. That was the case for the mapper component in Flow DPS at some point, where its main function consisted of a 453-lines-long loop which orchestrated the use of all the other components of the application. mapper_old.go package mapper import ( \"bytes\" \"context\" \"errors\" \"fmt\" \"os\" \"sort\" \"sync\" \"github.com/gammazero/deque\" \"github.com/rs/zerolog\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/flattener\" \"github.com/onflow/flow-go/ledger/complete/mtrie/node\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/ledger/complete/wal\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" \"github.com/optakt/flow-dps/models/index\" ) type Mapper struct { log zerolog . Logger cfg Config chain Chain feed Feeder index index . Writer wg * sync . WaitGroup stop chan struct {} } // New creates a new mapper that uses chain data to map trie updates to blocks // and then passes on the details to the indexer for indexing. func New ( log zerolog . Logger , chain Chain , feed Feeder , index index . Writer , options ... func ( * Config )) ( * Mapper , error ) { // We don't use a checkpoint by default. The options can set one, in which // case we will add the checkpoint as a finalized state commitment in our // trie registry. cfg := Config { CheckpointFile : \"\" , PostProcessing : PostNoop , } for _ , option := range options { option ( & cfg ) } // Check if the checkpoint file exists. if cfg . CheckpointFile != \"\" { stat , err := os . Stat ( cfg . CheckpointFile ) if err != nil { return nil , fmt . Errorf ( \"invalid checkpoint file: %w\" , err ) } if stat . IsDir () { return nil , fmt . Errorf ( \"invalid checkpoint file: directory\" ) } } i := Mapper { log : log , chain : chain , feed : feed , index : index , cfg : cfg , wg : & sync . WaitGroup {}, stop : make ( chan struct {}), } return & i , nil } func ( m * Mapper ) Stop ( ctx context . Context ) error { close ( m . stop ) done := make ( chan struct {}) go func () { m . wg . Wait () close ( done ) }() select { case <- ctx . Done (): return ctx . Err () case <- done : return nil } } // NOTE: We might want to move height and tree (checkpoint) to parameters of the // run function; that would make it quite easy to resume from an arbitrary // point in the LedgerWAL and get rid of the related struct fields. func ( m * Mapper ) Run () error { m . wg . Add ( 1 ) defer m . wg . Done () // We start trying to map at the root height. height , err := m . chain . Root () if err != nil { return fmt . Errorf ( \"could not get root height: %w\" , err ) } // We always initialize an empty state trie to refer to the first step // before the checkpoint. If there is no checkpoint, then the step after the // checkpoint will also just be the empty trie. Otherwise, the second trie // will load the checkpoint trie. empty := trie . NewEmptyMTrie () var tree * trie . MTrie if m . cfg . CheckpointFile == \"\" { tree = empty } else { m . log . Info (). Msg ( \"checkpoint rebuild started\" ) file , err := os . Open ( m . cfg . CheckpointFile ) if err != nil { return fmt . Errorf ( \"could not open checkpoint file: %w\" , err ) } checkpoint , err := wal . ReadCheckpoint ( file ) if err != nil { return fmt . Errorf ( \"could not read checkpoint: %w\" , err ) } trees , err := flattener . RebuildTries ( checkpoint ) if err != nil { return fmt . Errorf ( \"could not rebuild tries: %w\" , err ) } if len ( trees ) != 1 { return fmt . Errorf ( \"should only have one trie in root checkpoint (tries: %d)\" , len ( trees )) } tree = trees [ 0 ] m . log . Info (). Msg ( \"checkpoint rebuild finished\" ) } m . log . Info (). Msg ( \"path collection started\" ) // We have to index all of the paths from the checkpoint; otherwise, we will // miss every single one of the bootstrapped registers. paths := make ([] ledger . Path , 0 , len ( tree . AllPayloads ())) queue := deque . New () root := tree . RootNode () if root != nil { queue . PushBack ( root ) } for queue . Len () > 0 { node := queue . PopBack ().( * node . Node ) if node . IsLeaf () { path := node . Path () paths = append ( paths , * path ) continue } if node . LeftChild () != nil { queue . PushBack ( node . LeftChild ()) } if node . RightChild () != nil { queue . PushBack ( node . RightChild ()) } } m . log . Info (). Int ( \"paths\" , len ( paths )). Msg ( \"path collection finished\" ) m . log . Info (). Msg ( \"path sorting started\" ) sort . Slice ( paths , func ( i int , j int ) bool { return bytes . Compare ( paths [ i ][:], paths [ j ][:]) < 0 }) m . log . Info (). Msg ( \"path sorting finished\" ) // When trying to go from one finalized block to the next, we keep a list // of intermediary tries until the full set of transitions have been // identified. We keep track of these transitions as steps in this map. steps := make ( map [ flow . StateCommitment ] * Step ) // We start at an \"imaginary\" step that refers to an empty trie, has no // paths and no previous commit. We consider this step already done, so it // will never be indexed; it's merely used as the sentinel value for // stopping when we index the first block. It also makes sure that we don't // return a `nil` trie if we abort indexing before the first block is done. emptyCommit := flow . DummyStateCommitment steps [ emptyCommit ] = & Step { Commit : flow . StateCommitment {}, Paths : nil , Tree : empty , } // We then add a second step that refers to the first step that is already // done, which uses the commit of the initial state trie after the // checkpoint has been loaded, and contains all of the paths found in the // initial checkpoint state trie. This will make sure that we index all the // data from the checkpoint as part of the first block. rootCommit := flow . StateCommitment ( tree . RootHash ()) steps [ rootCommit ] = & Step { Commit : emptyCommit , Paths : paths , Tree : tree , } // This is how we let the indexing loop know that the first \"imaginary\" step // was already indexed. The `commitPrev` value is used as a sentinel value // for when to stop going backwards through the steps when indexing a block. // This means the value is always set to the last already indexed step. commitPrev := emptyCommit m . log . Info (). Msg ( \"state indexing started\" ) // Next, we launch into the loop that is responsible for mapping all // incoming trie updates to a block. The loop itself has no concept of what // the next state commitment is that we should look at. It will simply try // to find a previous step for _any_ trie update that comes in. This means // that the first trie update needs to either apply to the empty trie or to // the trie after the checkpoint in order to be processed. once := & sync . Once {} Outer : for { // We want to check in this tight loop if we want to quit, just in case // we get stuck on a timed out network connection. select { case <- m . stop : break Outer default : // keep going } log := m . log . With (). Uint64 ( \"height\" , height ). Hex ( \"commit_prev\" , commitPrev [:]). Logger () // As a first step, we retrieve the state commitment of the finalized // block at the current height; we start at the root height and then // increase it each time we are done indexing a block. Once an applied // trie update gives us a state trie with the same root hash as // `commitNext`, we have reached the end state of the next finalized // block and can index all steps in-between for that block height. commitNext , err := m . chain . Commit ( height ) // If the retrieval times out, it's possible that we are on a live chain // and the next block has not been finalized yet. We should thus simply // retry until we have a new block. if errors . Is ( err , dps . ErrTimeout ) { log . Warn (). Msg ( \"commit retrieval timed out, retrying\" ) continue Outer } // If we have reached the end of the finalized blocks, we are probably // on a historical chain and there are no more finalized blocks for the // related spork. We can exit without error. if errors . Is ( err , dps . ErrFinished ) { log . Debug (). Msg ( \"reached end of finalized chain\" ) break Outer } // Any other error should not happen and should crash explicitly. if err != nil { return fmt . Errorf ( \"could not retrieve next commit (height: %d): %w\" , height , err ) } log = log . With (). Hex ( \"commit_next\" , commitNext [:]). Logger () Inner : for { // We want to check in this tight loop if we want to quit, just in case // we get stuck on a timed out network connection. select { case <- m . stop : break Outer default : // keep going } // When we have the state commitment of the next finalized block, we // check to see if we find a trie for it in our steps. If we do, it // means that we have steps from the last finalized block to the // finalized block at the current height. This condition will // trigger immediately for every empty block. _ , ok := steps [ commitNext ] if ok { break Inner } // If we don't find a trie for the current state commitment, we need // to keep applying trie updates to state tries until one of them // does have the correct commit. We simply feed the next trie update // here. update , err := m . feed . Update () // Once more, we might be on a live spork and the next delta might not // be available yet. In that case, keep trying. if errors . Is ( err , dps . ErrTimeout ) { log . Warn (). Msg ( \"delta retrieval timed out, retrying\" ) continue Inner } // Similarly, if no more deltas are available, we reached the end of // the WAL and we are done reconstructing the execution state. if errors . Is ( err , dps . ErrFinished ) { log . Debug (). Msg ( \"reached end of delta log\" ) break Outer } // Other errors should fail execution as they should not happen. if err != nil { return fmt . Errorf ( \"could not retrieve next delta: %w\" , err ) } // NOTE: We used to require a copy of the `RootHash` here, when it // was still a byte slice, as the underlying slice was being reused. // It was changed to a value type that is always copied now. commitBefore := flow . StateCommitment ( update . RootHash ) log := log . With (). Hex ( \"commit_before\" , commitBefore [:]). Logger () // Once we have our new update and know which trie it should be // applied to, we check to see if we have such a trie in our current // steps. If not, we can simply skip it; this can happen, for // example, when there is an execution fork and the trie update // applies to an obsolete part of the blockchain history. step , ok := steps [ commitBefore ] if ! ok { log . Debug (). Msg ( \"skipping trie update without matching trie\" ) continue Inner } // We de-duplicate the paths and payloads here. This replicates some // code that is part of the execution node and has moved between // different layers of the architecture. We keep it to be safe for // all versions of the Flow dependencies. // NOTE: Past versions of this code required paths to be copied, // because the underlying slice was being re-used. In contrary, // deep-copying payloads was a bad idea, because they were already // being copied by the trie insertion code, and it would have led to // twice the memory usage. paths = make ([] ledger . Path , 0 , len ( update . Paths )) lookup := make ( map [ ledger . Path ] * ledger . Payload ) for i , path := range update . Paths { _ , ok := lookup [ path ] if ! ok { paths = append ( paths , path ) } lookup [ path ] = update . Payloads [ i ] } sort . Slice ( paths , func ( i , j int ) bool { return bytes . Compare ( paths [ i ][:], paths [ j ][:]) < 0 }) payloads := make ([] ledger . Payload , 0 , len ( paths )) for _ , path := range paths { payloads = append ( payloads , * lookup [ path ]) } // We can now apply the trie update to the state trie as it was at // the previous step. This is where the trie code will deep-copy the // payloads. // NOTE: It's important that we don't shadow the variable here, // otherwise the root trie will never go out of scope and we will // never garbage collect any of the root trie payloads that have // been replaced by subsequent trie updates. tree , err = trie . NewTrieWithUpdatedRegisters ( step . Tree , paths , payloads ) if err != nil { return fmt . Errorf ( \"could not update trie: %w\" , err ) } // We then store the new trie along with the state commitment of its // parent and the paths that were changed. This will make it // available for subsequent trie updates to be applied to it, and it // will also allow us to reconstruct the payloads changed in this // step by retrieving them directly from the trie with the given // paths. commitAfter := flow . StateCommitment ( tree . RootHash ()) step = & Step { Commit : commitBefore , Paths : paths , Tree : tree , } steps [ commitAfter ] = step log . Debug (). Hex ( \"commit_after\" , commitAfter [:]). Msg ( \"trie update applied\" ) } // At this point we have identified a step that has lead to the state // commitment of the finalized block at the current height. We can // retrieve some additional indexing data, such as the block header and // the events that resulted from transactions in the block. header , err := m . chain . Header ( height ) if err != nil { return fmt . Errorf ( \"could not retrieve header: %w (height: %d)\" , err , height ) } events , err := m . chain . Events ( height ) if err != nil { return fmt . Errorf ( \"could not retrieve events: %w (height: %d)\" , err , height ) } transactions , err := m . chain . Transactions ( height ) if err != nil { return fmt . Errorf ( \"could not retrieve transactions: %w (height: %d)\" , err , height ) } collections , err := m . chain . Collections ( height ) if err != nil { return fmt . Errorf ( \"could not retrieve collections: %w (height: %d)\" , err , height ) } blockID := header . ID () // TODO: Refactor the mapper in https://github.com/optakt/flow-dps/issues/128 // and replace naive if statements around indexing. // We then index the data for the finalized block at the current height. if m . cfg . IndexHeaders { err = m . index . Header ( height , header ) if err != nil { return fmt . Errorf ( \"could not index header: %w\" , err ) } } if m . cfg . IndexCommit { err = m . index . Commit ( height , commitNext ) if err != nil { return fmt . Errorf ( \"could not index commit: %w\" , err ) } } if m . cfg . IndexEvents { err = m . index . Events ( height , events ) if err != nil { return fmt . Errorf ( \"could not index events: %w\" , err ) } } if m . cfg . IndexBlocks { err = m . index . Height ( blockID , height ) if err != nil { return fmt . Errorf ( \"could not index block heights: %w\" , err ) } } if m . cfg . IndexTransactions { err = m . index . Transactions ( blockID , collections , transactions ) if err != nil { return fmt . Errorf ( \"could not index transactions: %w\" , err ) } } // In order to index the payloads, we step back from the state // commitment of the finalized block at the current height to the state // commitment of the last finalized block that was indexed. For each // step, we collect all the payloads by using the paths for the step and // index them as we go. // NOTE: We keep track of the paths for which we already indexed // payloads, so we can skip them in earlier steps. One inherent benefit // of stepping from the last step to the first step is that this will // automatically use only the latest update of a register, which is // exactly what we want. commit := commitNext updated := make ( map [ ledger . Path ] struct {}) for commit != commitPrev { // In the first part, we get the step we are currently at and filter // out any paths that have already been updated. step := steps [ commit ] paths := make ([] ledger . Path , 0 , len ( step . Paths )) for _ , path := range step . Paths { _ , ok := updated [ path ] if ok { continue } paths = append ( paths , path ) updated [ path ] = struct {}{} } if ! m . cfg . IndexPayloads { commit = step . Commit continue } // We then divide the remaining paths into chunks of 1000. For each // batch, we retrieve the payloads from the state trie as it was at // the end of this block and index them. count := 0 n := 1000 total := ( len ( paths ) + n - 1 ) / n log . Debug (). Int ( \"num_paths\" , len ( paths )). Int ( \"num_batches\" , total ). Msg ( \"path batching executed\" ) for start := 0 ; start < len ( paths ); start += n { // This loop may take a while, especially for the root checkpoint // updates, so check if we should quit. select { case <- m . stop : break Outer default : // keep going } end := start + n if end > len ( paths ) { end = len ( paths ) } batch := paths [ start : end ] payloads := step . Tree . UnsafeRead ( batch ) err = m . index . Payloads ( height , batch , payloads ) if err != nil { return fmt . Errorf ( \"could not index payloads: %w\" , err ) } count ++ log . Debug (). Int ( \"batch\" , count ). Int ( \"start\" , start ). Int ( \"end\" , end ). Msg ( \"path batch indexed\" ) } // Finally, we forward the commit to the previous trie update and // repeat until we have stepped all the way back to the last indexed // commit. commit = step . Commit } // At this point, we can delete any trie that does not correspond to // the state that we have just reached. This will allow the garbage // collector to free up any payload that has been changed and which is // no longer part of the state trie at the newly indexed finalized // block. for key := range steps { if key != commitNext { delete ( steps , key ) } } // Last but not least, we take care of properly indexing the height of // the first indexed block and the height of the last indexed block. once . Do ( func () { err = m . index . First ( height ) }) if err != nil { return fmt . Errorf ( \"could not index first height: %w\" , err ) } err = m . index . Last ( height ) if err != nil { return fmt . Errorf ( \"could not index last height: %w\" , err ) } // We have now successfully indexed all state trie changes and other // data at the current height. We set the last indexed step to the last // step from our current height, and then increase the height to start // the indexing of the next block. commitPrev = commitNext height ++ log . Info (). Hex ( \"block\" , blockID [:]). Int ( \"num_changes\" , len ( updated )). Int ( \"num_events\" , len ( events )). Msg ( \"block data indexed\" ) } m . log . Info (). Msg ( \"state indexing finished\" ) step := steps [ commitPrev ] m . cfg . PostProcessing ( step . Tree ) return nil } As it was, this code was untestable. Covering each possible case from this huge piece of logic would have required immense, complex, unreadable tests, that would break whenever a piece of this logic would change, and this would require a huge amount of maintenance effort. To solve that massive problem, we refactored our original mapper into a finite-state machine which replicates the same computation logic by applying transitions to a state. mapper_new.go package mapper import ( \"errors\" \"fmt\" \"sync\" \"time\" \"github.com/rs/zerolog\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" ) // TransitionFunc is a function that is applied onto the state machine's // state. type TransitionFunc func ( * State ) error // Transitions is what applies transitions to the state of an FSM. type Transitions struct { cfg Config log zerolog . Logger load Loader chain dps . Chain feed Feeder read dps . Reader write dps . Writer once * sync . Once } // NewTransitions returns a Transitions component using the given dependencies and using the given options func NewTransitions ( log zerolog . Logger , load Loader , chain dps . Chain , feed Feeder , read dps . Reader , write dps . Writer , options ... Option ) * Transitions { cfg := DefaultConfig for _ , option := range options { option ( & cfg ) } t := Transitions { log : log . With (). Str ( \"component\" , \"mapper_transitions\" ). Logger (), cfg : cfg , load : load , chain : chain , feed : feed , read : read , write : write , once : & sync . Once {}, } return & t } // InitializeMapper initializes the mapper by either going into bootstrapping or // into resuming, depending on the configuration. func ( t * Transitions ) InitializeMapper ( s * State ) error { if s . status != StatusInitialize { return fmt . Errorf ( \"invalid status for initializing mapper (%s)\" , s . status ) } if t . cfg . BootstrapState { s . status = StatusBootstrap return nil } s . status = StatusResume return nil } // BootstrapState bootstraps the state by loading the checkpoint if there is one // and initializing the elements subsequently used by the FSM. func ( t * Transitions ) BootstrapState ( s * State ) error { if s . status != StatusBootstrap { return fmt . Errorf ( \"invalid status for bootstrapping state (%s)\" , s . status ) } // We always need at least one step in our forest, which is used as the // stopping point when indexing the payloads since the last finalized // block. We thus introduce an empty tree, with no paths and an // irrelevant previous commit. empty := trie . NewEmptyMTrie () s . forest . Save ( empty , nil , flow . DummyStateCommitment ) // The chain indexing will forward last to next and next to current height, // which will be the one for the checkpoint. first := flow . StateCommitment ( empty . RootHash ()) s . last = flow . DummyStateCommitment s . next = first t . log . Info (). Hex ( \"commit\" , first [:]). Msg ( \"added empty tree to forest\" ) // Then, we can load the root height and apply it to the state. That // will allow us to load the root blockchain data in the next step. height , err := t . chain . Root () if err != nil { return fmt . Errorf ( \"could not get root height: %w\" , err ) } s . height = height // When bootstrapping, the loader injected into the mapper loads the root // checkpoint. tree , err := t . load . Trie () if err != nil { return fmt . Errorf ( \"could not load root trie: %w\" , err ) } paths := allPaths ( tree ) s . forest . Save ( tree , paths , first ) second := tree . RootHash () t . log . Info (). Uint64 ( \"height\" , s . height ). Hex ( \"commit\" , second [:]). Int ( \"registers\" , len ( paths )). Msg ( \"added checkpoint tree to forest\" ) // We have successfully bootstrapped. However, no chain data for the root // block has been indexed yet. This is why we \"pretend\" that we just // forwarded the state to this height, so we go straight to the chain data // indexing. s . status = StatusIndex return nil } // ResumeIndexing resumes indexing the data from a previous run. func ( t * Transitions ) ResumeIndexing ( s * State ) error { if s . status != StatusResume { return fmt . Errorf ( \"invalid status for resuming indexing (%s)\" , s . status ) } // When resuming, we want to avoid overwriting the `first` height in the // index with the height we are resuming from. Theoretically, all that would // be needed would be to execute a no-op on `once`, which would subsequently // be skipped in the height forwarding code. However, this bug was already // released, so we have databases where `first` was incorrectly set to the // height we resume from. In order to fix them, we explicitly write the // correct `first` height here again, while at the same time using `once` to // disable any subsequent attempts to write it. first , err := t . chain . Root () if err != nil { return fmt . Errorf ( \"could not get root height: %w\" , err ) } t . once . Do ( func () { err = t . write . First ( first ) }) if err != nil { return fmt . Errorf ( \"could not write first: %w\" , err ) } // We need to know what the last indexed height was at the point we stopped // indexing. last , err := t . read . Last () if err != nil { return fmt . Errorf ( \"could not get last height: %w\" , err ) } // When resuming, the loader injected into the mapper rebuilds the trie from // the paths and payloads stored in the index database. tree , err := t . load . Trie () if err != nil { return fmt . Errorf ( \"could not restore index trie: %w\" , err ) } // After loading the trie, we should do a sanity check on its hash against // the commit we indexed for it. hash := flow . StateCommitment ( tree . RootHash ()) commit , err := t . read . Commit ( last ) if err != nil { return fmt . Errorf ( \"could not get last commit: %w\" , err ) } if hash != commit { return fmt . Errorf ( \"restored trie hash does not match last commit (hash: %x, commit: %x)\" , hash , commit ) } // At this point, we can store the restored trie in our forest, as the trie // for the last finalized block. We do not need to care about the parent // state commitment or the paths, as they should not be used. s . last = flow . DummyStateCommitment s . next = commit s . forest . Save ( tree , nil , flow . DummyStateCommitment ) // Lastly, we just need to point to the next height. The chain indexing will // then proceed with the first non-indexed block and forward the state // commitments accordingly. s . height = last + 1 // At this point, we should be able to start indexing the chain data for // the next height. s . status = StatusIndex return nil } // IndexChain indexes chain data for the current height. func ( t * Transitions ) IndexChain ( s * State ) error { if s . status != StatusIndex { return fmt . Errorf ( \"invalid status for indexing chain (%s)\" , s . status ) } log := t . log . With (). Uint64 ( \"height\" , s . height ). Logger () // We try to retrieve the next header until it becomes available, which // means all data coming from the protocol state is available after this // point. header , err := t . chain . Header ( s . height ) if errors . Is ( err , dps . ErrUnavailable ) { log . Debug (). Msg ( \"waiting for next header\" ) time . Sleep ( t . cfg . WaitInterval ) return nil } if err != nil { return fmt . Errorf ( \"could not get header: %w\" , err ) } // At this point, we can retrieve the data from the consensus state. This is // a slight optimization for the live indexer, as it allows us to process // some data before the full execution data becomes available. guarantees , err := t . chain . Guarantees ( s . height ) if err != nil { return fmt . Errorf ( \"could not get guarantees: %w\" , err ) } seals , err := t . chain . Seals ( s . height ) if err != nil { return fmt . Errorf ( \"could not get seals: %w\" , err ) } // We can also proceed to already indexing the data related to the consensus // state, before dealing with anything related to execution data, which // might go into the wait state. blockID := header . ID () err = t . write . Height ( blockID , s . height ) if err != nil { return fmt . Errorf ( \"could not index height: %w\" , err ) } err = t . write . Header ( s . height , header ) if err != nil { return fmt . Errorf ( \"could not index header: %w\" , err ) } err = t . write . Guarantees ( s . height , guarantees ) if err != nil { return fmt . Errorf ( \"could not index guarantees: %w\" , err ) } err = t . write . Seals ( s . height , seals ) if err != nil { return fmt . Errorf ( \"could not index seals: %w\" , err ) } // Next, we try to retrieve the next commit until it becomes available, // at which point all the data coming from the execution data should be // available. commit , err := t . chain . Commit ( s . height ) if errors . Is ( err , dps . ErrUnavailable ) { log . Debug (). Msg ( \"waiting for next state commitment\" ) time . Sleep ( t . cfg . WaitInterval ) return nil } if err != nil { return fmt . Errorf ( \"could not get commit: %w\" , err ) } collections , err := t . chain . Collections ( s . height ) if err != nil { return fmt . Errorf ( \"could not get collections: %w\" , err ) } transactions , err := t . chain . Transactions ( s . height ) if err != nil { return fmt . Errorf ( \"could not get transactions: %w\" , err ) } results , err := t . chain . Results ( s . height ) if err != nil { return fmt . Errorf ( \"could not get transaction results: %w\" , err ) } events , err := t . chain . Events ( s . height ) if err != nil { return fmt . Errorf ( \"could not get events: %w\" , err ) } // Next, all we need to do is index the remaining data and we have fully // processed indexing for this block height. err = t . write . Commit ( s . height , commit ) if err != nil { return fmt . Errorf ( \"could not index commit: %w\" , err ) } err = t . write . Collections ( s . height , collections ) if err != nil { return fmt . Errorf ( \"could not index collections: %w\" , err ) } err = t . write . Transactions ( s . height , transactions ) if err != nil { return fmt . Errorf ( \"could not index transactions: %w\" , err ) } err = t . write . Results ( results ) if err != nil { return fmt . Errorf ( \"could not index transaction results: %w\" , err ) } err = t . write . Events ( s . height , events ) if err != nil { return fmt . Errorf ( \"could not index events: %w\" , err ) } // At this point, we need to forward the `last` state commitment to // `next`, so we know what the state commitment was at the last finalized // block we processed. This will allow us to know when to stop when // walking back through the forest to collect trie updates. s . last = s . next // Last but not least, we need to update `next` to point to the commit we // have just retrieved for the new block height. This is the sentinel that // tells us when we have collected enough trie updates for the forest to // have reached the next finalized block. s . next = commit log . Info (). Msg ( \"indexed blockchain data for finalized block\" ) // After indexing the blockchain data, we can go back to updating the state // tree until we find the commit of the finalized block. This will allow us // to index the payloads then. s . status = StatusUpdate return nil } // UpdateTree updates the state's tree. If the state's forest already matches with the next block's state commitment, // it immediately returns and sets the state's status to StatusMatched. func ( t * Transitions ) UpdateTree ( s * State ) error { if s . status != StatusUpdate { return fmt . Errorf ( \"invalid status for updating tree (%s)\" , s . status ) } log := t . log . With (). Uint64 ( \"height\" , s . height ). Hex ( \"last\" , s . last [:]). Hex ( \"next\" , s . next [:]). Logger () // If the forest contains a tree for the commit of the next finalized block, // we have reached our goal, and we can go to the next step in order to // collect the register payloads we want to index for that block. ok := s . forest . Has ( s . next ) if ok { log . Info (). Hex ( \"commit\" , s . next [:]). Msg ( \"matched commit of finalized block\" ) s . status = StatusCollect return nil } // First, we get the next tree update from the feeder. We can skip it if // it doesn't have any updated paths, or if we can't find the tree to apply // it to in the forest. This usually means that it was meant for a pruned // branch of the execution forest. update , err := t . feed . Update () if errors . Is ( err , dps . ErrUnavailable ) { time . Sleep ( t . cfg . WaitInterval ) log . Debug (). Msg ( \"waiting for next trie update\" ) return nil } if err != nil { return fmt . Errorf ( \"could not feed update: %w\" , err ) } parent := flow . StateCommitment ( update . RootHash ) tree , ok := s . forest . Tree ( parent ) if ! ok { log . Warn (). Msg ( \"state commitment mismatch, retrieving next trie update\" ) return nil } // We then apply the update to the relevant tree, as retrieved from the // forest, and save the updated tree in the forest. If the tree is not new, // we should error, as that should not happen. paths , payloads := pathsPayloads ( update ) tree , err = trie . NewTrieWithUpdatedRegisters ( tree , paths , payloads ) if err != nil { return fmt . Errorf ( \"could not update tree: %w\" , err ) } s . forest . Save ( tree , paths , parent ) hash := tree . RootHash () log . Info (). Hex ( \"commit\" , hash [:]). Int ( \"registers\" , len ( paths )). Msg ( \"updated tree with register payloads\" ) return nil } // CollectRegisters reads the payloads for the next block to be indexed from the state's forest, unless payload // indexing is disabled. func ( t * Transitions ) CollectRegisters ( s * State ) error { log := t . log . With (). Uint64 ( \"height\" , s . height ). Hex ( \"commit\" , s . next [:]). Logger () if s . status != StatusCollect { return fmt . Errorf ( \"invalid status for collecting registers (%s)\" , s . status ) } // If indexing payloads is disabled, we can bypass collection and indexing // of payloads and just go straight to forwarding the height to the next // finalized block. if t . cfg . SkipRegisters { s . status = StatusForward return nil } // If we index payloads, we are basically stepping back from (and including) // the tree that corresponds to the next finalized block all the way up to // (and excluding) the tree for the last finalized block we indexed. To do // so, we will use the parent state commit to retrieve the parent trees from // the forest, and we use the paths we recorded changes on to retrieve the // changed payloads at each step. commit := s . next for commit != s . last { // We do this check only once, so that we don't need to do it for // each item we retrieve. The tree should always be there, but we // should check just to not fail silently. ok := s . forest . Has ( commit ) if ! ok { return fmt . Errorf ( \"could not load tree (commit: %x)\" , commit ) } // For each path, we retrieve the payload and add it to the registers we // will index later. If we already have a payload for the path, it is // more recent as we iterate backwards in time, so we can skip the // outdated payload. // NOTE: We read from the tree one by one here, as the performance // overhead is minimal compared to the disk i/o for badger, and it // allows us to ignore sorting of paths. tree , _ := s . forest . Tree ( commit ) paths , _ := s . forest . Paths ( commit ) for _ , path := range paths { _ , ok := s . registers [ path ] if ok { continue } payloads := tree . UnsafeRead ([] ledger . Path { path }) s . registers [ path ] = payloads [ 0 ] } log . Debug (). Int ( \"batch\" , len ( paths )). Msg ( \"collected register batch for finalized block\" ) // We now step back to the parent of the current state trie. parent , _ := s . forest . Parent ( commit ) commit = parent } log . Info (). Int ( \"registers\" , len ( s . registers )). Msg ( \"collected all registers for finalized block\" ) // At this point, we have collected all the payloads, so we go to the next // step, where we will index them. s . status = StatusMap return nil } // MapRegisters maps the collected registers to the current block. func ( t * Transitions ) MapRegisters ( s * State ) error { if s . status != StatusMap { return fmt . Errorf ( \"invalid status for indexing registers (%s)\" , s . status ) } log := t . log . With (). Uint64 ( \"height\" , s . height ). Hex ( \"commit\" , s . next [:]). Logger () // If there are no registers left to be indexed, we can go to the next step, // which is about forwarding the height to the next finalized block. if len ( s . registers ) == 0 { log . Info (). Msg ( \"indexed all registers for finalized block\" ) s . status = StatusForward return nil } // We will now collect and index 1000 registers at a time. This gives the // FSM the chance to exit the loop between every 1000 payloads we index. It // doesn't really matter for badger if they are in random order, so this // way of iterating should be fine. n := 1000 paths := make ([] ledger . Path , 0 , n ) payloads := make ([] * ledger . Payload , 0 , n ) for path , payload := range s . registers { paths = append ( paths , path ) payloads = append ( payloads , payload ) delete ( s . registers , path ) if len ( paths ) >= n { break } } // Then we store the (maximum) 1000 paths and payloads. err := t . write . Payloads ( s . height , paths , payloads ) if err != nil { return fmt . Errorf ( \"could not index registers: %w\" , err ) } log . Debug (). Int ( \"batch\" , len ( paths )). Int ( \"remaining\" , len ( s . registers )). Msg ( \"indexed register batch for finalized block\" ) return nil } // ForwardHeight increments the height at which the mapping operates, and updates the last indexed height. func ( t * Transitions ) ForwardHeight ( s * State ) error { if s . status != StatusForward { return fmt . Errorf ( \"invalid status for forwarding height (%s)\" , s . status ) } // After finishing the indexing of the payloads for a finalized block, or // skipping it, we should document the last indexed height. On the first // pass, we will also index the first indexed height here. var err error t . once . Do ( func () { err = t . write . First ( s . height ) }) if err != nil { return fmt . Errorf ( \"could not index first height: %w\" , err ) } err = t . write . Last ( s . height ) if err != nil { return fmt . Errorf ( \"could not index last height: %w\" , err ) } // Now that we have indexed the heights, we can forward to the next height, // and reset the forest to free up memory. s . height ++ s . forest . Reset ( s . next ) t . log . Info (). Uint64 ( \"height\" , s . height ). Msg ( \"forwarded finalized block to next height\" ) // Once the height is forwarded, we can set the status so that we index // the blockchain data next. s . status = StatusIndex return nil } This refactoring effort allowed us to write simple and concise tests that call a transition function upon the state machine and make assertions upon the resulting state. mapper_new_internal_test.go package mapper import ( \"sync\" \"testing\" \"github.com/stretchr/testify/assert\" \"github.com/stretchr/testify/require\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" \"github.com/optakt/flow-dps/testing/mocks\" ) func TestNewTransitions ( t * testing . T ) { t . Run ( \"nominal case, without options\" , func ( t * testing . T ) { load := mocks . BaselineLoader ( t ) chain := mocks . BaselineChain ( t ) feed := mocks . BaselineFeeder ( t ) read := mocks . BaselineReader ( t ) write := mocks . BaselineWriter ( t ) tr := NewTransitions ( mocks . NoopLogger , load , chain , feed , read , write ) assert . NotNil ( t , tr ) assert . Equal ( t , chain , tr . chain ) assert . Equal ( t , feed , tr . feed ) assert . Equal ( t , write , tr . write ) assert . NotNil ( t , tr . once ) assert . Equal ( t , DefaultConfig , tr . cfg ) }) t . Run ( \"nominal case, with option\" , func ( t * testing . T ) { load := mocks . BaselineLoader ( t ) chain := mocks . BaselineChain ( t ) feed := mocks . BaselineFeeder ( t ) read := mocks . BaselineReader ( t ) write := mocks . BaselineWriter ( t ) skip := true tr := NewTransitions ( mocks . NoopLogger , load , chain , feed , read , write , WithSkipRegisters ( skip ), ) assert . NotNil ( t , tr ) assert . Equal ( t , chain , tr . chain ) assert . Equal ( t , feed , tr . feed ) assert . Equal ( t , write , tr . write ) assert . NotNil ( t , tr . once ) assert . NotEqual ( t , DefaultConfig , tr . cfg ) assert . Equal ( t , skip , tr . cfg . SkipRegisters ) assert . Equal ( t , DefaultConfig . WaitInterval , tr . cfg . WaitInterval ) }) } func TestTransitions_BootstrapState ( t * testing . T ) { t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) // Copy state in local scope so that we can override its SaveFunc without impacting other // tests running in parallel. var saveCalled bool forest := mocks . BaselineForest ( t , true ) forest . SaveFunc = func ( tree * trie . MTrie , paths [] ledger . Path , parent flow . StateCommitment ) { if ! saveCalled { assert . True ( t , tree . IsEmpty ()) assert . Nil ( t , paths ) assert . Zero ( t , parent ) saveCalled = true return } assert . False ( t , tree . IsEmpty ()) assert . Len ( t , tree . AllPayloads (), len ( paths )) assert . Len ( t , paths , 3 ) // Expect the three paths from leaves. assert . NotZero ( t , parent ) } err := tr . BootstrapState ( st ) assert . NoError ( t , err ) }) t . Run ( \"invalid state\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusForward ) err := tr . BootstrapState ( st ) assert . Error ( t , err ) }) t . Run ( \"handles failure to get root height\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return 0 , mocks . GenericError } tr . chain = chain err := tr . BootstrapState ( st ) assert . Error ( t , err ) }) } func TestTransitions_IndexChain ( t * testing . T ) { t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . HeaderFunc = func ( height uint64 ) ( * flow . Header , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericHeader , nil } chain . CommitFunc = func ( height uint64 ) ( flow . StateCommitment , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericCommit ( 0 ), nil } chain . CollectionsFunc = func ( height uint64 ) ([] * flow . LightCollection , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericCollections ( 2 ), nil } chain . GuaranteesFunc = func ( height uint64 ) ([] * flow . CollectionGuarantee , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericGuarantees ( 2 ), nil } chain . TransactionsFunc = func ( height uint64 ) ([] * flow . TransactionBody , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericTransactions ( 4 ), nil } chain . ResultsFunc = func ( height uint64 ) ([] * flow . TransactionResult , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericResults ( 4 ), nil } chain . EventsFunc = func ( height uint64 ) ([] flow . Event , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericEvents ( 8 ), nil } chain . SealsFunc = func ( height uint64 ) ([] * flow . Seal , error ) { assert . Equal ( t , mocks . GenericHeight , height ) return mocks . GenericSeals ( 4 ), nil } write := mocks . BaselineWriter ( t ) write . HeaderFunc = func ( height uint64 , header * flow . Header ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericHeader , header ) return nil } write . CommitFunc = func ( height uint64 , commit flow . StateCommitment ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericCommit ( 0 ), commit ) return nil } write . HeightFunc = func ( blockID flow . Identifier , height uint64 ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericHeader . ID (), blockID ) return nil } write . CollectionsFunc = func ( height uint64 , collections [] * flow . LightCollection ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericCollections ( 2 ), collections ) return nil } write . GuaranteesFunc = func ( height uint64 , guarantees [] * flow . CollectionGuarantee ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericGuarantees ( 2 ), guarantees ) return nil } write . TransactionsFunc = func ( height uint64 , transactions [] * flow . TransactionBody ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericTransactions ( 4 ), transactions ) return nil } write . ResultsFunc = func ( results [] * flow . TransactionResult ) error { assert . Equal ( t , mocks . GenericResults ( 4 ), results ) return nil } write . EventsFunc = func ( height uint64 , events [] flow . Event ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericEvents ( 8 ), events ) return nil } write . SealsFunc = func ( height uint64 , seals [] * flow . Seal ) error { assert . Equal ( t , mocks . GenericHeight , height ) assert . Equal ( t , mocks . GenericSeals ( 4 ), seals ) return nil } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain tr . write = write err := tr . IndexChain ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusUpdate , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve commit\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return flow . DummyStateCommitment , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index commit\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . CommitFunc = func ( uint64 , flow . StateCommitment ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve header\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . HeaderFunc = func ( uint64 ) ( * flow . Header , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index header\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . HeaderFunc = func ( uint64 , * flow . Header ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve transactions\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . TransactionsFunc = func ( uint64 ) ([] * flow . TransactionBody , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve transaction results\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . ResultsFunc = func ( uint64 ) ([] * flow . TransactionResult , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index transactions\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . ResultsFunc = func ([] * flow . TransactionResult ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve collections\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . CollectionsFunc = func ( uint64 ) ([] * flow . LightCollection , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index collections\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . CollectionsFunc = func ( uint64 , [] * flow . LightCollection ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve guarantees\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . GuaranteesFunc = func ( uint64 ) ([] * flow . CollectionGuarantee , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index guarantees\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . GuaranteesFunc = func ( uint64 , [] * flow . CollectionGuarantee ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve events\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . EventsFunc = func ( uint64 ) ([] flow . Event , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index events\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . EventsFunc = func ( uint64 , [] flow . Event ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles chain failure to retrieve seals\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . SealsFunc = func ( uint64 ) ([] * flow . Seal , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . chain = chain err := tr . IndexChain ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure to index seals\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . SealsFunc = func ( uint64 , [] * flow . Seal ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusIndex ) tr . write = write err := tr . IndexChain ( st ) assert . Error ( t , err ) }) } func TestTransitions_UpdateTree ( t * testing . T ) { update := mocks . GenericTrieUpdate ( 0 ) tree := mocks . GenericTrie t . Run ( \"nominal case without match\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusUpdate ) forest := mocks . BaselineForest ( t , false ) forest . SaveFunc = func ( tree * trie . MTrie , paths [] ledger . Path , parent flow . StateCommitment ) { // Parent is RootHash of the mocks.GenericTrie. assert . Equal ( t , update . RootHash [:], parent [:]) assert . ElementsMatch ( t , paths , update . Paths ) assert . NotZero ( t , tree ) } forest . TreeFunc = func ( commit flow . StateCommitment ) ( * trie . MTrie , bool ) { assert . Equal ( t , update . RootHash [:], commit [:]) return tree , true } st . forest = forest err := tr . UpdateTree ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusUpdate , st . status ) }) t . Run ( \"nominal case with no available update temporarily\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusUpdate ) // Set up the mock feeder to return an unavailable error on the first call and return successfully // to subsequent calls. var updateCalled bool feeder := mocks . BaselineFeeder ( t ) feeder . UpdateFunc = func () ( * ledger . TrieUpdate , error ) { if ! updateCalled { updateCalled = true return nil , dps . ErrUnavailable } return mocks . GenericTrieUpdate ( 0 ), nil } tr . feed = feeder forest := mocks . BaselineForest ( t , true ) forest . HasFunc = func ( flow . StateCommitment ) bool { return updateCalled } st . forest = forest // The first call should not error but should not change the status of the FSM to updating. It should // instead remain Updating until a match is found. err := tr . UpdateTree ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusUpdate , st . status ) // The second call is now successful and matches. err = tr . UpdateTree ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusCollect , st . status ) }) t . Run ( \"nominal case with match\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusUpdate ) err := tr . UpdateTree ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusCollect , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) err := tr . UpdateTree ( st ) assert . Error ( t , err ) }) t . Run ( \"handles feeder update failure\" , func ( t * testing . T ) { t . Parallel () feed := mocks . BaselineFeeder ( t ) feed . UpdateFunc = func () ( * ledger . TrieUpdate , error ) { return nil , mocks . GenericError } tr , st := baselineFSM ( t , StatusUpdate ) st . forest = mocks . BaselineForest ( t , false ) tr . feed = feed err := tr . UpdateTree ( st ) assert . Error ( t , err ) }) t . Run ( \"handles forest parent tree not found\" , func ( t * testing . T ) { t . Parallel () forest := mocks . BaselineForest ( t , false ) forest . TreeFunc = func ( _ flow . StateCommitment ) ( * trie . MTrie , bool ) { return nil , false } tr , st := baselineFSM ( t , StatusUpdate ) st . forest = forest err := tr . UpdateTree ( st ) assert . NoError ( t , err ) }) } func TestTransitions_CollectRegisters ( t * testing . T ) { t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () forest := mocks . BaselineForest ( t , true ) forest . ParentFunc = func ( commit flow . StateCommitment ) ( flow . StateCommitment , bool ) { assert . Equal ( t , mocks . GenericCommit ( 0 ), commit ) return mocks . GenericCommit ( 1 ), true } tr , st := baselineFSM ( t , StatusCollect ) st . forest = forest err := tr . CollectRegisters ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusMap , st . status ) for _ , wantPath := range mocks . GenericLedgerPaths ( 6 ) { assert . Contains ( t , st . registers , wantPath ) } }) t . Run ( \"indexing payloads disabled\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusCollect ) tr . cfg . SkipRegisters = true err := tr . CollectRegisters ( st ) require . NoError ( t , err ) assert . Empty ( t , st . registers ) assert . Equal ( t , StatusForward , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) err := tr . CollectRegisters ( st ) assert . Error ( t , err ) assert . Empty ( t , st . registers ) }) t . Run ( \"handles missing tree for commit\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusCollect ) st . forest = mocks . BaselineForest ( t , false ) err := tr . CollectRegisters ( st ) assert . Error ( t , err ) assert . Empty ( t , st . registers ) }) } func TestTransitions_MapRegisters ( t * testing . T ) { t . Run ( \"nominal case with registers to write\" , func ( t * testing . T ) { t . Parallel () // Path 2 and 4 are the same so the map effectively contains 5 entries. testRegisters := map [ ledger . Path ] * ledger . Payload { mocks . GenericLedgerPath ( 0 ): mocks . GenericLedgerPayload ( 0 ), mocks . GenericLedgerPath ( 1 ): mocks . GenericLedgerPayload ( 1 ), mocks . GenericLedgerPath ( 2 ): mocks . GenericLedgerPayload ( 2 ), mocks . GenericLedgerPath ( 1 ): mocks . GenericLedgerPayload ( 3 ), mocks . GenericLedgerPath ( 4 ): mocks . GenericLedgerPayload ( 4 ), mocks . GenericLedgerPath ( 5 ): mocks . GenericLedgerPayload ( 5 ), } write := mocks . BaselineWriter ( t ) write . PayloadsFunc = func ( height uint64 , paths [] ledger . Path , value [] * ledger . Payload ) error { assert . Equal ( t , mocks . GenericHeight , height ) // Expect the 5 entries from the map. assert . Len ( t , paths , 5 ) assert . Len ( t , value , 5 ) return nil } tr , st := baselineFSM ( t , StatusMap ) tr . write = write st . registers = testRegisters err := tr . MapRegisters ( st ) require . NoError ( t , err ) // Should not be StateIndexed because registers map was not empty. assert . Empty ( t , st . registers ) assert . Equal ( t , StatusMap , st . status ) }) t . Run ( \"nominal case no more registers left to write\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusMap ) err := tr . MapRegisters ( st ) assert . NoError ( t , err ) assert . Equal ( t , StatusForward , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () testRegisters := map [ ledger . Path ] * ledger . Payload { mocks . GenericLedgerPath ( 0 ): mocks . GenericLedgerPayload ( 0 ), mocks . GenericLedgerPath ( 1 ): mocks . GenericLedgerPayload ( 1 ), mocks . GenericLedgerPath ( 2 ): mocks . GenericLedgerPayload ( 2 ), mocks . GenericLedgerPath ( 3 ): mocks . GenericLedgerPayload ( 3 ), mocks . GenericLedgerPath ( 4 ): mocks . GenericLedgerPayload ( 4 ), mocks . GenericLedgerPath ( 5 ): mocks . GenericLedgerPayload ( 5 ), } tr , st := baselineFSM ( t , StatusBootstrap ) st . registers = testRegisters err := tr . MapRegisters ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure\" , func ( t * testing . T ) { t . Parallel () testRegisters := map [ ledger . Path ] * ledger . Payload { mocks . GenericLedgerPath ( 0 ): mocks . GenericLedgerPayload ( 0 ), mocks . GenericLedgerPath ( 1 ): mocks . GenericLedgerPayload ( 1 ), mocks . GenericLedgerPath ( 2 ): mocks . GenericLedgerPayload ( 2 ), mocks . GenericLedgerPath ( 3 ): mocks . GenericLedgerPayload ( 3 ), mocks . GenericLedgerPath ( 4 ): mocks . GenericLedgerPayload ( 4 ), mocks . GenericLedgerPath ( 5 ): mocks . GenericLedgerPayload ( 5 ), } write := mocks . BaselineWriter ( t ) write . PayloadsFunc = func ( uint64 , [] ledger . Path , [] * ledger . Payload ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusMap ) tr . write = write st . registers = testRegisters err := tr . MapRegisters ( st ) assert . Error ( t , err ) }) } func TestTransitions_ForwardHeight ( t * testing . T ) { t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () var ( firstCalled int lastCalled int ) write := mocks . BaselineWriter ( t ) write . FirstFunc = func ( height uint64 ) error { assert . Equal ( t , mocks . GenericHeight , height ) firstCalled ++ return nil } write . LastFunc = func ( height uint64 ) error { assert . Equal ( t , mocks . GenericHeight + uint64 ( lastCalled ), height ) lastCalled ++ return nil } forest := mocks . BaselineForest ( t , true ) forest . ResetFunc = func ( finalized flow . StateCommitment ) { assert . Equal ( t , mocks . GenericCommit ( 0 ), finalized ) } tr , st := baselineFSM ( t , StatusForward ) st . forest = forest tr . write = write err := tr . ForwardHeight ( st ) assert . NoError ( t , err ) assert . Equal ( t , StatusIndex , st . status ) assert . Equal ( t , mocks . GenericHeight + 1 , st . height ) // Reset status to allow next call. st . status = StatusForward err = tr . ForwardHeight ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusIndex , st . status ) assert . Equal ( t , mocks . GenericHeight + 2 , st . height ) // First should have been called only once. assert . Equal ( t , 1 , firstCalled ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusBootstrap ) err := tr . ForwardHeight ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer error on first\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . FirstFunc = func ( uint64 ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusForward ) tr . write = write err := tr . ForwardHeight ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer error on last\" , func ( t * testing . T ) { t . Parallel () write := mocks . BaselineWriter ( t ) write . LastFunc = func ( uint64 ) error { return mocks . GenericError } tr , st := baselineFSM ( t , StatusForward ) tr . write = write err := tr . ForwardHeight ( st ) assert . Error ( t , err ) }) } func TestTransitions_InitializeMapper ( t * testing . T ) { t . Run ( \"switches state to BootstrapState if configured to do so\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusInitialize ) tr . cfg . BootstrapState = true err := tr . InitializeMapper ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusBootstrap , st . status ) }) t . Run ( \"switches state to StatusResume if no bootstrapping configured\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusInitialize ) tr . cfg . BootstrapState = false err := tr . InitializeMapper ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusResume , st . status ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () tr , st := baselineFSM ( t , StatusForward ) err := tr . InitializeMapper ( st ) require . Error ( t , err ) }) } func TestTransitions_ResumeIndexing ( t * testing . T ) { header := mocks . GenericHeader tree := mocks . GenericTrie commit := flow . StateCommitment ( tree . RootHash ()) differentCommit := mocks . GenericCommit ( 0 ) t . Run ( \"nominal case\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } writer := mocks . BaselineWriter ( t ) writer . FirstFunc = func ( height uint64 ) error { assert . Equal ( t , header . Height , height ) return nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( height uint64 ) ( flow . StateCommitment , error ) { assert . Equal ( t , header . Height , height ) return commit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withWriter ( writer ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) require . NoError ( t , err ) assert . Equal ( t , StatusIndex , st . status ) assert . Equal ( t , header . Height + 1 , st . height ) assert . Equal ( t , flow . DummyStateCommitment , st . last ) assert . Equal ( t , commit , st . next ) }) t . Run ( \"handles chain failure on Root\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return 0 , mocks . GenericError } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles writer failure on First\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } writer := mocks . BaselineWriter ( t ) writer . FirstFunc = func ( uint64 ) error { return mocks . GenericError } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusResume , withWriter ( writer ), withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles reader failure on Last\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return 0 , mocks . GenericError } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles reader failure on Commit\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return flow . DummyStateCommitment , mocks . GenericError } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles loader failure on Trie\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return nil , mocks . GenericError } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles mismatch between tree root hash and indexed commit\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return differentCommit , nil } tr , st := baselineFSM ( t , StatusResume , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) t . Run ( \"handles invalid status\" , func ( t * testing . T ) { t . Parallel () chain := mocks . BaselineChain ( t ) chain . RootFunc = func () ( uint64 , error ) { return header . Height , nil } loader := mocks . BaselineLoader ( t ) loader . TrieFunc = func () ( * trie . MTrie , error ) { return tree , nil } reader := mocks . BaselineReader ( t ) reader . LastFunc = func () ( uint64 , error ) { return header . Height , nil } reader . CommitFunc = func ( uint64 ) ( flow . StateCommitment , error ) { return commit , nil } tr , st := baselineFSM ( t , StatusForward , withReader ( reader ), withLoader ( loader ), withChain ( chain ), ) err := tr . ResumeIndexing ( st ) assert . Error ( t , err ) }) } func baselineFSM ( t * testing . T , status Status , opts ... func ( tr * Transitions )) ( * Transitions , * State ) { t . Helper () load := mocks . BaselineLoader ( t ) chain := mocks . BaselineChain ( t ) feeder := mocks . BaselineFeeder ( t ) read := mocks . BaselineReader ( t ) write := mocks . BaselineWriter ( t ) forest := mocks . BaselineForest ( t , true ) once := & sync . Once {} doneCh := make ( chan struct {}) tr := Transitions { cfg : Config { BootstrapState : false , SkipRegisters : false , WaitInterval : 0 , }, log : mocks . NoopLogger , load : load , chain : chain , feed : feeder , read : read , write : write , once : once , } for _ , opt := range opts { opt ( & tr ) } st := State { forest : forest , status : status , height : mocks . GenericHeight , last : mocks . GenericCommit ( 1 ), next : mocks . GenericCommit ( 0 ), registers : make ( map [ ledger . Path ] * ledger . Payload ), done : doneCh , } return & tr , & st } func withLoader ( load Loader ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . load = load } } func withChain ( chain dps . Chain ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . chain = chain } } func withFeeder ( feed Feeder ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . feed = feed } } func withReader ( read dps . Reader ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . read = read } } func withWriter ( write dps . Writer ) func ( * Transitions ) { return func ( tr * Transitions ) { tr . write = write } }","title":"Case Study: The Flow DPS Mapper"},{"location":"testing/#integration-tests","text":"Integration tests are essential to ensure that components work together as expected. Those tests are usually much heavier and slower than unit tests, since they use real components instead of simple mocks, and often might run filesystem or network operations, wait for things to happen, or even run heavy computational tasks. Integration tests should always be specified in a separate test package and never run internally within the tested package.","title":"Integration Tests"},{"location":"testing/#build-tag","text":"Because integration tests are inherently slower than unit tests, they are placed in specific files that are suffixed with _integration_test.go and those files start with a build tag directive which prevents them from running unless the go test command is called with the integration tag. Both syntaxes should be specified, the <go1.17 one which is +build <tag> as well as the >=go1.17 one which is go:build <tag> . The former will be dropped when we feel like it is no longer relevant to support go 1.16 and prior. //go:build integration // +build integration package dps_test","title":"Build Tag"},{"location":"testing/#examples","text":"In Go, good package documentation includes not only comments for each public type and method, but also runnable examples and benchmarks in some cases. Godoc allows defining examples which are verified by running them as tests and can be manually launched by readers of the documentation on the package's Godoc webpage. As for typical tests, examples are functions that reside in a package's _test.go files. Unlike normal test functions, though, example functions take no arguments and begin with the word Example instead of Test . In order to specify what is the expected output of a given example, a comment has to be written at the end of the Example function, in the form of // Output: <expected output> . If this is missing, examples will not be executed and therefore not included in the documentation.","title":"Examples"},{"location":"testing/#benchmarks","text":"When a package exposes a performance-critical piece of code, it should be benchmarked, and benchmark tests must be available for anyone to reproduce the benchmark using their hardware. Writing benchmark results in a markdown file without providing a way to reproduce them is irrelevant.","title":"Benchmarks"},{"location":"knowledge_base/glossary/","text":"Glossary General Blockchain Concepts Blockchain Blockchain is a technology for implementing a stateful immutable distributed and decentralized ledger of records. Participants that update the ledger records represent the peers that run the blockchain network. Peers participate in the blockchain network collaborating with their resources in the same manner as in other types of distributed network architectures. Records in the ledger, called transactions, update the ledger state and represent any type of social exchangable value to the participants of the network - token representing ownership, money, access to some resource or event, etc. Transactions in the blockchain represent transfering of value from one party to the other. The foundamental difference the blockchain provide to the participants is that their values are exchanged without the participation of any third parties. For cryptocurrencies this idea drastically changes the concept of money transfers. Ledger records - transactions, are created and recorded in the ledger using specific steps and types of encryption on different parts of them - hashing, asymmetric and symmetric encryption. Transactions can be grouped in blocks. Blocks are created by the peers based on transaction updates they know about and then distributed in the network. Depending on the consensus protocol that is run in the blockchain network, blocks are eventually accepted and inserted in to the ledger. Each block of transactions follows the previous block inserted and reference it by its ID. This creates an immutable chain of blocks of transactions that represent the state of the ledger. Depending on the peers ability to participate in the blockchain network, blockchains are public and private . In public blockchains everyone is allowed to participate in the network. Peers can be of different types depending on what amount of data from the ledger they keep and how they update it, but in terms of functionality and resources they provide to the network - they are equally placed. This means that none of the peers, or a group of them, do not provide any specific functionality to the network that others do not and consequently, when peers leave or arrive in the blockchain network - its runing is not affected. Public blockchains do not require trust in other participants in the network. This is one of the core concepts behind the idea of distribution in the blockchain world. Private blockchains are a specific type of blockchains that have some type of authorization scheme used for identities that can enter the network and access its records. Private blockchains require trust in other participants of the network and some of the peers inside them have different levels of access compared to others. In this kind of blockchain networks, a random peer or a group of peers leaving (mailfunctioning) or arriving in the network affects affect the whole network stability. Depending on the access to participate in the network - being granted or not - blockchains are permissioned and permissionless Generally this difference lie in providing an access from some authority running the network, which concerns again the trust between participants. Permissioned blockchains are networks that require permission to enter it, usually used by organizations for managing their internal processes and data. Permissioned blockchains shifts from the core feature of decentralization in the blockchain world and from the initial idea of blockchains in general. A permissioned blockchain can also be a public network that only allows participation based on different access levels. The participants are usually known by a permissioned blockchain network operator, while the transaction history is not publicly accessible. A permissionless blockchain network does not require permissions to arrive as a peer in it, but can be public or private depending on the level or data access for the participants (and in the consensus trust levels between participants). In both permissionless and permissioned blockchains peers and groups of peers can have different roles and their presense or absence can affect the network functioning. Initial idea of blockchain networks, as described in Nakamoto consensus description , aims to reach the highest possible levels of transparency, decentralization, anonymity and immutability at the same time. During time, different types of blockchains emerged and today, they advance in some of the characteristics but lack in others (for example anonymity). Core concepts of blockchain according to the initial Nakamoto Consensus descriptions are: Transparency. Every full node in the network has a copy of the whole chain of blocks. This means that every transaction is available to each member, making the transactions traceable (not available in private and/or permissioned blockchains today). Immutability. Transactions included in the ledger become immutable records (in the general case). Immutability is present for all types of blockchains today. Decentralization. This is one of the biggest advantages of the blockchain, being a decentralized system allows for the lack of a central authority to control the transactions. Every full node has a copy of the chain, which they can update with new information, and every SPV node can update their records requesting them from a full node. Decentralization at its fullnest is not present in permissioned and private blockchains today as described above. Once a new block is created and inserted on the chain, the new block will have a link to the previous block, creating a chain. They all include the hash of the previous block except for the first one, which is called the genesis block and has a zero hash value. Consensus TODO: add Gossip Algorithms TODO: add Liveness Threshold The number of malicious participants that can be tolerated before the consensus protocol stops functioning. Nakamoto Type of Consensus TODO: introduce (for BTC) A consensus algorithm that provides a probabilistic safety guarantee; its decisions may revert with some probability \u03b5 . A permissionless protocol allowing each node to join or leave the system at any time. TODO: finish Safety TODO: add Transaction Transactions are data structures that encode the transfer of value between participants in the blockchain system. A transaction consumes previously recorded unspent transaction outputs and creates new transaction output(s) that can be consumed by future transactions. This is the way in which chunks of value move forward from one owner to the next one in the blockchain network. The two main parts of the transaction that represent the chain between available value and spent value are outputs and inputs. Transaction input Transaction input is a part of the transaction structure and it identifies (often by reference) which UTXO will be consumed. It provides a proof of ownership using an unblocking script. First part of the input is usually a pointer/reference to the UTXO, and the second part is the unlocking script (usually contructed by the wallet) that satisfies the spending condition set in the UTXO(s) that the transaction is going to consume. Input data structure can have arbitrary fields inside it depending on the blockchain network implementation, but they generally are: txid - referencing the transaction id that contains the UTXO index referencing which UTXO from that transaction is going to be spent scriptSig - a script signature that satisfies the spending conditions; known as \"unlocking script\" a sequence number An input may or may not reference any nominal value or other context depending on the organization. What it must contain is the unlocking script (witness part) that unclocks the UTXO. Transaction Output The transaction output is the fundamental building block of a blockchain transaction. Outputs are indivisible chunks of value denominated in the blockchain network exchanged value/currency, recorded on the blockchain and recognized as valid by the entire network. An output can contain an arbitrary value, but once created it is indivisible \u2014 it can be consumed only in its entirety by the transactions. Outputs are discrete and indivisible units of value. Transaction outputs consist generally of two parts: amount of the UTXO denominated value a cryptographic script or a public key that determines the conditions on which the UTXO can be spent; also known as the \"locking script\". The locking script is a hash representaion of the a public key hash (in the simplest situation) or it is a hash of an unlocking script. In both cases - the public key along with the signature or the unlocking script need to be presented at the time of consuming the output in order the values to be transfered. UTXO Set This is the set of all unspent transaction outputs that are available to the whole network. The UTXO set grows when a new transaction output is created and shrinks when UTXO is consumed. At any moment of time the set represents the UTXO that are available to be spent by the network participants. Avalanche Concepts Ancestor Set TODO: add Chit A transaction receives a chit (boolean value) when the majority (quorum) of queried nodes respond with an approval for that transaction. Confidence The sum of transaction/vertex's descendants chits and its own chit. The chit value is a result of a one-time query for its associated transaction and becomes immutable afterwards. Because chit values can be 0 or 1 \u2014 c \u2208 {0,1} confidence values are monotonic. Consecutive Success Number The number of times a transaction or its decendant received an approval from majority (quorum) of queried nodes. TODO: add more thorough description Progeny TODO: add Slush TODO: add Flow Concepts Quorum Certificate A process by which nodes using the HotStuff consensus algorithm submit signed messages in order to generate a certificate for bootstrapping HotStuff. Each collector cluster runs a mini-version of HotStuff, and since clusters are randomized each epoch, a new quorum ceritificate is required for each cluster each epoch. Other Graph TODO: introduce the idea in one sentence without technicals A graph is pair of sets G = (V, E). V is a set of elements called vertices (singular: vertex) and E is a set of paired vertices and its elements are called edges. An edge {x, y} contains vertices x and y and they are its endpoints. A vertex may belong to no edge and in that case it is not joined to any other vertex. The set of vertices in a graph is a discrete value, meaning the set of edges is also a discrete value. The number of vertices - |V| is the \"order\" of the graph and the number of edges - |E| is the \"size\" of the graph. Directed Graph A directed graph comprises of set of vertices connected by \"directed\" edges \u2014 also called arcs. A directed graph is represented as a pair of sets G = (V, A) , where: V is a set of elements (vertices, nodes) A is a set of ordered pairs of vertices, called arcs Directed Acyclic Graph (DAG) A Directed Acyclic Graph contains no directed cycles \u2014 each edge is directed from one vertex to another such that following those directions never forms a closed loop. A directed graph is a DAG if and only if it can be topologically ordered by arranging the vertices as a linear ordering that is consistent with all edge directions. A topological ordering of a directed graph is an ordering of its vertices into a sequence, such that for every edge the start vertex of the edge occurs earlier in the sequence than the ending vertex of the edge. A graph that has a topological ordering does not have any cycles, consequently every graph with a topological ordering is acyclic. Every acyclic directed graph has at least one topological ordering. Distributed System A system of computers connected by a network \u2014 local, hub-based or wide, that work together as one single computer. Applications built with distributed systems appear to users and other client applications as if they are running on a single machine. Distributed systems take advantage of each node's physical and virtual resources (horizontal scalability), minimize overload of the whole system and can be designed to achieve a certain performance level, speed or other characteristic levels needed for the application running to overcome. Partition of a Set A set's partition is a group of its elements into non-empty subsets, in such a way that every element is included in exactly one subset. Transitive Relation For a set T with elements a , b , c , d A{a, b, c, d} A relation R on the elements of the set that relates a to b , b to c and c to d , but also relates a to c , a to d and b to d is called transitive. a -> b, b -> c, c -> d AND a -> c, a -> d, b -> d Example of a Transitive Relation R A -> ancestor of B B -> ancestor of C => A -> ancestor of C A \u2500\u2500> B \u2500\u2500> C => A \u2500\u2500> C Example of a Non-Transitive Relation R A -> ancestor of B B -> ancestor of C A -> ancestor of D => D is NOT an ancestor of C A \u251c\u2500\u2500> B \u2500\u2500> C \u2514\u2500\u2500> D The inverse of a transtive relation is always transitive. See also Homogeneous Relation . Transitive Closure of a Graph A transitive closure of a directed graph G is a directed graph G' with an edge (i, j) corresponding to each directed path from i to j in G . The resultant directed graph G' representation in the form of the adjacency matrix is called the connectivity matrix .","title":"Glossary"},{"location":"knowledge_base/glossary/#glossary","text":"","title":"Glossary"},{"location":"knowledge_base/glossary/#general-blockchain-concepts","text":"","title":"General Blockchain Concepts"},{"location":"knowledge_base/glossary/#blockchain","text":"Blockchain is a technology for implementing a stateful immutable distributed and decentralized ledger of records. Participants that update the ledger records represent the peers that run the blockchain network. Peers participate in the blockchain network collaborating with their resources in the same manner as in other types of distributed network architectures. Records in the ledger, called transactions, update the ledger state and represent any type of social exchangable value to the participants of the network - token representing ownership, money, access to some resource or event, etc. Transactions in the blockchain represent transfering of value from one party to the other. The foundamental difference the blockchain provide to the participants is that their values are exchanged without the participation of any third parties. For cryptocurrencies this idea drastically changes the concept of money transfers. Ledger records - transactions, are created and recorded in the ledger using specific steps and types of encryption on different parts of them - hashing, asymmetric and symmetric encryption. Transactions can be grouped in blocks. Blocks are created by the peers based on transaction updates they know about and then distributed in the network. Depending on the consensus protocol that is run in the blockchain network, blocks are eventually accepted and inserted in to the ledger. Each block of transactions follows the previous block inserted and reference it by its ID. This creates an immutable chain of blocks of transactions that represent the state of the ledger. Depending on the peers ability to participate in the blockchain network, blockchains are public and private . In public blockchains everyone is allowed to participate in the network. Peers can be of different types depending on what amount of data from the ledger they keep and how they update it, but in terms of functionality and resources they provide to the network - they are equally placed. This means that none of the peers, or a group of them, do not provide any specific functionality to the network that others do not and consequently, when peers leave or arrive in the blockchain network - its runing is not affected. Public blockchains do not require trust in other participants in the network. This is one of the core concepts behind the idea of distribution in the blockchain world. Private blockchains are a specific type of blockchains that have some type of authorization scheme used for identities that can enter the network and access its records. Private blockchains require trust in other participants of the network and some of the peers inside them have different levels of access compared to others. In this kind of blockchain networks, a random peer or a group of peers leaving (mailfunctioning) or arriving in the network affects affect the whole network stability. Depending on the access to participate in the network - being granted or not - blockchains are permissioned and permissionless Generally this difference lie in providing an access from some authority running the network, which concerns again the trust between participants. Permissioned blockchains are networks that require permission to enter it, usually used by organizations for managing their internal processes and data. Permissioned blockchains shifts from the core feature of decentralization in the blockchain world and from the initial idea of blockchains in general. A permissioned blockchain can also be a public network that only allows participation based on different access levels. The participants are usually known by a permissioned blockchain network operator, while the transaction history is not publicly accessible. A permissionless blockchain network does not require permissions to arrive as a peer in it, but can be public or private depending on the level or data access for the participants (and in the consensus trust levels between participants). In both permissionless and permissioned blockchains peers and groups of peers can have different roles and their presense or absence can affect the network functioning. Initial idea of blockchain networks, as described in Nakamoto consensus description , aims to reach the highest possible levels of transparency, decentralization, anonymity and immutability at the same time. During time, different types of blockchains emerged and today, they advance in some of the characteristics but lack in others (for example anonymity). Core concepts of blockchain according to the initial Nakamoto Consensus descriptions are: Transparency. Every full node in the network has a copy of the whole chain of blocks. This means that every transaction is available to each member, making the transactions traceable (not available in private and/or permissioned blockchains today). Immutability. Transactions included in the ledger become immutable records (in the general case). Immutability is present for all types of blockchains today. Decentralization. This is one of the biggest advantages of the blockchain, being a decentralized system allows for the lack of a central authority to control the transactions. Every full node has a copy of the chain, which they can update with new information, and every SPV node can update their records requesting them from a full node. Decentralization at its fullnest is not present in permissioned and private blockchains today as described above. Once a new block is created and inserted on the chain, the new block will have a link to the previous block, creating a chain. They all include the hash of the previous block except for the first one, which is called the genesis block and has a zero hash value.","title":"Blockchain"},{"location":"knowledge_base/glossary/#consensus","text":"TODO: add","title":"Consensus"},{"location":"knowledge_base/glossary/#gossip-algorithms","text":"TODO: add","title":"Gossip Algorithms"},{"location":"knowledge_base/glossary/#liveness-threshold","text":"The number of malicious participants that can be tolerated before the consensus protocol stops functioning.","title":"Liveness Threshold"},{"location":"knowledge_base/glossary/#nakamoto-type-of-consensus","text":"TODO: introduce (for BTC) A consensus algorithm that provides a probabilistic safety guarantee; its decisions may revert with some probability \u03b5 . A permissionless protocol allowing each node to join or leave the system at any time. TODO: finish","title":"Nakamoto Type of Consensus"},{"location":"knowledge_base/glossary/#safety","text":"TODO: add","title":"Safety"},{"location":"knowledge_base/glossary/#transaction","text":"Transactions are data structures that encode the transfer of value between participants in the blockchain system. A transaction consumes previously recorded unspent transaction outputs and creates new transaction output(s) that can be consumed by future transactions. This is the way in which chunks of value move forward from one owner to the next one in the blockchain network. The two main parts of the transaction that represent the chain between available value and spent value are outputs and inputs.","title":"Transaction"},{"location":"knowledge_base/glossary/#transaction-input","text":"Transaction input is a part of the transaction structure and it identifies (often by reference) which UTXO will be consumed. It provides a proof of ownership using an unblocking script. First part of the input is usually a pointer/reference to the UTXO, and the second part is the unlocking script (usually contructed by the wallet) that satisfies the spending condition set in the UTXO(s) that the transaction is going to consume. Input data structure can have arbitrary fields inside it depending on the blockchain network implementation, but they generally are: txid - referencing the transaction id that contains the UTXO index referencing which UTXO from that transaction is going to be spent scriptSig - a script signature that satisfies the spending conditions; known as \"unlocking script\" a sequence number An input may or may not reference any nominal value or other context depending on the organization. What it must contain is the unlocking script (witness part) that unclocks the UTXO.","title":"Transaction input"},{"location":"knowledge_base/glossary/#transaction-output","text":"The transaction output is the fundamental building block of a blockchain transaction. Outputs are indivisible chunks of value denominated in the blockchain network exchanged value/currency, recorded on the blockchain and recognized as valid by the entire network. An output can contain an arbitrary value, but once created it is indivisible \u2014 it can be consumed only in its entirety by the transactions. Outputs are discrete and indivisible units of value. Transaction outputs consist generally of two parts: amount of the UTXO denominated value a cryptographic script or a public key that determines the conditions on which the UTXO can be spent; also known as the \"locking script\". The locking script is a hash representaion of the a public key hash (in the simplest situation) or it is a hash of an unlocking script. In both cases - the public key along with the signature or the unlocking script need to be presented at the time of consuming the output in order the values to be transfered.","title":"Transaction Output"},{"location":"knowledge_base/glossary/#utxo-set","text":"This is the set of all unspent transaction outputs that are available to the whole network. The UTXO set grows when a new transaction output is created and shrinks when UTXO is consumed. At any moment of time the set represents the UTXO that are available to be spent by the network participants.","title":"UTXO Set"},{"location":"knowledge_base/glossary/#avalanche-concepts","text":"","title":"Avalanche Concepts"},{"location":"knowledge_base/glossary/#ancestor-set","text":"TODO: add","title":"Ancestor Set"},{"location":"knowledge_base/glossary/#chit","text":"A transaction receives a chit (boolean value) when the majority (quorum) of queried nodes respond with an approval for that transaction.","title":"Chit"},{"location":"knowledge_base/glossary/#confidence","text":"The sum of transaction/vertex's descendants chits and its own chit. The chit value is a result of a one-time query for its associated transaction and becomes immutable afterwards. Because chit values can be 0 or 1 \u2014 c \u2208 {0,1} confidence values are monotonic.","title":"Confidence"},{"location":"knowledge_base/glossary/#consecutive-success-number","text":"The number of times a transaction or its decendant received an approval from majority (quorum) of queried nodes. TODO: add more thorough description","title":"Consecutive Success Number"},{"location":"knowledge_base/glossary/#progeny","text":"TODO: add","title":"Progeny"},{"location":"knowledge_base/glossary/#slush","text":"TODO: add","title":"Slush"},{"location":"knowledge_base/glossary/#flow-concepts","text":"","title":"Flow Concepts"},{"location":"knowledge_base/glossary/#quorum-certificate","text":"A process by which nodes using the HotStuff consensus algorithm submit signed messages in order to generate a certificate for bootstrapping HotStuff. Each collector cluster runs a mini-version of HotStuff, and since clusters are randomized each epoch, a new quorum ceritificate is required for each cluster each epoch.","title":"Quorum Certificate"},{"location":"knowledge_base/glossary/#other","text":"","title":"Other"},{"location":"knowledge_base/glossary/#graph","text":"TODO: introduce the idea in one sentence without technicals A graph is pair of sets G = (V, E). V is a set of elements called vertices (singular: vertex) and E is a set of paired vertices and its elements are called edges. An edge {x, y} contains vertices x and y and they are its endpoints. A vertex may belong to no edge and in that case it is not joined to any other vertex. The set of vertices in a graph is a discrete value, meaning the set of edges is also a discrete value. The number of vertices - |V| is the \"order\" of the graph and the number of edges - |E| is the \"size\" of the graph.","title":"Graph"},{"location":"knowledge_base/glossary/#directed-graph","text":"A directed graph comprises of set of vertices connected by \"directed\" edges \u2014 also called arcs. A directed graph is represented as a pair of sets G = (V, A) , where: V is a set of elements (vertices, nodes) A is a set of ordered pairs of vertices, called arcs","title":"Directed Graph"},{"location":"knowledge_base/glossary/#directed-acyclic-graph-dag","text":"A Directed Acyclic Graph contains no directed cycles \u2014 each edge is directed from one vertex to another such that following those directions never forms a closed loop. A directed graph is a DAG if and only if it can be topologically ordered by arranging the vertices as a linear ordering that is consistent with all edge directions. A topological ordering of a directed graph is an ordering of its vertices into a sequence, such that for every edge the start vertex of the edge occurs earlier in the sequence than the ending vertex of the edge. A graph that has a topological ordering does not have any cycles, consequently every graph with a topological ordering is acyclic. Every acyclic directed graph has at least one topological ordering.","title":"Directed Acyclic Graph (DAG)"},{"location":"knowledge_base/glossary/#distributed-system","text":"A system of computers connected by a network \u2014 local, hub-based or wide, that work together as one single computer. Applications built with distributed systems appear to users and other client applications as if they are running on a single machine. Distributed systems take advantage of each node's physical and virtual resources (horizontal scalability), minimize overload of the whole system and can be designed to achieve a certain performance level, speed or other characteristic levels needed for the application running to overcome.","title":"Distributed System"},{"location":"knowledge_base/glossary/#partition-of-a-set","text":"A set's partition is a group of its elements into non-empty subsets, in such a way that every element is included in exactly one subset.","title":"Partition of a Set"},{"location":"knowledge_base/glossary/#transitive-relation","text":"For a set T with elements a , b , c , d A{a, b, c, d} A relation R on the elements of the set that relates a to b , b to c and c to d , but also relates a to c , a to d and b to d is called transitive. a -> b, b -> c, c -> d AND a -> c, a -> d, b -> d","title":"Transitive Relation"},{"location":"knowledge_base/glossary/#example-of-a-transitive-relation-r","text":"A -> ancestor of B B -> ancestor of C => A -> ancestor of C A \u2500\u2500> B \u2500\u2500> C => A \u2500\u2500> C","title":"Example of a Transitive Relation R"},{"location":"knowledge_base/glossary/#example-of-a-non-transitive-relation-r","text":"A -> ancestor of B B -> ancestor of C A -> ancestor of D => D is NOT an ancestor of C A \u251c\u2500\u2500> B \u2500\u2500> C \u2514\u2500\u2500> D The inverse of a transtive relation is always transitive. See also Homogeneous Relation .","title":"Example of a Non-Transitive Relation R"},{"location":"knowledge_base/glossary/#transitive-closure-of-a-graph","text":"A transitive closure of a directed graph G is a directed graph G' with an edge (i, j) corresponding to each directed path from i to j in G . The resultant directed graph G' representation in the form of the adjacency matrix is called the connectivity matrix .","title":"Transitive Closure of a Graph"},{"location":"knowledge_base/flare/specification/","text":"","title":"Specification"},{"location":"knowledge_base/flare/consensus/avalanche/concepts/","text":"Avalanche Consensus Algorithm Concepts Snow Algorithm Group of Protocols A group of protocols with a strong probabilistic safety guarantee in the presence of Byzantine nodes. Description The core concept of operating is the execution of repeated sampling of the network at random, ultimately leading to correct nodes' behavior to obtain a common statement (decision, transaction value, outcome). This mechanism effectively brings the system to an irreversible state \u2014 meaning that a large potion of the network accepted a decision or a statement. So any such conflicting to that statement would be accepted with a negligible probability \u03b5 or smaller. TODO: finish A more thorough description of Snow algorithms specifications and examples can be found in the Avalanche Whitepaper . Comparison with Nakamoto Consensus TODO: add Specification TODO: add Leaderless Byzantine Fault Tolerance Leaderless Byzantine Fault Tolerance combines the Snowball and Practical Byzantine Fault Tolerance (pBFT) algorithms. It aims to take advantage of the decentralized aspect of Snowball and deterministic property of pBFT in order to eliminate the weakness of Snowballs's probabilistic property and pBFT's reliance on a \"leader\". LBFT Specification TODO: add","title":"Concepts"},{"location":"knowledge_base/flare/consensus/avalanche/concepts/#avalanche-consensus-algorithm-concepts","text":"","title":"Avalanche Consensus Algorithm Concepts"},{"location":"knowledge_base/flare/consensus/avalanche/concepts/#snow-algorithm-group-of-protocols","text":"A group of protocols with a strong probabilistic safety guarantee in the presence of Byzantine nodes.","title":"Snow Algorithm Group of Protocols"},{"location":"knowledge_base/flare/consensus/avalanche/concepts/#description","text":"The core concept of operating is the execution of repeated sampling of the network at random, ultimately leading to correct nodes' behavior to obtain a common statement (decision, transaction value, outcome). This mechanism effectively brings the system to an irreversible state \u2014 meaning that a large potion of the network accepted a decision or a statement. So any such conflicting to that statement would be accepted with a negligible probability \u03b5 or smaller. TODO: finish A more thorough description of Snow algorithms specifications and examples can be found in the Avalanche Whitepaper .","title":"Description"},{"location":"knowledge_base/flare/consensus/avalanche/concepts/#comparison-with-nakamoto-consensus","text":"TODO: add","title":"Comparison with Nakamoto Consensus"},{"location":"knowledge_base/flare/consensus/avalanche/concepts/#specification","text":"TODO: add","title":"Specification"},{"location":"knowledge_base/flare/consensus/avalanche/concepts/#leaderless-byzantine-fault-tolerance","text":"Leaderless Byzantine Fault Tolerance combines the Snowball and Practical Byzantine Fault Tolerance (pBFT) algorithms. It aims to take advantage of the decentralized aspect of Snowball and deterministic property of pBFT in order to eliminate the weakness of Snowballs's probabilistic property and pBFT's reliance on a \"leader\".","title":"Leaderless Byzantine Fault Tolerance"},{"location":"knowledge_base/flare/consensus/avalanche/concepts/#lbft-specification","text":"TODO: add","title":"LBFT Specification"},{"location":"knowledge_base/flare/consensus/avalanche/specification/","text":"Avalanche An internet-scale electronic payment system, evaluated in a large scale deployment, based on the Snow family of algorithms. Avalanche uses a DAG tree for transactions ancestry relation and some optimizations in terms of recursive query of each transaction parents and children, that are described below. However, the validation of transaction scripts, UTXO references and related specifics is left to the application implementation. A more thorough description of Avalanche specifications can be found in the Avalanche Whitepaper . TODO: elaborate more on the introduction Characteristics The Avalanche network operates with multiple single-decree instances (nodes), meaning that each instance can make a decision on only one value. Nodes use the Snowball algorithm for their decision-making process. In the case of a payment system and Avalanche, the Snowball logic used to determine a value is the process of deciding whether a transaction is valid. The protocol maintains the set of all known transactions using a dynamic, append-only Directed Acyclic Graph (DAG) structure. There are two main positives from this: DAG streamlines the path on which there are no conflicting transactions \u2014 a single vote on a DAG vertex implicitly votes for all transactions on the path to the genesis vertex using the same principle as Bitcoin Network (BTC), the DAG tree renders past decisions on the path of transactions as more and more difficult to undo without approval of the correct nodes The genesis block in the DAG structure is represented by the graph first link, called \"genesis vertex\". This is the point where the first two edges of the graph meet. When a new transaction is created, its parents are defined in the DAG structure as its ancestors and the transcation is their newly created descendant. The parent-child relationship in DAG is not mandatory for a child transaction may not have any relation to its parents transactions outputs. It can actually spend funds received in other transactions from its ancestor set . This is the set of all transactions reachable via parent edges throughout history. Oppositely \u2014 the term \"progeny\" means all existing (and potentially existing) children transactions and their children. The main purpose of a consensus protocol is to avoid the inclusion of conflicting transactions into the ledger. Even though the characteristics of a conflicting transaction are defined on an application level (ex. transactions that spend same UTXO), the notion of conflict can be abstracted in order to define the conflicting set. In Avalanche, every transaction belongs to a conflict set. This set consists of multiple transactions that are invalid towards each other or it is a singleton set - then it contains a single transaction (in the case of a virtuous transaction). Since reaching a consensus means to avoid any conflicts between transactions, only one transaction from each conflict set can be included in the approved path of transactions. Avalanche instantiates a Snowball instance for each conflict set. Avalanche treats the concept of repeated queries and multiple counters from Snowball taking advantage of the DAG structure. Specifically, when a transaction T is queried, all of its ancestors reachable through the DAG edges from it are implicitly part of the query. Consequently, nodes would respond positively to a query for transaction T only if T and all of its ancestors are the preferred option in their respective conflict sets. If more than a given threshold of responders vote positively, the transaction gets a chit . After that nodes compute their level of confidence as the sum of transaction chit and the chits its progeny, meaning they query the transaction just once and rely on new vertices and possible chits added to the progeny to build up their confidence. Described ties are broken in case of preference for first-seen transactions. TODO: explain this Chits can also be decoupled from the DAG structure, making the protocol immune to attacks where the attacker generates large padded subgraphs. Consensus Specification Each node u keeps track of all transactions it has learned about in set Tu . The set Tu is partitioned into mutually exclusive conflict sets \u2014 PT (partition of T ), and PT \u2208 Tu . Since conflicts are transitive , if PTi and PTj are conflicting, they belong to the same conflict set of transactions \u2014 PTi = PTj . Conflicting transactions have the equivalence relation because they are equivocations spending the same UTXO. The parent connection to T' from T is expressed as T' <\u2014 T . The relation T' *<\u2014 T is its reflexive transitive closure meaning that there is a path from T to T' . If T' <\u2014 T , then every node in the system that has T also has T' and knows about the same relation T' <\u2014 T . If such a relation between these transactions do not exist, then no node ends up with T' <\u2014 T . DAGs built in different nodes are guaranteed to be compatible, though at any moment in time they might not have a complete view of all vertices in the system. Each node can compute a confidence value d U (T) (confidence value for node u about transaction T ) from the progeny as the sum of transaction T descendants chits and its own chit. Each transaction initially has a chit of 0 before the node gets the query results. If the node collects a threshold of \u03b1 'yes' votes from the query the value of transaction T , chit is set to 1 , otherwise it will remain 0 . A chit value reflect the result from a single query of its associated transaction and becomes immutable afterwards, while confidence d can increase as the DAG grows by collecting more chits in its progeny. Additionally, each node maintains its own local list of known nodes: Nu \u2014 nodes known to u is a subset of the set of all existing nodes: Nu \u2286 N . Each node implements an event-driven state machine, centered around a query that serves both to solicit votes on each transaction and to notify other nodes of the existence of newly discovered transactions. When a node u discovers a transaction T through a query, it starts a one-time query process by sampling k random peers and sending a message to them, after T is delivered to the node by the onReceiveTx callback. Node u answers a query by checking whether each T' such that T' *<\u2014 T is currently preferred among competing transactions \u2200T'' \u2208 PT' from conflict set P of T' . If every single ancestor T' fulfills this criterion, the transaction is said to be strongly preferred and receives a yes-vote (1). A failure on this criterion at any T' yields a no-vote (0). When u accumulates k responses, it checks whether there are \u03b1 yes-votes for T , and if so grants the chit value ( cT = 1 ). This process yields a labeling of the DAG with a chit value and associated confidence for each transaction T . Example of (chit,confidence) values: T1(1,6) \u251c\u2500\u2500 T2(1,5) \u2502 \u251c\u2500\u2500 T4(1,2) \u2500\u2500 T8(1,1) \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 T5(1,3) \u2502 \u2514\u2500\u2500\u2500\u2500 T9(1,1) \u2502 \u2514\u2500\u2500 T3(0,0) \u251c\u2500\u2500 T6(0,0) \u2514\u2500\u2500 T7(0,0) Conflict set P for T1 has one element T1 . Conflict set P for T2 and T3 has two elements {T2, T3} . Conflict set P for T7 , T6 and T9 has 3 elements {T6, T7, T9} . Similar to Snowball, sampling in Avalanche creates a positive feedback for the preference of a single transaction in its conflict set. When a transaction has a larger confidence, its descendants are likely to collect chits in the future compared to others with lower confidence. For example T2 has higher confidence than T3 and its descendants are more likely to collect chits in the future. Similar to Bitcoin, Avalanche leaves determining the acceptance point of a transaction to the application. Committing a transaction can be performed through a safe early commitment. For virtuous transactions, T is accepted when it is the only transaction in its conflict set and has a confidence no less than a threshold of \u03b21 . If a virtuous transaction fails to get accepted due to a problem with parents, it could be accepted if reissued with different parents. Transaction Entangling How Avalanche entangles transactions: init: `Set T` = \u00d8 set of known transactions (empty set) Q = \u00d8 set of queried transactions (empty set) generateTx: edges = {T' <\u2014 T: T' \u2208 parentSelection(`Set T`)} T = Tx(data, edges) onReceiveEvent(T) onReceiveEvent(t): if T \u2209 `Set T` then if PT (conflict set of T) = \u00d8 then PT = {T}, Pt.pref = T PT.last = T, PT.cnt = 0 else PT = PT U {T} `Set T` = T U {T}, cT = 0 Because transactions that generate and consume the same UTXO do not conflict with each other, any transaction can be reissued with different parents. Node Main Loop Protocol main loop executed by each node: AvalancheLoop: while true do find T that satisfies T \u2208 `Set T` AND T \u2209 Q K = sample(N\\u, k) P = SUM(v \u2208 k) query(v, T) if P \u2265 \u03b1 then cT = 1 (chit for transaction T) update preferences for ancestors: for T' \u2208 `Set T`: T' *<\u2014 T do if T' confidence is larger than the confidence of preferred transation T` in the P conflict set - update the prefered transaction of the set: if d(T') > d(PT'.pref) then PT'.pref = T' if T' is not the last transaction in the T' conflict set - update the last place with it if T' \u2260 PT'.last then PT'.last = T', PT'.cnt=1 else ++PT'.cnt else for T' \u2208 `Set T`: T' *<\u2014 T do PT'.cnt = 0 otherwise confidence cnt remains 0 forever Q = Q U {T} - mark T as queried In each iteration the node tries to select a transaction that has not yet been queried. If no such transaction exists, the loop stalls until a new transaction is added to Set T . When a the new transaction is added, the node then select a number of peers k and queries them. If more than \u03b1 of these peers return a positive response, the chit value is set to 1 . After this step, the loop updates the preferred transaction of each conflict set of the transactions in its ancestry. Next, T is added to the set Q of queried transactions so it is never queried again by the node. Query for Transaction What happens when a node receives a query for transaction T from peer j : isPreferred(T): return T = PT.pref isStronglyPreferred(T): return \u2200T' \u2208 `Set T`, T' *<\u2014 T : isPreferred(T') isAccepted(T): return ((\u2200T' \u2208 `Set T`, T' <\u2014 T: isAccepted(T')) \u2229 |PT|=1 \u2229 PT.cnt \u2265 \u03b21) safe early commitment \u222a (PT.cnt \u2265 \u03b22) consecutive counter onQuery(j, T): onReceiveTx(T) respond(, isStronglyPreferred(T)) The first step is to add transaction T to Set T unless it is already included. After that step, u determines whether T is currently strongly preferred, in which case u returns a positive response to peer j , otherwise it returns a negative response. In the example pseudocode it is assumed that if a node knows about T , then it recursively knows about T 's ancestry. This can be achieved by postponing the delivery of T until its entire ancestry is recursively fetched (in practice an additional gossip process that disseminates the transaction is used in parallel). UTXO Graph In addition to the DAG structure used for transactions data, another graph is used for Unspent Transactions Outputs (UTXO) set, that captures the available outputs. It is used to realize the ledger for the payment system. In the present specifications transactions that encode the data for money transfer are called \"transactions\", and transactions T \u2208 Set T are DAG vertices. Transaction and address mechanisms are inherited from Bitcoin. In its simplest form, a transaction consists of multiple inputs and outputs with the corresponding redeem scripts. Addresses are identified by the hash of their public keys, and signatures are being generated by the correponding private keys. UTXO are fully consumed by a valid transaction and may generate new UTXOs spendable by the named recipients. Multiple-input transactions consume multiple UTXOs and in Avalanche may appear in multiple conflict sets. The conflict relation of transaction input pairs is transitive, because each pair only spends one unspent output. Each input of the transaction is checked by the logic of isAccepted(T) step, consequently a transaction is accepted only if all its input pairs are accepted in their respective Snowball conflict sets. The DAG of transaction-inputs is implemented in such a way that multiple transactions can be batched together per query. Evaluation of Performance TODO: add DAG Positives in Context of Avalanche TODO: add Cryptography Bottleneck TODO: add Snowball Comparison TODO: add","title":"Specification"},{"location":"knowledge_base/flare/consensus/avalanche/specification/#avalanche","text":"An internet-scale electronic payment system, evaluated in a large scale deployment, based on the Snow family of algorithms. Avalanche uses a DAG tree for transactions ancestry relation and some optimizations in terms of recursive query of each transaction parents and children, that are described below. However, the validation of transaction scripts, UTXO references and related specifics is left to the application implementation. A more thorough description of Avalanche specifications can be found in the Avalanche Whitepaper . TODO: elaborate more on the introduction","title":"Avalanche"},{"location":"knowledge_base/flare/consensus/avalanche/specification/#characteristics","text":"The Avalanche network operates with multiple single-decree instances (nodes), meaning that each instance can make a decision on only one value. Nodes use the Snowball algorithm for their decision-making process. In the case of a payment system and Avalanche, the Snowball logic used to determine a value is the process of deciding whether a transaction is valid. The protocol maintains the set of all known transactions using a dynamic, append-only Directed Acyclic Graph (DAG) structure. There are two main positives from this: DAG streamlines the path on which there are no conflicting transactions \u2014 a single vote on a DAG vertex implicitly votes for all transactions on the path to the genesis vertex using the same principle as Bitcoin Network (BTC), the DAG tree renders past decisions on the path of transactions as more and more difficult to undo without approval of the correct nodes The genesis block in the DAG structure is represented by the graph first link, called \"genesis vertex\". This is the point where the first two edges of the graph meet. When a new transaction is created, its parents are defined in the DAG structure as its ancestors and the transcation is their newly created descendant. The parent-child relationship in DAG is not mandatory for a child transaction may not have any relation to its parents transactions outputs. It can actually spend funds received in other transactions from its ancestor set . This is the set of all transactions reachable via parent edges throughout history. Oppositely \u2014 the term \"progeny\" means all existing (and potentially existing) children transactions and their children. The main purpose of a consensus protocol is to avoid the inclusion of conflicting transactions into the ledger. Even though the characteristics of a conflicting transaction are defined on an application level (ex. transactions that spend same UTXO), the notion of conflict can be abstracted in order to define the conflicting set. In Avalanche, every transaction belongs to a conflict set. This set consists of multiple transactions that are invalid towards each other or it is a singleton set - then it contains a single transaction (in the case of a virtuous transaction). Since reaching a consensus means to avoid any conflicts between transactions, only one transaction from each conflict set can be included in the approved path of transactions. Avalanche instantiates a Snowball instance for each conflict set. Avalanche treats the concept of repeated queries and multiple counters from Snowball taking advantage of the DAG structure. Specifically, when a transaction T is queried, all of its ancestors reachable through the DAG edges from it are implicitly part of the query. Consequently, nodes would respond positively to a query for transaction T only if T and all of its ancestors are the preferred option in their respective conflict sets. If more than a given threshold of responders vote positively, the transaction gets a chit . After that nodes compute their level of confidence as the sum of transaction chit and the chits its progeny, meaning they query the transaction just once and rely on new vertices and possible chits added to the progeny to build up their confidence. Described ties are broken in case of preference for first-seen transactions. TODO: explain this Chits can also be decoupled from the DAG structure, making the protocol immune to attacks where the attacker generates large padded subgraphs.","title":"Characteristics"},{"location":"knowledge_base/flare/consensus/avalanche/specification/#consensus-specification","text":"Each node u keeps track of all transactions it has learned about in set Tu . The set Tu is partitioned into mutually exclusive conflict sets \u2014 PT (partition of T ), and PT \u2208 Tu . Since conflicts are transitive , if PTi and PTj are conflicting, they belong to the same conflict set of transactions \u2014 PTi = PTj . Conflicting transactions have the equivalence relation because they are equivocations spending the same UTXO. The parent connection to T' from T is expressed as T' <\u2014 T . The relation T' *<\u2014 T is its reflexive transitive closure meaning that there is a path from T to T' . If T' <\u2014 T , then every node in the system that has T also has T' and knows about the same relation T' <\u2014 T . If such a relation between these transactions do not exist, then no node ends up with T' <\u2014 T . DAGs built in different nodes are guaranteed to be compatible, though at any moment in time they might not have a complete view of all vertices in the system. Each node can compute a confidence value d U (T) (confidence value for node u about transaction T ) from the progeny as the sum of transaction T descendants chits and its own chit. Each transaction initially has a chit of 0 before the node gets the query results. If the node collects a threshold of \u03b1 'yes' votes from the query the value of transaction T , chit is set to 1 , otherwise it will remain 0 . A chit value reflect the result from a single query of its associated transaction and becomes immutable afterwards, while confidence d can increase as the DAG grows by collecting more chits in its progeny. Additionally, each node maintains its own local list of known nodes: Nu \u2014 nodes known to u is a subset of the set of all existing nodes: Nu \u2286 N . Each node implements an event-driven state machine, centered around a query that serves both to solicit votes on each transaction and to notify other nodes of the existence of newly discovered transactions. When a node u discovers a transaction T through a query, it starts a one-time query process by sampling k random peers and sending a message to them, after T is delivered to the node by the onReceiveTx callback. Node u answers a query by checking whether each T' such that T' *<\u2014 T is currently preferred among competing transactions \u2200T'' \u2208 PT' from conflict set P of T' . If every single ancestor T' fulfills this criterion, the transaction is said to be strongly preferred and receives a yes-vote (1). A failure on this criterion at any T' yields a no-vote (0). When u accumulates k responses, it checks whether there are \u03b1 yes-votes for T , and if so grants the chit value ( cT = 1 ). This process yields a labeling of the DAG with a chit value and associated confidence for each transaction T . Example of (chit,confidence) values: T1(1,6) \u251c\u2500\u2500 T2(1,5) \u2502 \u251c\u2500\u2500 T4(1,2) \u2500\u2500 T8(1,1) \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 T5(1,3) \u2502 \u2514\u2500\u2500\u2500\u2500 T9(1,1) \u2502 \u2514\u2500\u2500 T3(0,0) \u251c\u2500\u2500 T6(0,0) \u2514\u2500\u2500 T7(0,0) Conflict set P for T1 has one element T1 . Conflict set P for T2 and T3 has two elements {T2, T3} . Conflict set P for T7 , T6 and T9 has 3 elements {T6, T7, T9} . Similar to Snowball, sampling in Avalanche creates a positive feedback for the preference of a single transaction in its conflict set. When a transaction has a larger confidence, its descendants are likely to collect chits in the future compared to others with lower confidence. For example T2 has higher confidence than T3 and its descendants are more likely to collect chits in the future. Similar to Bitcoin, Avalanche leaves determining the acceptance point of a transaction to the application. Committing a transaction can be performed through a safe early commitment. For virtuous transactions, T is accepted when it is the only transaction in its conflict set and has a confidence no less than a threshold of \u03b21 . If a virtuous transaction fails to get accepted due to a problem with parents, it could be accepted if reissued with different parents.","title":"Consensus Specification"},{"location":"knowledge_base/flare/consensus/avalanche/specification/#transaction-entangling","text":"How Avalanche entangles transactions: init: `Set T` = \u00d8 set of known transactions (empty set) Q = \u00d8 set of queried transactions (empty set) generateTx: edges = {T' <\u2014 T: T' \u2208 parentSelection(`Set T`)} T = Tx(data, edges) onReceiveEvent(T) onReceiveEvent(t): if T \u2209 `Set T` then if PT (conflict set of T) = \u00d8 then PT = {T}, Pt.pref = T PT.last = T, PT.cnt = 0 else PT = PT U {T} `Set T` = T U {T}, cT = 0 Because transactions that generate and consume the same UTXO do not conflict with each other, any transaction can be reissued with different parents.","title":"Transaction Entangling"},{"location":"knowledge_base/flare/consensus/avalanche/specification/#node-main-loop","text":"Protocol main loop executed by each node: AvalancheLoop: while true do find T that satisfies T \u2208 `Set T` AND T \u2209 Q K = sample(N\\u, k) P = SUM(v \u2208 k) query(v, T) if P \u2265 \u03b1 then cT = 1 (chit for transaction T) update preferences for ancestors: for T' \u2208 `Set T`: T' *<\u2014 T do if T' confidence is larger than the confidence of preferred transation T` in the P conflict set - update the prefered transaction of the set: if d(T') > d(PT'.pref) then PT'.pref = T' if T' is not the last transaction in the T' conflict set - update the last place with it if T' \u2260 PT'.last then PT'.last = T', PT'.cnt=1 else ++PT'.cnt else for T' \u2208 `Set T`: T' *<\u2014 T do PT'.cnt = 0 otherwise confidence cnt remains 0 forever Q = Q U {T} - mark T as queried In each iteration the node tries to select a transaction that has not yet been queried. If no such transaction exists, the loop stalls until a new transaction is added to Set T . When a the new transaction is added, the node then select a number of peers k and queries them. If more than \u03b1 of these peers return a positive response, the chit value is set to 1 . After this step, the loop updates the preferred transaction of each conflict set of the transactions in its ancestry. Next, T is added to the set Q of queried transactions so it is never queried again by the node.","title":"Node Main Loop"},{"location":"knowledge_base/flare/consensus/avalanche/specification/#query-for-transaction","text":"What happens when a node receives a query for transaction T from peer j : isPreferred(T): return T = PT.pref isStronglyPreferred(T): return \u2200T' \u2208 `Set T`, T' *<\u2014 T : isPreferred(T') isAccepted(T): return ((\u2200T' \u2208 `Set T`, T' <\u2014 T: isAccepted(T')) \u2229 |PT|=1 \u2229 PT.cnt \u2265 \u03b21) safe early commitment \u222a (PT.cnt \u2265 \u03b22) consecutive counter onQuery(j, T): onReceiveTx(T) respond(, isStronglyPreferred(T)) The first step is to add transaction T to Set T unless it is already included. After that step, u determines whether T is currently strongly preferred, in which case u returns a positive response to peer j , otherwise it returns a negative response. In the example pseudocode it is assumed that if a node knows about T , then it recursively knows about T 's ancestry. This can be achieved by postponing the delivery of T until its entire ancestry is recursively fetched (in practice an additional gossip process that disseminates the transaction is used in parallel).","title":"Query for Transaction"},{"location":"knowledge_base/flare/consensus/avalanche/specification/#utxo-graph","text":"In addition to the DAG structure used for transactions data, another graph is used for Unspent Transactions Outputs (UTXO) set, that captures the available outputs. It is used to realize the ledger for the payment system. In the present specifications transactions that encode the data for money transfer are called \"transactions\", and transactions T \u2208 Set T are DAG vertices. Transaction and address mechanisms are inherited from Bitcoin. In its simplest form, a transaction consists of multiple inputs and outputs with the corresponding redeem scripts. Addresses are identified by the hash of their public keys, and signatures are being generated by the correponding private keys. UTXO are fully consumed by a valid transaction and may generate new UTXOs spendable by the named recipients. Multiple-input transactions consume multiple UTXOs and in Avalanche may appear in multiple conflict sets. The conflict relation of transaction input pairs is transitive, because each pair only spends one unspent output. Each input of the transaction is checked by the logic of isAccepted(T) step, consequently a transaction is accepted only if all its input pairs are accepted in their respective Snowball conflict sets. The DAG of transaction-inputs is implemented in such a way that multiple transactions can be batched together per query.","title":"UTXO Graph"},{"location":"knowledge_base/flare/consensus/avalanche/specification/#evaluation-of-performance","text":"TODO: add","title":"Evaluation of Performance"},{"location":"knowledge_base/flare/consensus/avalanche/specification/#dag-positives-in-context-of-avalanche","text":"TODO: add","title":"DAG Positives in Context of Avalanche"},{"location":"knowledge_base/flare/consensus/avalanche/specification/#cryptography-bottleneck","text":"TODO: add","title":"Cryptography Bottleneck"},{"location":"knowledge_base/flare/consensus/avalanche/specification/#snowball-comparison","text":"TODO: add","title":"Snowball Comparison"},{"location":"knowledge_base/flare/consensus/stellar/concepts/","text":"Stellar Consensus Algorithm Concepts Federated Byzantine Agreement (FBA) Federated Byzantine agreement is a new model of consensus that achieves robustness through quorum slices. It is an adaptation of Byzantine agreement to systems with open membership, where different nodes have different concepts about which group of nodes are important to them about deciding on a statement. These groups of nodes let the inidividual node take decisions based on its own trust criteria. The groups bind the system together in the same way as inidividual networks' peering and transit decisions unifiy the Internet. Description FBA is a model suitable for a worldwide consensus. In FBA each particiapnt knows and relies on opinion of others that it considers important. It waits for the vast majority of them to agree on a transaction before considering the transaction settled. In turn, those important participant do not agree to the transaction until the participants they consider important agree as well, and so on. Eventually \u2014 enough of the network accepts a transaction and consequently it becomes infeasible for an attacker to roll it back. Only at that point all of the participants consider it settled. Stellar Consensus Protocol Stellar consensus protocol (SCP) is a construction of FBA \u2014 an FBA system (FBAS). It is free from blocked states, in which consensus is no longer possible, unless participant failures make it impossible to satisfy trust dependencies. Stellar claims to be the first provably safe consensusm mechanism to have the following key properties simultaneously: Decentralized control \u2014 anyone is able to participate and no central authority dictates whose approval is required for consensus. Low latency \u2014 nodes can reach consensus at timescales humans expect for web payments \u2014 a few seconds. Flexible trust \u2014 users have the freedom to trust any combination of parties they see fit. Asymptotic security \u2014 safety rests on digital signatures and hash families whose parameteres can reallistically be tuned to protect against adversaries with vast computing power. FBA Specification Like non-federated Byzantine agreement, FBA addresses the problem of updating replicated state, such as a transaction ledger or certificate tree. By agreeing on what updates to apply, nodes avoid contradictory, irreconcilable states. Each update is identified by a unque slot from which the inter-update dependencies can be inferred \u2014 for example slots can be consequtively numbered positions in a sequentially applied log. An FBA system runs a consensus protocol that ensures nodes agree on slot contents. A node u can safely apply update x in slot i when it has safely applied updates in all slots upon which i depends. Additionally, node u considers that all correctly functioning nodes wil eventually agree on x for slot i . At this point u externalized x for slot i . The outside world may react to externalized values in irreversible ways, so a node cannot change its mind about them later. A chanllenge for FBA is that malicious parties can join many times and outnumber honest nodes. Consequently, tranditional majority-based quorums do not work. Quorum Slices In a consensus protocol, nodes exchange messages asserting statements about slots. FBA assumes such assertions cannot be forged, which can be guaranteed if nodes are named by public key and they digitally sign the messages. When a node knows about a sufficient set of nodes to assert a statement, it assumes that no other functioning node would contradict that statement. Such a sufficient set of nodes is called a quorum slice , or just a slice. A node may have multiple slices, any of which is sufficient to convince it of a statement. At a high level the FBA system consists of a loose federation of nodes each of which has chosen one or more slices. One node might consider that a quorum needs to contain\u2004\u2265\u20043/4 of the nodes in some set S , while another believes that a quorum should contain \u2004>\u20042/3 of some similar but not identical set S\u2032 . Nodes might also have less symmetric requirements on quorums, for example a third node might require that a quorum contain a majority of the nodes run by company X and a majority of the nodes run by company Y , where X runs many more nodes than Y. By definition, a Federated Byzantine Agreement System (FBAS) is a pair {U, Q} that comprises of a set of nodes - U and a quorum function Q . Q specifies one or more quorum slices for each node in U , where a node belongs to all of its quorum slices. A quorum is a set of nodes sufficient to reach an agreement. A quorum slice is the subset of a quorum convincing one particular node of this agreement. A quorum slice can be smaller or the same as the quorum. Example of a four-node system, where each node has a single slice and arrows point to the other members of that slice: u1 \u2014> {u2, u3} Q(u1) = (Undefined, Undefined, Undefined) u2 -> {u2, u3, u4} u3 \u2014> {u2, u3, u3} u4 -> {u2, u3, u4} => Q(u2) = Q(u3) = Q(u4) The slice of node u1 is {u1, u2, u3} and it is sufficient to convince u1 of a statement. But u2 and u3 have slices that include also u4 node. This means that neither u2 or u3 can assert a statement without u4 . Consequently, no agreement is possible without the participation of u4 and the only quorum including u1 is the set of all nodes. Traditional non-federated Byzantine agreement requires all nodes to accept the same sllices for quorums - meaning quorums for each node should be equal. Since every member accepts every slice, traditional systems do not distinguish between slices and quorums. Traditional PBFT typically has 3f+1 nodes, any 2f+1 of which comprises a quorum, and f is the maximum number of Byzantine failures the system can handle. FBA enables each node u to chose its own quorum size Q(u) . In this way system-wide quorums arise from individual preferences any node may have. In some settings, no individual node may have a complete knowledge of all nodes in the system and yet consensus still should be possible. Federated Byzantine agreement is thus a generalization of the Byzantine agreement. Safety and Liveness Nodes in FBA are categorized as well-behaved or ill-behaved. A well-behaved node choses sensisble quorum slices and obeys the protocol, including eventually repsonding to all requests. An ill-behaved node does not. Ill-behaved nodes may act arbitrarily \u2014 they might be compromised, their owner may have maliciously modified the software or may be crashed. The goal of Byzantine agreement is to ensure that well-behaved nodes extrenalize the same values despite the presence of ill-behaved nodes. This goal contains two parts: preventing nodes from diverging and externalizing different values of the same slot ensuring nodes can actually externalize values as opposed to getting blocked in some dead-end state from which the consensus is no loger possible These properties concern two terms: liveness and safety. A set of nodes in FBAS enjoy safety if no two of them ever externalize different values from the same slot. A node in FBAS enjoys liveness if it can externalize new values without the participation of any failed (including ill-behaved) nodes. Well-behaved nodes that has noth safety and liveness properties are called correct nodes. liveness AND safety = correct nodes Nodes that are not correct are failed . NOT correct = failed nodes All ill-behaved nodes are failed, but a well-behaved node can fail too, by waiting indefinitely for messages from ill-behaved nodes or by having its state poisoned by incorrect messages from ill-behaved nodes. Nodes that lack liveness are blocked and nodes that lack safety are divergent . Quorum Intersection In FBAS there is a quorum intersection, if any two of the quorums share a node that is present in all quorums of the system. Disjoint quorums can independently agree on contradictionary statements that undermine the system wide agreement. That is why, when many quorums exists - quorum intersection fails if any two do not intersect. No protocol can guarantee safety in the absense of quorum intersection, since such a configuration can operate as two different FBA systems that do not exchange any messages. Even with quorum intersection, safety might be impossible when the node which intersects the distinct quorums is ill-behaved. In that case the effect of having an ill-behaved shared node is equivalent to lack of quorum intersection. Since ill-behaved nodes contribute nothing to safety, no protocol can guarantee safety without the well-behaved nodes being intersections points. In a worst-case scenario of safety, ill-behaved nodes can just always make any possible contradictionary statement and two quorums overlapping only at ill-behaved nodes will again be able to operate like two different FBA system. In short, FBAS can survive Byzantine failure by a number of nodes, if after deleting (ignoring) the ill-behaved nodes from the system and quorum slices, it still has a quorum intersection. It is the responsibility of each node to ensure that the selected quorum(s) it uses do not violate the quorum intersection. One way to solve this is to pick conservative slices that lead to large quorums. Malicious nodes can intentionally pick slices that violate the quorum intersection rule, lie about the value returned by the quorum or ingnore it in order to make arbitrary assertions. In short the value produced by the quorum is not meaningful, if an ill-behaved node is receiving that value. That is why the necessary property for safety \u2014 quorum intersection of well-behaved nodes after deleting ill-behaved nodes \u2014 is unaffected by the slices of ill-behaved nodes. Deletion is conceptual for describing better the optimal safety. A protocol should guarantee safety for quorum slices without the need to know about the ill-behaved nodes. Dispensable Sets (DSets) The fault tolerance of slices selected by the nodes is captured by the notion of a dispensable set or DSet. The safety and liveness of nodes outside a DSet can be guaranteed regardless of the behavior of nodes inside the the DSet \u2014 in an optimally resilient FBAS, if a single DSet encompasses every ill-behaved node, it also contains every failed node, and conversely all nodes outside a DSet are correct. To prevent a misbehaving DSet from affecting the correctness of other nodes, two properties must hold. safety \u2014 deleting the DSet cannot undermine quorum intersection. liveness \u2014 the DSet cannot deny other nodes a functioning quorum. Quorum availability despite the existense of a Dispensable set that contains ill-behaved nodes, protects against ill-behaved nodes in it that refuse to answer requests and block others' progress. Quorum intersection despite the existense of a Dispensable set that contains ill-behaved nodes, protects against the opposite \u2014 nodes in the DSet making contradictory assertions that enable other nodes to externalize inconsistent values for the same slot. Node must balance between the two threads in slice selection. All else equal, bigger slices lead to bigger quorums with greater overlap and that leads to the case when fewer DSets with failed nodes will undermine quorum intersection when deleted. On the other hand, bigger slices are more likely to contain failed nodes that endanger quorum availability. The smallest DSet that contains all ill-behaved nodes may encompass well-behaved nodes as well, reflecting the fact that a sufficiently large set of ill-behaved nodes can cause well-behaved nodes to fail. The DSets in FBA are determined 'a priori' by the Quorum function Q . Which nodes are well-behaved and ill-behaved depends on runtime behavior, such as machines getting compromised. The DSets that are important are those that encompass all ill-behaved nodes, as they help to distinguish nodes that should be guaranteed correct from ones that cannot. A node in FBA is intact if there exists a DSet that contains all ill-behaved nodes, and none of its nodes are overlapping with the node. A node in FBA is befouled if it is not intact. A befouled node is surrounded by enough failed nodes to block its progress or poison its state, even it is well-behaved itself. No FBAS can guarantee the correctness of a befouled node. Optimal FBAS guarantees that every intact node remains correct. Federated Voting Voting Federated Voting consists of nodes casting vote messages that also specify their quorum slices. Recipients of these messages can dynamically discover quorums based on voters\u2019 stated slices. Quorum overlap ensures intertwined nodes will not find themselves in contradictory quorums. However, knowing that no quorum will contradict a is insufficient to confirm a When a member of an intact set confirms a , we also need to guarantee that the rest of the set will eventually do so. This requires ensuring not just that a received a quorum, but that a quorum knows that a received a quorum. Furthermore, this quorum must be able to convince other nodes, including ones that voted against a , to confirm a . Federated voting employs a three-phase protocol in which nodes first vote for a statement (broadcasting a message to this effect), then accept it (again broadcasting the fact), and finally confirm it. From the perspective of each node, agreement process on statement is divided in three phases \u2014 unknown, accepted and confirmed. Initially the statement status is completely unknown to the node \u2014 it can be true , false or can become completely stuck in a indeterminate state. If the first phase ( vote ) succeeds, meaning the statement is accepted as true, the node may accept the statement as well. A node may vote for any valid statement a that is consistent with its other outstanding votes and accepted statements. The node accepts a when it is a member of a quorum, in which every node either votes for a or accepts a . Even if the node did not vote for a , if every one of its quorum slices contains a node accepting a and the node has accepted nothing contradicting a , it also accepts a . No two nodes that are intact cannot accept contradictory statements. This means that if the node is intact and accepts a statement as true, then the statement cannot be false. The set of accepting nodes intersecting all of the node's quorum slices overrules any contradictory votes that the node may have previously cast by proving these contradictory votes could not have been part of a quorum. Finally, when the node is a member of a quorum in which every node accepts a , then this node confirms a . However, there are reasons for which after the statement is accepted from the node, it might not act upon it. One is the statement can be stuck for other nodes. Second is the node could be befouled, which is something it does not know itself, then accepting the statement means nothing (the statement could be false for well-behaved nodes). Even if the node is befouled, the system might still have quorum intersection of well-behaved nodes. In this case, for optimal safety, the node would need a greater assurance for the statement. This is when the confirmation phase takes place. This phase addresses both problems: if the second voting on the statements succeeds, the node moves to the confirm phase and in this phase it can consider the statement as true and act on it (considering a statement as true equals to acceptin a value and addiing it to own records). Voting with Open Membership A correct node in a Byzantine agreement system acts on a statement only when it knows that other corect nodes will never agree to other contraditory statements. Most protocols employ voting for this purpose. Well-behaved nodes vote for a statement only if it is valid and also never change their votes. Consequently in a centralized Byzantine agreement, it is safe to accept a statement as valid, if a quorum comprising a majority of well-behaved nodes has voted for it. We say the statement is ratified when it received the necessary votes. The FBA adapts the voting to open membership. A quorum, as mentioned above, no more corresponds to a majority of well-behaved nodes. In FBA the majority requirement serves to ensure quorum intersection of well-behaved nodes that is present. A node votes for a statement if and only if the node asserts the statement as valid and consistent with all statements it accepted in the past the node asserts it has never voted against this statement implicitly \u2014 meaning did not vote for a statement that contradicts the one that it is currently accepting the node promises never to vote against that statement in the future \u2014 the node will not vote against it implicitly accepting a future contradictory statement A quorum ratifies a statement if and only if all every member of it votes for the statement being true. A node ratifies a statement if the quorum it is a member of ratified the statement. Two contradictory statements in FBAS cannot be both ratified if there is a quorum intersection in that system.","title":"Concepts"},{"location":"knowledge_base/flare/consensus/stellar/concepts/#stellar-consensus-algorithm-concepts","text":"","title":"Stellar Consensus Algorithm Concepts"},{"location":"knowledge_base/flare/consensus/stellar/concepts/#federated-byzantine-agreement-fba","text":"Federated Byzantine agreement is a new model of consensus that achieves robustness through quorum slices. It is an adaptation of Byzantine agreement to systems with open membership, where different nodes have different concepts about which group of nodes are important to them about deciding on a statement. These groups of nodes let the inidividual node take decisions based on its own trust criteria. The groups bind the system together in the same way as inidividual networks' peering and transit decisions unifiy the Internet.","title":"Federated Byzantine Agreement (FBA)"},{"location":"knowledge_base/flare/consensus/stellar/concepts/#description","text":"FBA is a model suitable for a worldwide consensus. In FBA each particiapnt knows and relies on opinion of others that it considers important. It waits for the vast majority of them to agree on a transaction before considering the transaction settled. In turn, those important participant do not agree to the transaction until the participants they consider important agree as well, and so on. Eventually \u2014 enough of the network accepts a transaction and consequently it becomes infeasible for an attacker to roll it back. Only at that point all of the participants consider it settled.","title":"Description"},{"location":"knowledge_base/flare/consensus/stellar/concepts/#stellar-consensus-protocol","text":"Stellar consensus protocol (SCP) is a construction of FBA \u2014 an FBA system (FBAS). It is free from blocked states, in which consensus is no longer possible, unless participant failures make it impossible to satisfy trust dependencies. Stellar claims to be the first provably safe consensusm mechanism to have the following key properties simultaneously: Decentralized control \u2014 anyone is able to participate and no central authority dictates whose approval is required for consensus. Low latency \u2014 nodes can reach consensus at timescales humans expect for web payments \u2014 a few seconds. Flexible trust \u2014 users have the freedom to trust any combination of parties they see fit. Asymptotic security \u2014 safety rests on digital signatures and hash families whose parameteres can reallistically be tuned to protect against adversaries with vast computing power.","title":"Stellar Consensus Protocol"},{"location":"knowledge_base/flare/consensus/stellar/concepts/#fba-specification","text":"Like non-federated Byzantine agreement, FBA addresses the problem of updating replicated state, such as a transaction ledger or certificate tree. By agreeing on what updates to apply, nodes avoid contradictory, irreconcilable states. Each update is identified by a unque slot from which the inter-update dependencies can be inferred \u2014 for example slots can be consequtively numbered positions in a sequentially applied log. An FBA system runs a consensus protocol that ensures nodes agree on slot contents. A node u can safely apply update x in slot i when it has safely applied updates in all slots upon which i depends. Additionally, node u considers that all correctly functioning nodes wil eventually agree on x for slot i . At this point u externalized x for slot i . The outside world may react to externalized values in irreversible ways, so a node cannot change its mind about them later. A chanllenge for FBA is that malicious parties can join many times and outnumber honest nodes. Consequently, tranditional majority-based quorums do not work.","title":"FBA Specification"},{"location":"knowledge_base/flare/consensus/stellar/concepts/#quorum-slices","text":"In a consensus protocol, nodes exchange messages asserting statements about slots. FBA assumes such assertions cannot be forged, which can be guaranteed if nodes are named by public key and they digitally sign the messages. When a node knows about a sufficient set of nodes to assert a statement, it assumes that no other functioning node would contradict that statement. Such a sufficient set of nodes is called a quorum slice , or just a slice. A node may have multiple slices, any of which is sufficient to convince it of a statement. At a high level the FBA system consists of a loose federation of nodes each of which has chosen one or more slices. One node might consider that a quorum needs to contain\u2004\u2265\u20043/4 of the nodes in some set S , while another believes that a quorum should contain \u2004>\u20042/3 of some similar but not identical set S\u2032 . Nodes might also have less symmetric requirements on quorums, for example a third node might require that a quorum contain a majority of the nodes run by company X and a majority of the nodes run by company Y , where X runs many more nodes than Y. By definition, a Federated Byzantine Agreement System (FBAS) is a pair {U, Q} that comprises of a set of nodes - U and a quorum function Q . Q specifies one or more quorum slices for each node in U , where a node belongs to all of its quorum slices. A quorum is a set of nodes sufficient to reach an agreement. A quorum slice is the subset of a quorum convincing one particular node of this agreement. A quorum slice can be smaller or the same as the quorum. Example of a four-node system, where each node has a single slice and arrows point to the other members of that slice: u1 \u2014> {u2, u3} Q(u1) = (Undefined, Undefined, Undefined) u2 -> {u2, u3, u4} u3 \u2014> {u2, u3, u3} u4 -> {u2, u3, u4} => Q(u2) = Q(u3) = Q(u4) The slice of node u1 is {u1, u2, u3} and it is sufficient to convince u1 of a statement. But u2 and u3 have slices that include also u4 node. This means that neither u2 or u3 can assert a statement without u4 . Consequently, no agreement is possible without the participation of u4 and the only quorum including u1 is the set of all nodes. Traditional non-federated Byzantine agreement requires all nodes to accept the same sllices for quorums - meaning quorums for each node should be equal. Since every member accepts every slice, traditional systems do not distinguish between slices and quorums. Traditional PBFT typically has 3f+1 nodes, any 2f+1 of which comprises a quorum, and f is the maximum number of Byzantine failures the system can handle. FBA enables each node u to chose its own quorum size Q(u) . In this way system-wide quorums arise from individual preferences any node may have. In some settings, no individual node may have a complete knowledge of all nodes in the system and yet consensus still should be possible. Federated Byzantine agreement is thus a generalization of the Byzantine agreement.","title":"Quorum Slices"},{"location":"knowledge_base/flare/consensus/stellar/concepts/#safety-and-liveness","text":"Nodes in FBA are categorized as well-behaved or ill-behaved. A well-behaved node choses sensisble quorum slices and obeys the protocol, including eventually repsonding to all requests. An ill-behaved node does not. Ill-behaved nodes may act arbitrarily \u2014 they might be compromised, their owner may have maliciously modified the software or may be crashed. The goal of Byzantine agreement is to ensure that well-behaved nodes extrenalize the same values despite the presence of ill-behaved nodes. This goal contains two parts: preventing nodes from diverging and externalizing different values of the same slot ensuring nodes can actually externalize values as opposed to getting blocked in some dead-end state from which the consensus is no loger possible These properties concern two terms: liveness and safety. A set of nodes in FBAS enjoy safety if no two of them ever externalize different values from the same slot. A node in FBAS enjoys liveness if it can externalize new values without the participation of any failed (including ill-behaved) nodes. Well-behaved nodes that has noth safety and liveness properties are called correct nodes. liveness AND safety = correct nodes Nodes that are not correct are failed . NOT correct = failed nodes All ill-behaved nodes are failed, but a well-behaved node can fail too, by waiting indefinitely for messages from ill-behaved nodes or by having its state poisoned by incorrect messages from ill-behaved nodes. Nodes that lack liveness are blocked and nodes that lack safety are divergent .","title":"Safety and Liveness"},{"location":"knowledge_base/flare/consensus/stellar/concepts/#quorum-intersection","text":"In FBAS there is a quorum intersection, if any two of the quorums share a node that is present in all quorums of the system. Disjoint quorums can independently agree on contradictionary statements that undermine the system wide agreement. That is why, when many quorums exists - quorum intersection fails if any two do not intersect. No protocol can guarantee safety in the absense of quorum intersection, since such a configuration can operate as two different FBA systems that do not exchange any messages. Even with quorum intersection, safety might be impossible when the node which intersects the distinct quorums is ill-behaved. In that case the effect of having an ill-behaved shared node is equivalent to lack of quorum intersection. Since ill-behaved nodes contribute nothing to safety, no protocol can guarantee safety without the well-behaved nodes being intersections points. In a worst-case scenario of safety, ill-behaved nodes can just always make any possible contradictionary statement and two quorums overlapping only at ill-behaved nodes will again be able to operate like two different FBA system. In short, FBAS can survive Byzantine failure by a number of nodes, if after deleting (ignoring) the ill-behaved nodes from the system and quorum slices, it still has a quorum intersection. It is the responsibility of each node to ensure that the selected quorum(s) it uses do not violate the quorum intersection. One way to solve this is to pick conservative slices that lead to large quorums. Malicious nodes can intentionally pick slices that violate the quorum intersection rule, lie about the value returned by the quorum or ingnore it in order to make arbitrary assertions. In short the value produced by the quorum is not meaningful, if an ill-behaved node is receiving that value. That is why the necessary property for safety \u2014 quorum intersection of well-behaved nodes after deleting ill-behaved nodes \u2014 is unaffected by the slices of ill-behaved nodes. Deletion is conceptual for describing better the optimal safety. A protocol should guarantee safety for quorum slices without the need to know about the ill-behaved nodes.","title":"Quorum Intersection"},{"location":"knowledge_base/flare/consensus/stellar/concepts/#dispensable-sets-dsets","text":"The fault tolerance of slices selected by the nodes is captured by the notion of a dispensable set or DSet. The safety and liveness of nodes outside a DSet can be guaranteed regardless of the behavior of nodes inside the the DSet \u2014 in an optimally resilient FBAS, if a single DSet encompasses every ill-behaved node, it also contains every failed node, and conversely all nodes outside a DSet are correct. To prevent a misbehaving DSet from affecting the correctness of other nodes, two properties must hold. safety \u2014 deleting the DSet cannot undermine quorum intersection. liveness \u2014 the DSet cannot deny other nodes a functioning quorum. Quorum availability despite the existense of a Dispensable set that contains ill-behaved nodes, protects against ill-behaved nodes in it that refuse to answer requests and block others' progress. Quorum intersection despite the existense of a Dispensable set that contains ill-behaved nodes, protects against the opposite \u2014 nodes in the DSet making contradictory assertions that enable other nodes to externalize inconsistent values for the same slot. Node must balance between the two threads in slice selection. All else equal, bigger slices lead to bigger quorums with greater overlap and that leads to the case when fewer DSets with failed nodes will undermine quorum intersection when deleted. On the other hand, bigger slices are more likely to contain failed nodes that endanger quorum availability. The smallest DSet that contains all ill-behaved nodes may encompass well-behaved nodes as well, reflecting the fact that a sufficiently large set of ill-behaved nodes can cause well-behaved nodes to fail. The DSets in FBA are determined 'a priori' by the Quorum function Q . Which nodes are well-behaved and ill-behaved depends on runtime behavior, such as machines getting compromised. The DSets that are important are those that encompass all ill-behaved nodes, as they help to distinguish nodes that should be guaranteed correct from ones that cannot. A node in FBA is intact if there exists a DSet that contains all ill-behaved nodes, and none of its nodes are overlapping with the node. A node in FBA is befouled if it is not intact. A befouled node is surrounded by enough failed nodes to block its progress or poison its state, even it is well-behaved itself. No FBAS can guarantee the correctness of a befouled node. Optimal FBAS guarantees that every intact node remains correct.","title":"Dispensable Sets (DSets)"},{"location":"knowledge_base/flare/consensus/stellar/concepts/#federated-voting","text":"","title":"Federated Voting"},{"location":"knowledge_base/flare/consensus/stellar/concepts/#voting","text":"Federated Voting consists of nodes casting vote messages that also specify their quorum slices. Recipients of these messages can dynamically discover quorums based on voters\u2019 stated slices. Quorum overlap ensures intertwined nodes will not find themselves in contradictory quorums. However, knowing that no quorum will contradict a is insufficient to confirm a When a member of an intact set confirms a , we also need to guarantee that the rest of the set will eventually do so. This requires ensuring not just that a received a quorum, but that a quorum knows that a received a quorum. Furthermore, this quorum must be able to convince other nodes, including ones that voted against a , to confirm a . Federated voting employs a three-phase protocol in which nodes first vote for a statement (broadcasting a message to this effect), then accept it (again broadcasting the fact), and finally confirm it. From the perspective of each node, agreement process on statement is divided in three phases \u2014 unknown, accepted and confirmed. Initially the statement status is completely unknown to the node \u2014 it can be true , false or can become completely stuck in a indeterminate state. If the first phase ( vote ) succeeds, meaning the statement is accepted as true, the node may accept the statement as well. A node may vote for any valid statement a that is consistent with its other outstanding votes and accepted statements. The node accepts a when it is a member of a quorum, in which every node either votes for a or accepts a . Even if the node did not vote for a , if every one of its quorum slices contains a node accepting a and the node has accepted nothing contradicting a , it also accepts a . No two nodes that are intact cannot accept contradictory statements. This means that if the node is intact and accepts a statement as true, then the statement cannot be false. The set of accepting nodes intersecting all of the node's quorum slices overrules any contradictory votes that the node may have previously cast by proving these contradictory votes could not have been part of a quorum. Finally, when the node is a member of a quorum in which every node accepts a , then this node confirms a . However, there are reasons for which after the statement is accepted from the node, it might not act upon it. One is the statement can be stuck for other nodes. Second is the node could be befouled, which is something it does not know itself, then accepting the statement means nothing (the statement could be false for well-behaved nodes). Even if the node is befouled, the system might still have quorum intersection of well-behaved nodes. In this case, for optimal safety, the node would need a greater assurance for the statement. This is when the confirmation phase takes place. This phase addresses both problems: if the second voting on the statements succeeds, the node moves to the confirm phase and in this phase it can consider the statement as true and act on it (considering a statement as true equals to acceptin a value and addiing it to own records).","title":"Voting"},{"location":"knowledge_base/flare/consensus/stellar/concepts/#voting-with-open-membership","text":"A correct node in a Byzantine agreement system acts on a statement only when it knows that other corect nodes will never agree to other contraditory statements. Most protocols employ voting for this purpose. Well-behaved nodes vote for a statement only if it is valid and also never change their votes. Consequently in a centralized Byzantine agreement, it is safe to accept a statement as valid, if a quorum comprising a majority of well-behaved nodes has voted for it. We say the statement is ratified when it received the necessary votes. The FBA adapts the voting to open membership. A quorum, as mentioned above, no more corresponds to a majority of well-behaved nodes. In FBA the majority requirement serves to ensure quorum intersection of well-behaved nodes that is present. A node votes for a statement if and only if the node asserts the statement as valid and consistent with all statements it accepted in the past the node asserts it has never voted against this statement implicitly \u2014 meaning did not vote for a statement that contradicts the one that it is currently accepting the node promises never to vote against that statement in the future \u2014 the node will not vote against it implicitly accepting a future contradictory statement A quorum ratifies a statement if and only if all every member of it votes for the statement being true. A node ratifies a statement if the quorum it is a member of ratified the statement. Two contradictory statements in FBAS cannot be both ratified if there is a quorum intersection in that system.","title":"Voting with Open Membership"},{"location":"knowledge_base/flare/consensus/stellar/specification/","text":"","title":"Specification"},{"location":"knowledge_base/flow/specification/","text":"","title":"Specification"},{"location":"knowledge_base/flow/cadence/accounts/","text":"Account Accounts are address on the flow network, e.g. 0xf233dcee88fe0abe . Every account can be accessed through two types \u2014 PublicAccount and AuthAccount , each corresponding to a level of accessabilty. Cadence transactions consist of four optional phases \u2014 prepare, precondition, execution and postconditions phase. Phases can be omitted, but, if present, they must appear in that order. Each account signing the transaction appears as an argument in the prepare() phase. These signers appear as arguments of AuthAccount type, and their number must match the number of signers of the transaction. The prepare phase is the only place where direct access to the signing accounts is possible. transaction { prepare ( account : AuthAccount , account2 : AuthAccount , ( .. .), accountN : AuthAccount ) { // ... } } PublicAccount PublicAccount represents the publicly available information about an account. This information can be retrieved about any account using the getAccount() function. struct PublicAccount { let address : Address // The FLOW balance of the default vault of this account let balance : UFix64 // The FLOW balance of the default vault of this account that is available to be moved let availableBalance : UFix64 // Amount of storage used by the account, in bytes let storageUsed : UInt64 // storage capacity of the account, in bytes let storageCapacity : UInt64 // Contracts deployed to the account let contracts : PublicAccount . Contracts // Keys assigned to the account let keys : PublicAccount . Keys // Storage operations fun getCapability < T > ( _ path : PublicPath ) : Capability < T > fun getLinkTarget ( _ path : CapabilityPath ) : Path ? struct Contracts { let names : [ String ] fun get ( name : String ) : DeployedContract ? } struct Keys { // Returns the key at the given index, if it exists. // Revoked keys are always returned, but they have `isRevoked` field set to true. fun get ( keyIndex : Int ) : AccountKey ? } } AuthAccount On the other hand, AuthAccount represents an authorized account. Authorized accounts can only be encountered in the prepare function of a signed transaction. struct AuthAccount { let address : Address // The FLOW balance of the default vault of this account let balance : UFix64 // The FLOW balance of the default vault of this account that is available to be moved let availableBalance : UFix64 // Amount of storage used by the account, in bytes let storageUsed : UInt64 // storage capacity of the account, in bytes let storageCapacity : UInt64 // Contracts deployed to the account let contracts : AuthAccount . Contracts // Keys assigned to the account let keys : AuthAccount . Keys // Key management // Adds a public key to the account. // The public key must be encoded together with their signature algorithm, hashing algorithm and weight. // This method is currently deprecated and is available only for the backward compatibility. // `keys.add` method can be use instead. fun addPublicKey ( _ publicKey : [ UInt8 ]) // Revokes the key at the given index. // This method is currently deprecated and is available only for the backward compatibility. // `keys.revoke` method can be use instead. fun removePublicKey ( _ index : Int ) // Account storage API (see the section below for documentation) fun save < T > ( _ value : T , to : StoragePath ) fun load < T > ( from : StoragePath ) : T ? fun copy < T : AnyStruct > ( from : StoragePath ) : T ? fun borrow < T : & Any > ( from : StoragePath ) : T ? fun link < T : & Any > ( _ newCapabilityPath : CapabilityPath , target : Path ) : Capability < T >? fun getCapability < T > ( _ path : CapabilityPath ) : Capability < T > fun getLinkTarget ( _ path : CapabilityPath ) : Path ? fun unlink ( _ path : CapabilityPath ) struct Contracts { // The names of each contract deployed to the account let names : [ String ] fun add ( name : String , code : [ UInt8 ], .. . contractInitializerArguments ) : DeployedContract fun update__experimental ( name : String , code : [ UInt8 ]) : DeployedContract fun get ( name : String ) : DeployedContract ? fun remove ( name : String ) : DeployedContract ? } struct Keys { // Adds a new key with the given hashing algorithm and a weight, and returns the added key. fun add ( publicKey : PublicKey , hashAlgorithm : HashAlgorithm , weight : UFix64 ) : AccountKey // Returns the key at the given index, if it exists, or nil otherwise. // Revoked keys are always returned, but they have `isRevoked` field set to true. fun get ( keyIndex : Int ) : AccountKey ? // Marks the key at the given index revoked, but does not delete it. // Returns the revoked key if it exists, or nil otherwise. fun revoke ( keyIndex : Int ) : AccountKey ? } } struct DeployedContract { let name : String let code : [ UInt8 ] } Account Storage Each account has storage, and this is where resources and structs can be persisted. Authorized accounts have full access to the accounts storage. Objects in storage are stored under paths. Each storage path location corresponds to a single register. Paths correspond to the Key part of the register ID. Paths have a format of /<domain>/<identifier> . There are three valid domains \u2014 storage , public and private . Objects in storage are always stored in the storage domain \u2014 meaning this is where resources and any other data is kept. The public domain allows the account to let other accounts access the objects, links to storage , inside it.","title":"Accounts"},{"location":"knowledge_base/flow/cadence/accounts/#account","text":"Accounts are address on the flow network, e.g. 0xf233dcee88fe0abe . Every account can be accessed through two types \u2014 PublicAccount and AuthAccount , each corresponding to a level of accessabilty. Cadence transactions consist of four optional phases \u2014 prepare, precondition, execution and postconditions phase. Phases can be omitted, but, if present, they must appear in that order. Each account signing the transaction appears as an argument in the prepare() phase. These signers appear as arguments of AuthAccount type, and their number must match the number of signers of the transaction. The prepare phase is the only place where direct access to the signing accounts is possible. transaction { prepare ( account : AuthAccount , account2 : AuthAccount , ( .. .), accountN : AuthAccount ) { // ... } }","title":"Account"},{"location":"knowledge_base/flow/cadence/accounts/#publicaccount","text":"PublicAccount represents the publicly available information about an account. This information can be retrieved about any account using the getAccount() function. struct PublicAccount { let address : Address // The FLOW balance of the default vault of this account let balance : UFix64 // The FLOW balance of the default vault of this account that is available to be moved let availableBalance : UFix64 // Amount of storage used by the account, in bytes let storageUsed : UInt64 // storage capacity of the account, in bytes let storageCapacity : UInt64 // Contracts deployed to the account let contracts : PublicAccount . Contracts // Keys assigned to the account let keys : PublicAccount . Keys // Storage operations fun getCapability < T > ( _ path : PublicPath ) : Capability < T > fun getLinkTarget ( _ path : CapabilityPath ) : Path ? struct Contracts { let names : [ String ] fun get ( name : String ) : DeployedContract ? } struct Keys { // Returns the key at the given index, if it exists. // Revoked keys are always returned, but they have `isRevoked` field set to true. fun get ( keyIndex : Int ) : AccountKey ? } }","title":"PublicAccount"},{"location":"knowledge_base/flow/cadence/accounts/#authaccount","text":"On the other hand, AuthAccount represents an authorized account. Authorized accounts can only be encountered in the prepare function of a signed transaction. struct AuthAccount { let address : Address // The FLOW balance of the default vault of this account let balance : UFix64 // The FLOW balance of the default vault of this account that is available to be moved let availableBalance : UFix64 // Amount of storage used by the account, in bytes let storageUsed : UInt64 // storage capacity of the account, in bytes let storageCapacity : UInt64 // Contracts deployed to the account let contracts : AuthAccount . Contracts // Keys assigned to the account let keys : AuthAccount . Keys // Key management // Adds a public key to the account. // The public key must be encoded together with their signature algorithm, hashing algorithm and weight. // This method is currently deprecated and is available only for the backward compatibility. // `keys.add` method can be use instead. fun addPublicKey ( _ publicKey : [ UInt8 ]) // Revokes the key at the given index. // This method is currently deprecated and is available only for the backward compatibility. // `keys.revoke` method can be use instead. fun removePublicKey ( _ index : Int ) // Account storage API (see the section below for documentation) fun save < T > ( _ value : T , to : StoragePath ) fun load < T > ( from : StoragePath ) : T ? fun copy < T : AnyStruct > ( from : StoragePath ) : T ? fun borrow < T : & Any > ( from : StoragePath ) : T ? fun link < T : & Any > ( _ newCapabilityPath : CapabilityPath , target : Path ) : Capability < T >? fun getCapability < T > ( _ path : CapabilityPath ) : Capability < T > fun getLinkTarget ( _ path : CapabilityPath ) : Path ? fun unlink ( _ path : CapabilityPath ) struct Contracts { // The names of each contract deployed to the account let names : [ String ] fun add ( name : String , code : [ UInt8 ], .. . contractInitializerArguments ) : DeployedContract fun update__experimental ( name : String , code : [ UInt8 ]) : DeployedContract fun get ( name : String ) : DeployedContract ? fun remove ( name : String ) : DeployedContract ? } struct Keys { // Adds a new key with the given hashing algorithm and a weight, and returns the added key. fun add ( publicKey : PublicKey , hashAlgorithm : HashAlgorithm , weight : UFix64 ) : AccountKey // Returns the key at the given index, if it exists, or nil otherwise. // Revoked keys are always returned, but they have `isRevoked` field set to true. fun get ( keyIndex : Int ) : AccountKey ? // Marks the key at the given index revoked, but does not delete it. // Returns the revoked key if it exists, or nil otherwise. fun revoke ( keyIndex : Int ) : AccountKey ? } } struct DeployedContract { let name : String let code : [ UInt8 ] }","title":"AuthAccount"},{"location":"knowledge_base/flow/cadence/accounts/#account-storage","text":"Each account has storage, and this is where resources and structs can be persisted. Authorized accounts have full access to the accounts storage. Objects in storage are stored under paths. Each storage path location corresponds to a single register. Paths correspond to the Key part of the register ID. Paths have a format of /<domain>/<identifier> . There are three valid domains \u2014 storage , public and private . Objects in storage are always stored in the storage domain \u2014 meaning this is where resources and any other data is kept. The public domain allows the account to let other accounts access the objects, links to storage , inside it.","title":"Account Storage"},{"location":"knowledge_base/flow/cadence/introduction/","text":"Introduction Cadence is a resource-oriented programming language specifically designed for smart-contract programming. Goals Cadence Language was designed with these tree goals in mind: Safety and security Safety is the underlying reliability of any smart contract. Security is the prevention of attacks on the network or smart contracts. Clarity Code needs to be easy to read, and its meaning should be as unambiguous as possible. Simplicity Writing code and creating programs should be as approachable as possible. Features Some of the features of the Cadence programming language are: type safety and a strong static type system resource-oriented programming resources are types that can only exist in one location at a time and cannot be copied, lost or stolen; Thus there is a concept of scarcity and ownership over these objects built-in pre-conditions and post-conditions for functions and transactions capability-based security access to objects is restricted only to the owner of the object and those who have a valid reference to it Terminology Invalid : The invalid program is not even be allowed to run. The program error is detected and reported statically by the type checker. Run-time error : The erroneous program can run, but bad behavior will result in the execution of the program being aborted.","title":"Introduction"},{"location":"knowledge_base/flow/cadence/introduction/#introduction","text":"Cadence is a resource-oriented programming language specifically designed for smart-contract programming.","title":"Introduction"},{"location":"knowledge_base/flow/cadence/introduction/#goals","text":"Cadence Language was designed with these tree goals in mind: Safety and security Safety is the underlying reliability of any smart contract. Security is the prevention of attacks on the network or smart contracts. Clarity Code needs to be easy to read, and its meaning should be as unambiguous as possible. Simplicity Writing code and creating programs should be as approachable as possible.","title":"Goals"},{"location":"knowledge_base/flow/cadence/introduction/#features","text":"Some of the features of the Cadence programming language are: type safety and a strong static type system resource-oriented programming resources are types that can only exist in one location at a time and cannot be copied, lost or stolen; Thus there is a concept of scarcity and ownership over these objects built-in pre-conditions and post-conditions for functions and transactions capability-based security access to objects is restricted only to the owner of the object and those who have a valid reference to it","title":"Features"},{"location":"knowledge_base/flow/cadence/introduction/#terminology","text":"Invalid : The invalid program is not even be allowed to run. The program error is detected and reported statically by the type checker. Run-time error : The erroneous program can run, but bad behavior will result in the execution of the program being aborted.","title":"Terminology"},{"location":"knowledge_base/flow/cadence/reference/","text":"Reference Comments Single-line comments in Cadence use // , multi-line comments use /* */ . // This is a comment on a single line. /* This is a comment which spans multiple lines. */ Documentation Comments Cadence has a specific way to create documentation comments. For single-line comments use /// , for multi-line comments use the syntax /** **/ . /// This is a single-line documentation comment /// You can keep them going. /** Multi-line Documentation Comment **/ Names Names can have letters, underscores and numbers, but can only start with any letter or underscore. By convention, variables, constants, and functions have lowercase names; and types have title-case names. // Valid: title-case // PersonID // Valid: with underscore // token_name // Valid: leading underscore and characters // _balance // Valid: leading underscore and numbers _8264 // Valid: characters and number // account2 // Invalid: leading number // 1something // Invalid: invalid character # _#1 // Invalid: various invalid characters // !@#$%^&* Types There are several built-in types for Cadence. Integer Cadence supports the following integer types: Int , Int8 , Int16 , Int32 , Int64 , Int128 , Int256 , UInt , UInt8 , UInt16 , UInt32 , UInt64 , UInt128 , UInt256 , Word8 , Word16 , Word32 , Word64 . The supported built-ins for this type family are: fun toString(): String fun toBigEndianBytes(): [Uint8] Uint8.max Uint8.min Fixed-Point Number Cadence supports the following fixed-point types: Fix64 , UFix64 . The supported built-ins for this type family are: fun toString(): String fun toBigEndianBytes(): [Uint8] Fix64.max Fix64.min Address The supported built-ins for this type family are: fun toString(): String fun toBytes(): [Uint8] String Strings are collections of characters. Strings can be used to work with text in a Unicode-compliant way. Strings are immutable. The supported built-ins for this type family are: let length: Int let utf8: [Uint8] fun concat(_ other: String): String fun slice(from: Int, upTo: Int): String fun decodeHex(): [Uint8] fun toLower(): String fun String.encodeHex(_ data: [Uin8]): String Arrays The supported built-ins for this type family are: let length: Int fun concat(_ array: T): T fun contains(_ element: T) Bool Specific to Variable size Array Functions: fun append(_ element: T): Void fun appendAll(_ array: T) Void fun insert(at index: Int, _ element: T): Void fun remove(at index: Int): T fun removeFirst(): T fun removeLast(): T Dictionary The supported built-ins for this type family are: let length: Int fun insert(key: K, _ value: V) V? fun remove(key: K): V? let keys: [K] let values: [V] fun containsKey(key: K): Bool Floating-Point There is no support for floating point numbers. These kind of numbers, not natural/cardinal, are handle by the Fixed-Point type. AnyStruct and AnyResource AnyStruct is the base type of all non-resource types, i.e., all non-resource types are a subtype of it. ( Int8 , String , Bool , struct , not type specific) AnyResource is the base type of all resource types. Optionals Optionals are values which can represent the absence of a value. An optional type is declared using the ? suffix for another type. For example, Int is a non-optional integer, and Int? is an optional integer, i.e. either an integer, or nothing. The value representing the absence of a value is nil . Nil-Coalescing Operator The nil-coalescing operator ?? returns the value inside an optional if it contains a value, or returns an alternative value if the optional has no value. let a : Int ? = nil let b : Int = a ?? 42 // `b` is 42, as `a` is nil Force Unwrap The force-unwrap operator ! returns the value inside an optional if it contains a value, or panics and aborts the execution if the optional has no value. Never Never can be used as the return type for functions that never return normally. For example, it is the return type of the panic function. Array Types Arrays either have a fixed or variable size. Fixed-size array types have the form [T; N] , where T is the element type, and N is the size of the array. For example, a fixed-size array of 3 Int8 elements has the type [Int8; 3] . Variable-size array types have the form [T] , where T is the element type. For example, the type [Int16] specifies a variable-size array of elements that have type Int16 . Dictionary Types Dictionary keys must be hashable and equatable, i.e., must implement the Hashable and Equatable interfaces. Most of the built-in types, like booleans and integers, are hashable and equatable, so can be used as keys in dictionaries. Swapping The binary swap operator <-> can be used to exchange the values of two variables. It is only allowed in a statement and is not allowed in expressions. var a = 1 var b = 2 var c = 3 // Invalid: The swap operation cannot be used in an expression. a < -> b < -> c // Instead, the intended swap must be // written in multiple statements. b < -> c a < -> b Argument Passing Behavior Arguments are passed to functions by value. Therefore, values passed to a function are unchanged in the caller's scope when that function returns. Function Preconditions and Postconditions The functions preconditions and postconditions are blocks of expressions that check input and outputs. fun factorial ( _ n : Int ) : Int { pre { // Require the parameter `n` to be greater than or equal to zero. // n >= 0 : \"factorial is only defined for integers greater than or equal to zero\" } post { // Ensure the result will be greater than or equal to 1. // result >= 1 : \"the result must be greater than or equal to 1\" } if n < 1 { return 1 } return n * factorial ( n - 1 ) } factorial ( 5 ) // is `120` // Run-time error: The given argument does not satisfy // the precondition `n >= 0` of the function, the program aborts. // factorial ( - 2 ) Switches As opposed to some languages, Cadence does not feature \"fall through\" in switch statements. Composite Types Composite types can only be declared within a contract and nowhere else. There are two types: Structures They are copied. They are value types. Resources They are moved. They are linear types. Equatable Interface struct interface Equatable { pub fun equals ( _ other : { Equatable }) : Bool } Hashable Interface struct interface Hashable : Equatable { pub hashValue : Int } Restricted Types Resources and structs can be used with the restricted types. For example, a Resource that implements the interface Balance can be accessed by accessing only the functions and/or fields available in Balance . resource interface HasCount { pub let count : Int } pub resource Counter : HasCount { pub var count : Int init ( count : Int ) { self . count = count } pub fun increment () { self . count = self . count + 1 } } let counter : @ Counter <- create Counter ( count : 42 ) counter . count // is `42` counter . increment () counter . count // is `43` // Move the resource in variable `counter` to a new variable `restrictedCounter`, // but typed with the restricted type `Counter{HasCount}`: // The variable may hold any `Counter`, but only the functionality // defined in the given restriction, the interface `HasCount`, may be accessed // let restrictedCounter : @ Counter { HasCount } <- counter // Invalid: Only functionality of restriction `Count` is available, // i.e. the read-only field `count`, but not the function `increment` of `Counter` // restrictedCounter . increment () Events Events can only be declared within contracts and cannot use resources as parameters or outputs. // Invalid: An event cannot be declared globally // event GlobalEvent ( field : Int ) pub contract Events { // Event with explicit argument labels // event BarEvent ( labelA fieldA : Int , labelB fieldB : Int ) // Invalid: A resource type is not allowed to be used // because it would be moved and lost // event ResourceEvent ( resourceField : @ Vault ) } Emitting Events To emit an event, the keyword emit is used. For example, emitting an event of the type Test(field: Int) : emit Test(1) . Core Events There are some core events built-in in Cadence. pub event AccountCreated(address: Address) pub event AccountKeyAdded(address: Address, publicKey: [UInt8]) pub event AccountKeyRemoved(address: Address, publicKey: [UInt8]) pub event AccountContractAdded(address: Address, codeHash: [UInt8], contract: String) pub event AccountContractUpdated(address: Address, codeHash: [UInt8], contract: String) pub event AccountContractRemoved(address: Address, codeHash: [UInt8], contract: String)","title":"Reference"},{"location":"knowledge_base/flow/cadence/reference/#reference","text":"","title":"Reference"},{"location":"knowledge_base/flow/cadence/reference/#comments","text":"Single-line comments in Cadence use // , multi-line comments use /* */ . // This is a comment on a single line. /* This is a comment which spans multiple lines. */","title":"Comments"},{"location":"knowledge_base/flow/cadence/reference/#documentation-comments","text":"Cadence has a specific way to create documentation comments. For single-line comments use /// , for multi-line comments use the syntax /** **/ . /// This is a single-line documentation comment /// You can keep them going. /** Multi-line Documentation Comment **/","title":"Documentation Comments"},{"location":"knowledge_base/flow/cadence/reference/#names","text":"Names can have letters, underscores and numbers, but can only start with any letter or underscore. By convention, variables, constants, and functions have lowercase names; and types have title-case names. // Valid: title-case // PersonID // Valid: with underscore // token_name // Valid: leading underscore and characters // _balance // Valid: leading underscore and numbers _8264 // Valid: characters and number // account2 // Invalid: leading number // 1something // Invalid: invalid character # _#1 // Invalid: various invalid characters // !@#$%^&*","title":"Names"},{"location":"knowledge_base/flow/cadence/reference/#types","text":"There are several built-in types for Cadence.","title":"Types"},{"location":"knowledge_base/flow/cadence/reference/#integer","text":"Cadence supports the following integer types: Int , Int8 , Int16 , Int32 , Int64 , Int128 , Int256 , UInt , UInt8 , UInt16 , UInt32 , UInt64 , UInt128 , UInt256 , Word8 , Word16 , Word32 , Word64 . The supported built-ins for this type family are: fun toString(): String fun toBigEndianBytes(): [Uint8] Uint8.max Uint8.min","title":"Integer"},{"location":"knowledge_base/flow/cadence/reference/#fixed-point-number","text":"Cadence supports the following fixed-point types: Fix64 , UFix64 . The supported built-ins for this type family are: fun toString(): String fun toBigEndianBytes(): [Uint8] Fix64.max Fix64.min","title":"Fixed-Point Number"},{"location":"knowledge_base/flow/cadence/reference/#address","text":"The supported built-ins for this type family are: fun toString(): String fun toBytes(): [Uint8]","title":"Address"},{"location":"knowledge_base/flow/cadence/reference/#string","text":"Strings are collections of characters. Strings can be used to work with text in a Unicode-compliant way. Strings are immutable. The supported built-ins for this type family are: let length: Int let utf8: [Uint8] fun concat(_ other: String): String fun slice(from: Int, upTo: Int): String fun decodeHex(): [Uint8] fun toLower(): String fun String.encodeHex(_ data: [Uin8]): String","title":"String"},{"location":"knowledge_base/flow/cadence/reference/#arrays","text":"The supported built-ins for this type family are: let length: Int fun concat(_ array: T): T fun contains(_ element: T) Bool Specific to Variable size Array Functions: fun append(_ element: T): Void fun appendAll(_ array: T) Void fun insert(at index: Int, _ element: T): Void fun remove(at index: Int): T fun removeFirst(): T fun removeLast(): T","title":"Arrays"},{"location":"knowledge_base/flow/cadence/reference/#dictionary","text":"The supported built-ins for this type family are: let length: Int fun insert(key: K, _ value: V) V? fun remove(key: K): V? let keys: [K] let values: [V] fun containsKey(key: K): Bool","title":"Dictionary"},{"location":"knowledge_base/flow/cadence/reference/#floating-point","text":"There is no support for floating point numbers. These kind of numbers, not natural/cardinal, are handle by the Fixed-Point type.","title":"Floating-Point"},{"location":"knowledge_base/flow/cadence/reference/#anystruct-and-anyresource","text":"AnyStruct is the base type of all non-resource types, i.e., all non-resource types are a subtype of it. ( Int8 , String , Bool , struct , not type specific) AnyResource is the base type of all resource types.","title":"AnyStruct and AnyResource"},{"location":"knowledge_base/flow/cadence/reference/#optionals","text":"Optionals are values which can represent the absence of a value. An optional type is declared using the ? suffix for another type. For example, Int is a non-optional integer, and Int? is an optional integer, i.e. either an integer, or nothing. The value representing the absence of a value is nil .","title":"Optionals"},{"location":"knowledge_base/flow/cadence/reference/#nil-coalescing-operator","text":"The nil-coalescing operator ?? returns the value inside an optional if it contains a value, or returns an alternative value if the optional has no value. let a : Int ? = nil let b : Int = a ?? 42 // `b` is 42, as `a` is nil","title":"Nil-Coalescing Operator"},{"location":"knowledge_base/flow/cadence/reference/#force-unwrap","text":"The force-unwrap operator ! returns the value inside an optional if it contains a value, or panics and aborts the execution if the optional has no value.","title":"Force Unwrap"},{"location":"knowledge_base/flow/cadence/reference/#never","text":"Never can be used as the return type for functions that never return normally. For example, it is the return type of the panic function.","title":"Never"},{"location":"knowledge_base/flow/cadence/reference/#array-types","text":"Arrays either have a fixed or variable size. Fixed-size array types have the form [T; N] , where T is the element type, and N is the size of the array. For example, a fixed-size array of 3 Int8 elements has the type [Int8; 3] . Variable-size array types have the form [T] , where T is the element type. For example, the type [Int16] specifies a variable-size array of elements that have type Int16 .","title":"Array Types"},{"location":"knowledge_base/flow/cadence/reference/#dictionary-types","text":"Dictionary keys must be hashable and equatable, i.e., must implement the Hashable and Equatable interfaces. Most of the built-in types, like booleans and integers, are hashable and equatable, so can be used as keys in dictionaries.","title":"Dictionary Types"},{"location":"knowledge_base/flow/cadence/reference/#swapping","text":"The binary swap operator <-> can be used to exchange the values of two variables. It is only allowed in a statement and is not allowed in expressions. var a = 1 var b = 2 var c = 3 // Invalid: The swap operation cannot be used in an expression. a < -> b < -> c // Instead, the intended swap must be // written in multiple statements. b < -> c a < -> b","title":"Swapping"},{"location":"knowledge_base/flow/cadence/reference/#argument-passing-behavior","text":"Arguments are passed to functions by value. Therefore, values passed to a function are unchanged in the caller's scope when that function returns.","title":"Argument Passing Behavior"},{"location":"knowledge_base/flow/cadence/reference/#function-preconditions-and-postconditions","text":"The functions preconditions and postconditions are blocks of expressions that check input and outputs. fun factorial ( _ n : Int ) : Int { pre { // Require the parameter `n` to be greater than or equal to zero. // n >= 0 : \"factorial is only defined for integers greater than or equal to zero\" } post { // Ensure the result will be greater than or equal to 1. // result >= 1 : \"the result must be greater than or equal to 1\" } if n < 1 { return 1 } return n * factorial ( n - 1 ) } factorial ( 5 ) // is `120` // Run-time error: The given argument does not satisfy // the precondition `n >= 0` of the function, the program aborts. // factorial ( - 2 )","title":"Function Preconditions and Postconditions"},{"location":"knowledge_base/flow/cadence/reference/#switches","text":"As opposed to some languages, Cadence does not feature \"fall through\" in switch statements.","title":"Switches"},{"location":"knowledge_base/flow/cadence/reference/#composite-types","text":"Composite types can only be declared within a contract and nowhere else. There are two types: Structures They are copied. They are value types. Resources They are moved. They are linear types.","title":"Composite Types"},{"location":"knowledge_base/flow/cadence/reference/#equatable-interface","text":"struct interface Equatable { pub fun equals ( _ other : { Equatable }) : Bool }","title":"Equatable Interface"},{"location":"knowledge_base/flow/cadence/reference/#hashable-interface","text":"struct interface Hashable : Equatable { pub hashValue : Int }","title":"Hashable Interface"},{"location":"knowledge_base/flow/cadence/reference/#restricted-types","text":"Resources and structs can be used with the restricted types. For example, a Resource that implements the interface Balance can be accessed by accessing only the functions and/or fields available in Balance . resource interface HasCount { pub let count : Int } pub resource Counter : HasCount { pub var count : Int init ( count : Int ) { self . count = count } pub fun increment () { self . count = self . count + 1 } } let counter : @ Counter <- create Counter ( count : 42 ) counter . count // is `42` counter . increment () counter . count // is `43` // Move the resource in variable `counter` to a new variable `restrictedCounter`, // but typed with the restricted type `Counter{HasCount}`: // The variable may hold any `Counter`, but only the functionality // defined in the given restriction, the interface `HasCount`, may be accessed // let restrictedCounter : @ Counter { HasCount } <- counter // Invalid: Only functionality of restriction `Count` is available, // i.e. the read-only field `count`, but not the function `increment` of `Counter` // restrictedCounter . increment ()","title":"Restricted Types"},{"location":"knowledge_base/flow/cadence/reference/#events","text":"Events can only be declared within contracts and cannot use resources as parameters or outputs. // Invalid: An event cannot be declared globally // event GlobalEvent ( field : Int ) pub contract Events { // Event with explicit argument labels // event BarEvent ( labelA fieldA : Int , labelB fieldB : Int ) // Invalid: A resource type is not allowed to be used // because it would be moved and lost // event ResourceEvent ( resourceField : @ Vault ) }","title":"Events"},{"location":"knowledge_base/flow/cadence/reference/#emitting-events","text":"To emit an event, the keyword emit is used. For example, emitting an event of the type Test(field: Int) : emit Test(1) .","title":"Emitting Events"},{"location":"knowledge_base/flow/cadence/reference/#core-events","text":"There are some core events built-in in Cadence. pub event AccountCreated(address: Address) pub event AccountKeyAdded(address: Address, publicKey: [UInt8]) pub event AccountKeyRemoved(address: Address, publicKey: [UInt8]) pub event AccountContractAdded(address: Address, codeHash: [UInt8], contract: String) pub event AccountContractUpdated(address: Address, codeHash: [UInt8], contract: String) pub event AccountContractRemoved(address: Address, codeHash: [UInt8], contract: String)","title":"Core Events"},{"location":"knowledge_base/flow/cadence/resources/","text":"Resources Resources are types that can exist only in one memory location at a time. At the end of a function which has resources in scope, resources must either be moved or destroyed . After it was destroyed , the resource can no longer be used. Moving a resource means either assigning it to a different constant or a variable, passing it as an argument to another function, or returning it from a function. After the resource was moved \u2014 e.g. assigned to a const , the previous reference to the resource is invalid and cannot be used anymore. To make the resource behavior clear and explicit, the prefix @ must be used in all type annotations dealing with resources, and the resource movement is denoted with the move operator \u2014 <- . // Resource definition. pub resource SomeResource { // The resource has a single field &#0151 the `value` integer. pub var value : Int // Define the resource initializer init ( value : Int ) { self . value = value } } // Resource is created using the `create` keyword. // Note that the const `a` has type of `@SomeResource`. let a : @ SomeResurce <- create SomeResource ( value : 2 ) // Resource is moved from const a to const b. // `a` can no longer be used to access the resource. let b <- a // Resource is destroyed. destroy b // Neither `a` or `b` can now be used to access the resource as it was destroyed. When a resource is returned from a function, the function caller has the responsibility to use the returned resource. // This function is invalid as it does not use the resource. pub fun invalid_use ( r : @ SomeResource ) { } // This function uses the resource by moving it to a new const. // This new const is used by being returned from the function. pub fun use ( res : @ SomeResource ) : @ SomeResource { let moved <- res // Note that the return call still uses the `move` operator. return <- moved } // Create a new resource. let a : @ SomeResource <- create SomeResource ( value : 3 ) // Move the function return value to a new const. // The const `a` can no longer be used to access the resource. let result <- use ( res : <- a ) // Destroy the resource. destroy result Resource can have a destructor, which is called when the resource is destroyed. pub resource SomeResource { destroy () { // Some logic here, e.g. decrement the variable indicating the number of resources in existence. } } Since the resource variables cannot be assigned to, there are two options to replace the values of resource variables \u2014 the swap operator ( <-> ) and the shift operator ( <- target <- ) pub resource SomeResource {} var x <- create SomeResource () var y <- create SomeResource () // Swap the resources. x < -> y // Alternatively, use the shift operator. // The shift operator moves the resource from `x` to `oldX`. // At the same time, `x` receives the value of the new resource. let oldX <- x <- create SomeResource () // oldX still needs to be used. Resources have an implicit unique identifier in the form of the predeclared public field uuid of type UInt64 . This field is incremented on each resource creation, and can never be the same for two resources, even if some of them were destroyed.","title":"Resources"},{"location":"knowledge_base/flow/cadence/resources/#resources","text":"Resources are types that can exist only in one memory location at a time. At the end of a function which has resources in scope, resources must either be moved or destroyed . After it was destroyed , the resource can no longer be used. Moving a resource means either assigning it to a different constant or a variable, passing it as an argument to another function, or returning it from a function. After the resource was moved \u2014 e.g. assigned to a const , the previous reference to the resource is invalid and cannot be used anymore. To make the resource behavior clear and explicit, the prefix @ must be used in all type annotations dealing with resources, and the resource movement is denoted with the move operator \u2014 <- . // Resource definition. pub resource SomeResource { // The resource has a single field &#0151 the `value` integer. pub var value : Int // Define the resource initializer init ( value : Int ) { self . value = value } } // Resource is created using the `create` keyword. // Note that the const `a` has type of `@SomeResource`. let a : @ SomeResurce <- create SomeResource ( value : 2 ) // Resource is moved from const a to const b. // `a` can no longer be used to access the resource. let b <- a // Resource is destroyed. destroy b // Neither `a` or `b` can now be used to access the resource as it was destroyed. When a resource is returned from a function, the function caller has the responsibility to use the returned resource. // This function is invalid as it does not use the resource. pub fun invalid_use ( r : @ SomeResource ) { } // This function uses the resource by moving it to a new const. // This new const is used by being returned from the function. pub fun use ( res : @ SomeResource ) : @ SomeResource { let moved <- res // Note that the return call still uses the `move` operator. return <- moved } // Create a new resource. let a : @ SomeResource <- create SomeResource ( value : 3 ) // Move the function return value to a new const. // The const `a` can no longer be used to access the resource. let result <- use ( res : <- a ) // Destroy the resource. destroy result Resource can have a destructor, which is called when the resource is destroyed. pub resource SomeResource { destroy () { // Some logic here, e.g. decrement the variable indicating the number of resources in existence. } } Since the resource variables cannot be assigned to, there are two options to replace the values of resource variables \u2014 the swap operator ( <-> ) and the shift operator ( <- target <- ) pub resource SomeResource {} var x <- create SomeResource () var y <- create SomeResource () // Swap the resources. x < -> y // Alternatively, use the shift operator. // The shift operator moves the resource from `x` to `oldX`. // At the same time, `x` receives the value of the new resource. let oldX <- x <- create SomeResource () // oldX still needs to be used. Resources have an implicit unique identifier in the form of the predeclared public field uuid of type UInt64 . This field is incremented on each resource creation, and can never be the same for two resources, even if some of them were destroyed.","title":"Resources"},{"location":"knowledge_base/flow/cadence/contracts/contracts/","text":"Contracts A contract in Cadence is a collection of definitions of interfaces, structs, resources, data (its state) and code (its functions). Contracts live in the contract storage area of an account and they can be added, updated and removed. Composite types \u2014 structs, resources, events and interfaces for these types have to be defined in a contract. However, there is an exception to this rule, since there is a number of native event types that are not defined in a contract, but built into the Cadence runtime itself. These events are emitted on user account creation, account key addition or removal, and contract deployment, update or removal. Contracts themselves are types, similar to composite types, but cannot be used as values, copied or moved like resources or structs. Example of a simple contract is given below: // Name of this contract is HelloWorld. pub contract HelloWorld { // Public constants &#0151 state fields. pub let description : String pub let greeting : String // init function, called when a contract is created. init ( description : String ) { self . description = description self . greeting = \"Hello World!\" } // Public function hello(), returning a string. // This function can be called by anyone importing the contract. pub fun hello () : String { return self . greeting } } This contract can be imported by Cadence scripts, transactions or other contracts at the beginning of the script, transaction or contract definition. // 0x01 is the account address where the contract is deployed. import HelloWorld from 0x01 // Invoke the hello() function of the imported contract. log ( HelloWorld . hello ()) Any number of contracts can be present on an account, which could include an arbitrary amount of data. Each contract has an implicit field \u2014 let account: AuthAccount , which is the account in which the contract is deployed. Deploying a Contract A new contract can be deployed to an account using the add function. An example transaction deploying a HelloWorld contract is given below: transaction ( code : String ) { prepare ( signer : AuthAccount ) { signer . contracts . add ( name : \"HelloWorld\" , code : code . decodeHex (), description : \"This is a new contract on an existing account\" ) } } When executing a transaction with this Cadence script, given a string argument that is a hex-encoded text of the Cadence contract, a new contract gets deployed to the account that signed the transaction. The contract's text can then be found in the code.HelloWorld account register. An event of type flow.AccountContractAdded is emitted as a result of contract deployment. This and a number of other events are built into the Cadence standard library . Another register is created in the given account, prefixed contract , followed by the \\x1F character and the contract name. This register holds the state of the contract. Updating a Contract The contract's updates are currently experimental. Updates to existing contracts are done using the update__experimental function. Contract updates are very similar to the contract deployments, except the init() function of the contract is not invoked on update. transaction ( code : String ) { prepare ( signer : AuthAccount ) { signer . contracts . update__experimental ( name : \"HelloWorld\" , code : code . decodeHex () ) } } There is a number of limitations to the types of updates that can be made to a contract. For example, it is valid to remove a field or change its access modifier from public to private, but it is invalid to add a field, or change its type. This is done in order to ensure data consistency, since changing a contract changes how the program interprets the data, but does not change the actual stored data. After a successful contract update, an event of flow.AccountContractUpdated type is emitted. Removing a Contract An existing contract can be deleted from an account using the remove function. Example of a transaction removing a contract: transaction ( name : String ) { prepare ( signer : AuthAccount ) { // name is the name of the contract that should be removed signer . contracts . remove ( name : name , ) } } After a successful contract removal, an event of flow.AccountContractRemoved type is emitted.","title":"Introduction"},{"location":"knowledge_base/flow/cadence/contracts/contracts/#contracts","text":"A contract in Cadence is a collection of definitions of interfaces, structs, resources, data (its state) and code (its functions). Contracts live in the contract storage area of an account and they can be added, updated and removed. Composite types \u2014 structs, resources, events and interfaces for these types have to be defined in a contract. However, there is an exception to this rule, since there is a number of native event types that are not defined in a contract, but built into the Cadence runtime itself. These events are emitted on user account creation, account key addition or removal, and contract deployment, update or removal. Contracts themselves are types, similar to composite types, but cannot be used as values, copied or moved like resources or structs. Example of a simple contract is given below: // Name of this contract is HelloWorld. pub contract HelloWorld { // Public constants &#0151 state fields. pub let description : String pub let greeting : String // init function, called when a contract is created. init ( description : String ) { self . description = description self . greeting = \"Hello World!\" } // Public function hello(), returning a string. // This function can be called by anyone importing the contract. pub fun hello () : String { return self . greeting } } This contract can be imported by Cadence scripts, transactions or other contracts at the beginning of the script, transaction or contract definition. // 0x01 is the account address where the contract is deployed. import HelloWorld from 0x01 // Invoke the hello() function of the imported contract. log ( HelloWorld . hello ()) Any number of contracts can be present on an account, which could include an arbitrary amount of data. Each contract has an implicit field \u2014 let account: AuthAccount , which is the account in which the contract is deployed.","title":"Contracts"},{"location":"knowledge_base/flow/cadence/contracts/contracts/#deploying-a-contract","text":"A new contract can be deployed to an account using the add function. An example transaction deploying a HelloWorld contract is given below: transaction ( code : String ) { prepare ( signer : AuthAccount ) { signer . contracts . add ( name : \"HelloWorld\" , code : code . decodeHex (), description : \"This is a new contract on an existing account\" ) } } When executing a transaction with this Cadence script, given a string argument that is a hex-encoded text of the Cadence contract, a new contract gets deployed to the account that signed the transaction. The contract's text can then be found in the code.HelloWorld account register. An event of type flow.AccountContractAdded is emitted as a result of contract deployment. This and a number of other events are built into the Cadence standard library . Another register is created in the given account, prefixed contract , followed by the \\x1F character and the contract name. This register holds the state of the contract.","title":"Deploying a Contract"},{"location":"knowledge_base/flow/cadence/contracts/contracts/#updating-a-contract","text":"The contract's updates are currently experimental. Updates to existing contracts are done using the update__experimental function. Contract updates are very similar to the contract deployments, except the init() function of the contract is not invoked on update. transaction ( code : String ) { prepare ( signer : AuthAccount ) { signer . contracts . update__experimental ( name : \"HelloWorld\" , code : code . decodeHex () ) } } There is a number of limitations to the types of updates that can be made to a contract. For example, it is valid to remove a field or change its access modifier from public to private, but it is invalid to add a field, or change its type. This is done in order to ensure data consistency, since changing a contract changes how the program interprets the data, but does not change the actual stored data. After a successful contract update, an event of flow.AccountContractUpdated type is emitted.","title":"Updating a Contract"},{"location":"knowledge_base/flow/cadence/contracts/contracts/#removing-a-contract","text":"An existing contract can be deleted from an account using the remove function. Example of a transaction removing a contract: transaction ( name : String ) { prepare ( signer : AuthAccount ) { // name is the name of the contract that should be removed signer . contracts . remove ( name : name , ) } } After a successful contract removal, an event of flow.AccountContractRemoved type is emitted.","title":"Removing a Contract"},{"location":"knowledge_base/flow/cadence/contracts/flow_contracts/","text":"Flow Contracts There are core Cadence contracts that are deployed on the Flow network as soon as it is bootstrapped. These contracts are essential to the normal operation of the Flow network. FungibleToken The Flow Fungible Token standard is defined in 0xf233dcee88fe0abe for the mainnet. It defines the FungibleToken contract interaface, to which all fungible token contracts need to conform to. The Vault resource is also defined in the same contract, which is the resource that each account needs to have in storage in order to own tokens. Resources and interfaces defined here allow sending and receiving of tokens peer-to-peer, by withdrawing tokens from one users Vault and depositing them to another user's Vault. FlowToken Implementation of the FungibleToken interface for the FLOW token. FlowServiceAccount The Flow Service Account is an account like any other on Flow except it is responsible for managing core network operations. The Service Account can be referred to as the keeper of core network parameters which will be managed algorithmically over time but are currently set by a group of core contributors to ensure ease of updates to the network in this early stage of its development. FlowEpoch Top-level smart contract that manages the lifecycle of epochs . Epochs are the smallest unit of time during which the set of network operators is either static or can decrease due to nodes election or malicious nodes presence. Nodes may only leave or join the network at epoch boundaries. The list of participating nodes is kept in the Identity Table . Epochs have three phases \u2014 staking , setup and committed . Staking Phase During the staking phase (or staking Auction), nodes submit staking requests for the next epoch. At the end of this phase, the identity table for the next epoch is determined. Setup Phase During the setup phase , the participants are preparing for the next epoch. Collection nodes submit votes for their cluster's root quorum certificate and consensus nodes run the distributed key generation protocol (DKG) to set up the random beacon. Committed Phase When the committed phase begins, the network is fully prepared to move to the next epoch. Failure to enter this phase before transitioning to the next epoch would be a critical failure and would cause the chain to halt. FlowFees Contract that manages the fee Vault and deposits and withdrawal of fees to and from the fee vault. FlowStorageFees Storage capacity of an account determines how much storage on chain it can use. There is a set of minimum amount of tokens reserved for storage capacity that is paid during account creation, by the creator. If any transaction that results in account's storage use greater that the storage capacity, the transaction will fail. FlowClusterQC This contract manages the process of collecting votes for the root Quorum Certificate (QC) of the upcoming epoch for all Collection node clusters assigned for the next epoch. During the setup phase of the epoch, collection nodes in a specific cluster generate a root block for the cluster. The nodes then need to submit a vote for the root block. Once enough cluster nodes have voted with the same unique vote, the cluster is considered complete. Once all clusters are complete, the QC is complete. FlowDKG This contract manages the process of generating a group key with the participation of all consensus nodes for the upcoming epoch (DKG). FlowIDTableStaking The Flow ID Table and Staking contract manages the node operators' and delegators' information and Flow tokens that are staked as part of the protocol. Nodes submit their stakes during the staking phase of the epoch. When staking, nodes receive an object they can use to stake, unstake and withdraw rewards. Tokens are held in multiple token buckets depending on their status \u2014 staked, unstaking, unstaked or rewarded. This is also where the Delegator functionality, which manages delegation of FLOW to node operators, is specified. FlowStakingCollection This contract defines a collection for staking and delegating objects which allows users to stake and delegate to as many nodes as they want. LockedTokens This contract implements the functionality required to manage FLOW buyers locked tokens from the token sale. Each token holder gets two accounts. The first account is the locked token account, jointly controlled by the user and the token administrator. Token administrator cannot interact with the account without approval from the token holder except for depositing additional tokens, or to unlock existing tokens at the appropriate milestones. Second account is the unlocked user account, which is in full possession of the user. This account stores a capability which allows the account owner to withdraw tokens when they become unlocked, and also to perform staking operations with the locked tokens. StakingProxy This contract defines an interface for node stakers to use to be able to perform common staking actions. Contract Addresses FungibleToken Mainnet: 0xf233dcee88fe0abe NonFungibleToken Mainnet: 0x1d7e57aa55817448 DapperUtilityCoin Mainnet: 0xead892083b3e2c6c","title":"Flow Contracts"},{"location":"knowledge_base/flow/cadence/contracts/flow_contracts/#flow-contracts","text":"There are core Cadence contracts that are deployed on the Flow network as soon as it is bootstrapped. These contracts are essential to the normal operation of the Flow network.","title":"Flow Contracts"},{"location":"knowledge_base/flow/cadence/contracts/flow_contracts/#fungibletoken","text":"The Flow Fungible Token standard is defined in 0xf233dcee88fe0abe for the mainnet. It defines the FungibleToken contract interaface, to which all fungible token contracts need to conform to. The Vault resource is also defined in the same contract, which is the resource that each account needs to have in storage in order to own tokens. Resources and interfaces defined here allow sending and receiving of tokens peer-to-peer, by withdrawing tokens from one users Vault and depositing them to another user's Vault.","title":"FungibleToken"},{"location":"knowledge_base/flow/cadence/contracts/flow_contracts/#flowtoken","text":"Implementation of the FungibleToken interface for the FLOW token.","title":"FlowToken"},{"location":"knowledge_base/flow/cadence/contracts/flow_contracts/#flowserviceaccount","text":"The Flow Service Account is an account like any other on Flow except it is responsible for managing core network operations. The Service Account can be referred to as the keeper of core network parameters which will be managed algorithmically over time but are currently set by a group of core contributors to ensure ease of updates to the network in this early stage of its development.","title":"FlowServiceAccount"},{"location":"knowledge_base/flow/cadence/contracts/flow_contracts/#flowepoch","text":"Top-level smart contract that manages the lifecycle of epochs . Epochs are the smallest unit of time during which the set of network operators is either static or can decrease due to nodes election or malicious nodes presence. Nodes may only leave or join the network at epoch boundaries. The list of participating nodes is kept in the Identity Table . Epochs have three phases \u2014 staking , setup and committed .","title":"FlowEpoch"},{"location":"knowledge_base/flow/cadence/contracts/flow_contracts/#staking-phase","text":"During the staking phase (or staking Auction), nodes submit staking requests for the next epoch. At the end of this phase, the identity table for the next epoch is determined.","title":"Staking Phase"},{"location":"knowledge_base/flow/cadence/contracts/flow_contracts/#setup-phase","text":"During the setup phase , the participants are preparing for the next epoch. Collection nodes submit votes for their cluster's root quorum certificate and consensus nodes run the distributed key generation protocol (DKG) to set up the random beacon.","title":"Setup Phase"},{"location":"knowledge_base/flow/cadence/contracts/flow_contracts/#committed-phase","text":"When the committed phase begins, the network is fully prepared to move to the next epoch. Failure to enter this phase before transitioning to the next epoch would be a critical failure and would cause the chain to halt.","title":"Committed Phase"},{"location":"knowledge_base/flow/cadence/contracts/flow_contracts/#flowfees","text":"Contract that manages the fee Vault and deposits and withdrawal of fees to and from the fee vault.","title":"FlowFees"},{"location":"knowledge_base/flow/cadence/contracts/flow_contracts/#flowstoragefees","text":"Storage capacity of an account determines how much storage on chain it can use. There is a set of minimum amount of tokens reserved for storage capacity that is paid during account creation, by the creator. If any transaction that results in account's storage use greater that the storage capacity, the transaction will fail.","title":"FlowStorageFees"},{"location":"knowledge_base/flow/cadence/contracts/flow_contracts/#flowclusterqc","text":"This contract manages the process of collecting votes for the root Quorum Certificate (QC) of the upcoming epoch for all Collection node clusters assigned for the next epoch. During the setup phase of the epoch, collection nodes in a specific cluster generate a root block for the cluster. The nodes then need to submit a vote for the root block. Once enough cluster nodes have voted with the same unique vote, the cluster is considered complete. Once all clusters are complete, the QC is complete.","title":"FlowClusterQC"},{"location":"knowledge_base/flow/cadence/contracts/flow_contracts/#flowdkg","text":"This contract manages the process of generating a group key with the participation of all consensus nodes for the upcoming epoch (DKG).","title":"FlowDKG"},{"location":"knowledge_base/flow/cadence/contracts/flow_contracts/#flowidtablestaking","text":"The Flow ID Table and Staking contract manages the node operators' and delegators' information and Flow tokens that are staked as part of the protocol. Nodes submit their stakes during the staking phase of the epoch. When staking, nodes receive an object they can use to stake, unstake and withdraw rewards. Tokens are held in multiple token buckets depending on their status \u2014 staked, unstaking, unstaked or rewarded. This is also where the Delegator functionality, which manages delegation of FLOW to node operators, is specified.","title":"FlowIDTableStaking"},{"location":"knowledge_base/flow/cadence/contracts/flow_contracts/#flowstakingcollection","text":"This contract defines a collection for staking and delegating objects which allows users to stake and delegate to as many nodes as they want.","title":"FlowStakingCollection"},{"location":"knowledge_base/flow/cadence/contracts/flow_contracts/#lockedtokens","text":"This contract implements the functionality required to manage FLOW buyers locked tokens from the token sale. Each token holder gets two accounts. The first account is the locked token account, jointly controlled by the user and the token administrator. Token administrator cannot interact with the account without approval from the token holder except for depositing additional tokens, or to unlock existing tokens at the appropriate milestones. Second account is the unlocked user account, which is in full possession of the user. This account stores a capability which allows the account owner to withdraw tokens when they become unlocked, and also to perform staking operations with the locked tokens.","title":"LockedTokens"},{"location":"knowledge_base/flow/cadence/contracts/flow_contracts/#stakingproxy","text":"This contract defines an interface for node stakers to use to be able to perform common staking actions.","title":"StakingProxy"},{"location":"knowledge_base/flow/cadence/contracts/flow_contracts/#contract-addresses","text":"FungibleToken Mainnet: 0xf233dcee88fe0abe NonFungibleToken Mainnet: 0x1d7e57aa55817448 DapperUtilityCoin Mainnet: 0xead892083b3e2c6c","title":"Contract Addresses"},{"location":"knowledge_base/flow/examples/example/","text":"Flow Localnet TODO: describe this better Starting a Localnet git clone build crypto cd internal/localnet make init make start - docker-compose (...) Indexing Localnet's past Sporks with Flow DPS git clone DPS build dps README flow-dps-index (...) Using Flow Insights to inspect some Registers git clone git checkout branch flow-insights (...)","title":"Flow Localnet"},{"location":"knowledge_base/flow/examples/example/#flow-localnet","text":"TODO: describe this better","title":"Flow Localnet"},{"location":"knowledge_base/flow/examples/example/#starting-a-localnet","text":"git clone build crypto cd internal/localnet make init make start - docker-compose (...)","title":"Starting a Localnet"},{"location":"knowledge_base/flow/examples/example/#indexing-localnets-past-sporks-with-flow-dps","text":"git clone DPS build dps README flow-dps-index (...)","title":"Indexing Localnet's past Sporks with Flow DPS"},{"location":"knowledge_base/flow/examples/example/#using-flow-insights-to-inspect-some-registers","text":"git clone git checkout branch flow-insights (...)","title":"Using Flow Insights to inspect some Registers"},{"location":"knowledge_base/flow/examples/flow-go-sdk/","text":"Flow Go SDK Flow Go SDK allows users to interact with the Flow Access API. This allows Go code applications to send requests directly to the network. The SDK has the ability to send transactions, scripts and get information about the state of the network. Sending a Transaction Flow SDK exposes the method SendTransaction to send transactions to the Flow Network. Sending a transaction In this example the amount of 100 FLOW tokens will be sent between the service account and the RECEIVER_ACCOUNT . In the prepare phase of the transaction, the reference to the sender's vault is retrieved and the specified amount of tokens is withdrawn to a temporary vault. In the execute phase of the transaction, the FungibleToken.Receiver capability is used to get access to the deposit function of the receiver's vault. The tokens are then deposited to the receiver from the temporary vault. /// 0xFUNGIBLE_TOKEN and 0xFLOW_TOKEN are place for the actual addresses of the contracts that differ from network to network. import FungibleToken from 0xF UNGIBLE_TOKEN import FlowToken from 0xF LOW_TOKEN transaction () { let sentVault : @ FungibleToken . Vault prepare ( signer : AuthAccount ) { // Get a reference to the signer's stored vault let vaultRef = signer . borrow <& FlowToken . Vault > ( from : / storage / flowTokenVault ) ?? panic ( \"Could not borrow reference to the owner's Vault!\" ) // Withdraw tokens from the signer's stored vault self . sentVault <- vaultRef . withdraw ( amount : 100 ) } execute { // Get a reference to the recipient's Receiver let receiverRef = getAccount ( 0 xRECEIVER_ACCOUNT ) . getCapability ( / public / flowTokenReceiver ) . borrow <& { FungibleToken . Receiver } > () ?? panic ( \"Could not borrow receiver reference to the recipient's Vault\" ) // Deposit the withdrawn tokens in the recipient's receiver receiverRef . deposit ( from : <- self . sentVault ) } } Using the SDK the script connects to the access node using client.New(ADDRESS) , then gets the private key and the first account key of the service account and uses it to pay and sign the transaction. Finally the transaction is sent. package main import ( \"context\" \"github.com/onflow/flow-go-sdk\" \"github.com/onflow/flow-go-sdk/crypto\" ) var ( cadenceScript = \"Cadence transaction code describing token transfer...\" serviceAccountAdress = \"0xADDRESS\" serviceAccountPrivateKey = \"privateKey\" ) func main () { ctx := context . Background () cli , err := client . New ( \"127.0.0.1\" , grpc . WithInsecure ()) if err != nil { panic ( err ) } privateKey , err := crypto . DecodePrivateKeyHex ( crypto . ECDSA_P256 , serviceAccountPrivateKey ) if err != nil { panic ( err ) } addr := flow . HexToAddress ( serviceAccountAdress ) acc , err := cli . GetAccount ( ctx , addr ) if err != nil { panic ( err ) } accountKey := acc . Keys [ 0 ] signer := crypto . NewInMemorySigner ( privateKey , accountKey . HashAlgo ) tx := flow . NewTransaction (). SetPayer ( addr ). SetProposalKey ( addr , accountKey . Index , accountKey . SequenceNumber ). SetScript ([] byte ( cadenceScript )) err := tx . SignEnvelope ( addr , accountKey . Index , signer ) if err != nil { panic ( err ) } err := cli . SendTransaction ( ctx , tx ) if err != nil { panic ( err ) } } Sending a Script Flow SDK exposes the method ExecuteScriptAtLatestBlock to execute a script on the latest block. It also offers two more functions to execute scripts on a block at a defined height or referenced by its id: ExecuteScriptAtBlockHeight and ExecuteScriptAtBlockID respectavelly. Flow Account Balance To get the current FLOW token balance from an account the following cadence script should be executed. In this script the ACCOUNT placeholder is the account address of the target balance. import FlowToken from 0xF LOW_TOKEN pub fun main () { let account = getAccount ( 0xACC OUNT ) let balanceRef = account . getCapability <& FlowToken . Vault { FlowToken . Balance } > ( / public / flowTokenBalance ) . borrow () ?? panic ( \"Could not borrow a reference to the account balance\" ) return balanceRef . balance } To execute the script from the Go SDK first it requires to create a new connection to an access node. This is done using client.New(ACCESS_NODE_ADDRESS) . With this connection the script can be executed using ExecuteScriptAtLatestBlock(CONTEXT, CADENCE_SCRIPT, ARGUMENTS) . package main import ( \"context\" \"fmt\" \"github.com/onflow/flow-go-sdk\" ) var ( cadenceScript = \"Cadence script code describing retrieving token balance...\" ) func main () { ctx := context . Background () cli , err := client . New ( \"127.0.0.1\" , grpc . WithInsecure ()) if err != nil { panic ( err ) } script := [] byte ( cadenceScript ) value , err := cli . ExecuteScriptAtLatestBlock ( ctx , script , nil ) if err != nil { panic ( err ) } fmt . Printf ( \"\\nValue: %s\" , value . String ()) }","title":"Go SDK"},{"location":"knowledge_base/flow/examples/flow-go-sdk/#flow-go-sdk","text":"Flow Go SDK allows users to interact with the Flow Access API. This allows Go code applications to send requests directly to the network. The SDK has the ability to send transactions, scripts and get information about the state of the network.","title":"Flow Go SDK"},{"location":"knowledge_base/flow/examples/flow-go-sdk/#sending-a-transaction","text":"Flow SDK exposes the method SendTransaction to send transactions to the Flow Network.","title":"Sending a Transaction"},{"location":"knowledge_base/flow/examples/flow-go-sdk/#sending-a-transaction_1","text":"In this example the amount of 100 FLOW tokens will be sent between the service account and the RECEIVER_ACCOUNT . In the prepare phase of the transaction, the reference to the sender's vault is retrieved and the specified amount of tokens is withdrawn to a temporary vault. In the execute phase of the transaction, the FungibleToken.Receiver capability is used to get access to the deposit function of the receiver's vault. The tokens are then deposited to the receiver from the temporary vault. /// 0xFUNGIBLE_TOKEN and 0xFLOW_TOKEN are place for the actual addresses of the contracts that differ from network to network. import FungibleToken from 0xF UNGIBLE_TOKEN import FlowToken from 0xF LOW_TOKEN transaction () { let sentVault : @ FungibleToken . Vault prepare ( signer : AuthAccount ) { // Get a reference to the signer's stored vault let vaultRef = signer . borrow <& FlowToken . Vault > ( from : / storage / flowTokenVault ) ?? panic ( \"Could not borrow reference to the owner's Vault!\" ) // Withdraw tokens from the signer's stored vault self . sentVault <- vaultRef . withdraw ( amount : 100 ) } execute { // Get a reference to the recipient's Receiver let receiverRef = getAccount ( 0 xRECEIVER_ACCOUNT ) . getCapability ( / public / flowTokenReceiver ) . borrow <& { FungibleToken . Receiver } > () ?? panic ( \"Could not borrow receiver reference to the recipient's Vault\" ) // Deposit the withdrawn tokens in the recipient's receiver receiverRef . deposit ( from : <- self . sentVault ) } } Using the SDK the script connects to the access node using client.New(ADDRESS) , then gets the private key and the first account key of the service account and uses it to pay and sign the transaction. Finally the transaction is sent. package main import ( \"context\" \"github.com/onflow/flow-go-sdk\" \"github.com/onflow/flow-go-sdk/crypto\" ) var ( cadenceScript = \"Cadence transaction code describing token transfer...\" serviceAccountAdress = \"0xADDRESS\" serviceAccountPrivateKey = \"privateKey\" ) func main () { ctx := context . Background () cli , err := client . New ( \"127.0.0.1\" , grpc . WithInsecure ()) if err != nil { panic ( err ) } privateKey , err := crypto . DecodePrivateKeyHex ( crypto . ECDSA_P256 , serviceAccountPrivateKey ) if err != nil { panic ( err ) } addr := flow . HexToAddress ( serviceAccountAdress ) acc , err := cli . GetAccount ( ctx , addr ) if err != nil { panic ( err ) } accountKey := acc . Keys [ 0 ] signer := crypto . NewInMemorySigner ( privateKey , accountKey . HashAlgo ) tx := flow . NewTransaction (). SetPayer ( addr ). SetProposalKey ( addr , accountKey . Index , accountKey . SequenceNumber ). SetScript ([] byte ( cadenceScript )) err := tx . SignEnvelope ( addr , accountKey . Index , signer ) if err != nil { panic ( err ) } err := cli . SendTransaction ( ctx , tx ) if err != nil { panic ( err ) } }","title":"Sending a transaction"},{"location":"knowledge_base/flow/examples/flow-go-sdk/#sending-a-script","text":"Flow SDK exposes the method ExecuteScriptAtLatestBlock to execute a script on the latest block. It also offers two more functions to execute scripts on a block at a defined height or referenced by its id: ExecuteScriptAtBlockHeight and ExecuteScriptAtBlockID respectavelly.","title":"Sending a Script"},{"location":"knowledge_base/flow/examples/flow-go-sdk/#flow-account-balance","text":"To get the current FLOW token balance from an account the following cadence script should be executed. In this script the ACCOUNT placeholder is the account address of the target balance. import FlowToken from 0xF LOW_TOKEN pub fun main () { let account = getAccount ( 0xACC OUNT ) let balanceRef = account . getCapability <& FlowToken . Vault { FlowToken . Balance } > ( / public / flowTokenBalance ) . borrow () ?? panic ( \"Could not borrow a reference to the account balance\" ) return balanceRef . balance } To execute the script from the Go SDK first it requires to create a new connection to an access node. This is done using client.New(ACCESS_NODE_ADDRESS) . With this connection the script can be executed using ExecuteScriptAtLatestBlock(CONTEXT, CADENCE_SCRIPT, ARGUMENTS) . package main import ( \"context\" \"fmt\" \"github.com/onflow/flow-go-sdk\" ) var ( cadenceScript = \"Cadence script code describing retrieving token balance...\" ) func main () { ctx := context . Background () cli , err := client . New ( \"127.0.0.1\" , grpc . WithInsecure ()) if err != nil { panic ( err ) } script := [] byte ( cadenceScript ) value , err := cli . ExecuteScriptAtLatestBlock ( ctx , script , nil ) if err != nil { panic ( err ) } fmt . Printf ( \"\\nValue: %s\" , value . String ()) }","title":"Flow Account Balance"},{"location":"knowledge_base/flow/flow_virtual_machine/case_study/","text":"Case study - execution of a Cadence script This section is a detailed walkthrough for a simple scenario, in which a simple Cadence script is executed. It includes the creation of a Flow Virtual machine, Cadence interpreter and runtime, as well as the interaction with the underlying storage (ledger) provided by the ScriptEnv . // In this example 0x01 represents the address where the contract is located. import FungibleToken from 0x01 pub fun main () : UFix64 { return 1.0 } To execute this, we first need to create the FVM interpreter runtime in order to execute Cadence scripts and procedures. Default implementation of the Cadence interpreter runtime can be found in Cadence runtime/runtime.go . Then, the Flow Virtual Machine is created. runtime := fvm . NewInterpreterRuntime () vm := fvm . NewVirtualMachine ( runtime ) Now, we will create a fvm.ScriptProcedure . A Procedure in the context of the FVM is an operation that reads or updates the ledger state. Both Cadence scripts and transactions are procedures, via the ScriptProcedure and TransactionProcedure types. // Create a fvm.ScriptProcedure with the given script. // script argument is a byte slice with the Cadence script text. procedure := fvm . Script ( script ) // The fvm.ScriptProcedure is then run using the Flow Virtual Marchine. err = vm . Run ( ctx , procedure , view , programs ) When using the FVM to run a procedure, it invokes the procedure's Run() method. During the preparation for running the script, first a new ScriptEnv is created, and then the ExecuteScript() method of the runtime is executed. func ( i ScriptInvocator ) Process ( vm * VirtualMachine , ctx Context , proc * ScriptProcedure , sth * state . StateHolder , programs * programs . Programs ) error { env := NewScriptEnvironment ( ctx , vm , sth , programs ) value , err := vm . Runtime . ExecuteScript ( // ... runtime . Context { Interface : env , // ... }, ) } The ScriptEnv created earlier is used as the runtime storage provider. func ( r * interpreterRuntime ) ExecuteScript ( script Script , context Context ) ( cadence . Value , error ) { context . InitializeCodesAndPrograms () runtimeStorage := newRuntimeStorage ( context . Interface ) // ... } This runtime storage handler is in charge of I/O operations when it comes to the ledger. The Cadence runtime then parses the provided script and check its correctness (via the parseAndCheckProgram() method of the runtime). One of the things that the check does is look at any imports found, and it uses the interpreter's GetAccountContractCode() method to load it. As the interpreter's storage provider was previously initialized to ScriptEnv , it's effectively invoking ScriptEnv s GetAccountContractCode() method. GetAccountContractCode() checks whether the account is frozen, and, if not, tries to read the contract code from the appropriate location. Skipping few levels down the call stack, GetAccountContractCode() will result in a call to fvm/state/accounts.go getValue() method. The code of getValue() shown below demonstrates two distinct solutions for resolving parameters for the register getter, the difference being the isController boolean flag. func ( a * StatefulAccounts ) getValue ( address flow . Address , isController bool , key string ) ( flow . RegisterValue , error ) { if isController { return a . stateHolder . State (). Get ( string ( address . Bytes ()), string ( address . Bytes ()), key ) } return a . stateHolder . State (). Get ( string ( address . Bytes ()), \"\" , key ) } The State s Get() method shown there can be simplified to this: func ( s * State ) Get ( owner , controller , key string ) ( flow . RegisterValue , error ) { return s . view . Get ( owner , controller , key ); } To get to the origin of the StateHolder / State objects, we need to look back at the FVM creation. For instance, in the Flow DPS invoker code, the state provider, named view below, is created like this: read := readRegister ( i . index , i . cache , height ) view := delta . NewView ( read ) vm . Run ( ctx , procedure , view , programs ) What's important to note is that the readRegister function returns a GetRegisterFunc , in charge of returning value for an appropriate register. In the Flow DPS ecosystem, it in essence means translating the owner , controller and key combination into a ledger path and reading it from the DPS index at the appropriate height. type GetRegisterFunc func ( owner , controller , key string ) ( flow . RegisterValue , error ) What's done with the ledger.Value / flow.RegisterValue later depends on the context. In the case of previous script example, the contents of the register /0/0x01/1/0x01/2/code.FungibleToken are shown as plaintext and can be imported to the Cadence program that is to be executed. The data in the register itself may be formatted differently based on the specific register, and it is up to the caller to unpack it correctly. For instance, the code.FungibleToken register contains a plaintext Cadence script. On the other hand, the contract_names register contains a CBOR-encoded array of strings. Registers can also contain complex types such as dictionaries, structs or arrays.","title":"Example"},{"location":"knowledge_base/flow/flow_virtual_machine/case_study/#case-study-execution-of-a-cadence-script","text":"This section is a detailed walkthrough for a simple scenario, in which a simple Cadence script is executed. It includes the creation of a Flow Virtual machine, Cadence interpreter and runtime, as well as the interaction with the underlying storage (ledger) provided by the ScriptEnv . // In this example 0x01 represents the address where the contract is located. import FungibleToken from 0x01 pub fun main () : UFix64 { return 1.0 } To execute this, we first need to create the FVM interpreter runtime in order to execute Cadence scripts and procedures. Default implementation of the Cadence interpreter runtime can be found in Cadence runtime/runtime.go . Then, the Flow Virtual Machine is created. runtime := fvm . NewInterpreterRuntime () vm := fvm . NewVirtualMachine ( runtime ) Now, we will create a fvm.ScriptProcedure . A Procedure in the context of the FVM is an operation that reads or updates the ledger state. Both Cadence scripts and transactions are procedures, via the ScriptProcedure and TransactionProcedure types. // Create a fvm.ScriptProcedure with the given script. // script argument is a byte slice with the Cadence script text. procedure := fvm . Script ( script ) // The fvm.ScriptProcedure is then run using the Flow Virtual Marchine. err = vm . Run ( ctx , procedure , view , programs ) When using the FVM to run a procedure, it invokes the procedure's Run() method. During the preparation for running the script, first a new ScriptEnv is created, and then the ExecuteScript() method of the runtime is executed. func ( i ScriptInvocator ) Process ( vm * VirtualMachine , ctx Context , proc * ScriptProcedure , sth * state . StateHolder , programs * programs . Programs ) error { env := NewScriptEnvironment ( ctx , vm , sth , programs ) value , err := vm . Runtime . ExecuteScript ( // ... runtime . Context { Interface : env , // ... }, ) } The ScriptEnv created earlier is used as the runtime storage provider. func ( r * interpreterRuntime ) ExecuteScript ( script Script , context Context ) ( cadence . Value , error ) { context . InitializeCodesAndPrograms () runtimeStorage := newRuntimeStorage ( context . Interface ) // ... } This runtime storage handler is in charge of I/O operations when it comes to the ledger. The Cadence runtime then parses the provided script and check its correctness (via the parseAndCheckProgram() method of the runtime). One of the things that the check does is look at any imports found, and it uses the interpreter's GetAccountContractCode() method to load it. As the interpreter's storage provider was previously initialized to ScriptEnv , it's effectively invoking ScriptEnv s GetAccountContractCode() method. GetAccountContractCode() checks whether the account is frozen, and, if not, tries to read the contract code from the appropriate location. Skipping few levels down the call stack, GetAccountContractCode() will result in a call to fvm/state/accounts.go getValue() method. The code of getValue() shown below demonstrates two distinct solutions for resolving parameters for the register getter, the difference being the isController boolean flag. func ( a * StatefulAccounts ) getValue ( address flow . Address , isController bool , key string ) ( flow . RegisterValue , error ) { if isController { return a . stateHolder . State (). Get ( string ( address . Bytes ()), string ( address . Bytes ()), key ) } return a . stateHolder . State (). Get ( string ( address . Bytes ()), \"\" , key ) } The State s Get() method shown there can be simplified to this: func ( s * State ) Get ( owner , controller , key string ) ( flow . RegisterValue , error ) { return s . view . Get ( owner , controller , key ); } To get to the origin of the StateHolder / State objects, we need to look back at the FVM creation. For instance, in the Flow DPS invoker code, the state provider, named view below, is created like this: read := readRegister ( i . index , i . cache , height ) view := delta . NewView ( read ) vm . Run ( ctx , procedure , view , programs ) What's important to note is that the readRegister function returns a GetRegisterFunc , in charge of returning value for an appropriate register. In the Flow DPS ecosystem, it in essence means translating the owner , controller and key combination into a ledger path and reading it from the DPS index at the appropriate height. type GetRegisterFunc func ( owner , controller , key string ) ( flow . RegisterValue , error ) What's done with the ledger.Value / flow.RegisterValue later depends on the context. In the case of previous script example, the contents of the register /0/0x01/1/0x01/2/code.FungibleToken are shown as plaintext and can be imported to the Cadence program that is to be executed. The data in the register itself may be formatted differently based on the specific register, and it is up to the caller to unpack it correctly. For instance, the code.FungibleToken register contains a plaintext Cadence script. On the other hand, the contract_names register contains a CBOR-encoded array of strings. Registers can also contain complex types such as dictionaries, structs or arrays.","title":"Case study - execution of a Cadence script"},{"location":"knowledge_base/flow/flow_virtual_machine/introduction/","text":"Flow Virtual Machine The Flow Virtual Machine (FVM) augments the Cadence runtime with the domain-specific functionality required by the Flow protocol. There are two types of operations that can be executed in the FVM - scripts and transactions. Scripts are operations that have read-only access to the ledger, while transactions can read or write to it. Cadence runtime.Interface defines a set of functions Cadence will use to query state from the FVM/ledger. The fvm package provides two execution environments that satisfy the runtime.Interface - the scriptEnv for scripts, and transactionEnv for transaction execution. These environments allow reading and writing values to the underlying storage (ledger), as expected by the Cadence interpreter.","title":"Introduction"},{"location":"knowledge_base/flow/flow_virtual_machine/introduction/#flow-virtual-machine","text":"The Flow Virtual Machine (FVM) augments the Cadence runtime with the domain-specific functionality required by the Flow protocol. There are two types of operations that can be executed in the FVM - scripts and transactions. Scripts are operations that have read-only access to the ledger, while transactions can read or write to it. Cadence runtime.Interface defines a set of functions Cadence will use to query state from the FVM/ledger. The fvm package provides two execution environments that satisfy the runtime.Interface - the scriptEnv for scripts, and transactionEnv for transaction execution. These environments allow reading and writing values to the underlying storage (ledger), as expected by the Cadence interpreter.","title":"Flow Virtual Machine"},{"location":"knowledge_base/flow/flow_virtual_machine/registers/","text":"Registers The FVM interacts with the Flow execution state by running atomic operations against a ledger. What we call a ledger is a type of key/value storage. It holds an array of key-value pairs called registers. In Flow, the ledger implementation provides a number of functionalities and guarantees, including speed, memory-efficiency, crash resillience (via write-ahead logs and checkpoints), thread safety etc. It is also a stateful key/value storage, and every update to the ledger creates a new state. A limited number of recent states is kept, and updates can be applied to any one of those states. Each ledger register is referenced by an ID - the key and holds a value - binary data. Register ID to Ledger Path When referencing storage locations from Flow code, each register is uniquely identified by three components - owner , controller and key . Owner represents the account to which the register belongs to. Depending on the register, the controller field can be either empty or have the value of the owner field. The key field represents the specific storage location of the account. Definition of a register ID ( source ): type RegisterID struct { Owner string Controller string Key string } Register ID is converted to a ledger.Key by concatenating specific key parts ( source ). const ( KeyPartOwner = uint16 ( 0 ) KeyPartController = uint16 ( 1 ) KeyPartKey = uint16 ( 2 ) ) // ... func RegisterIDToKey ( reg flow . RegisterID ) ledger . Key { return ledger . NewKey ([] ledger . KeyPart { ledger . NewKeyPart ( KeyPartOwner , [] byte ( reg . Owner )), ledger . NewKeyPart ( KeyPartController , [] byte ( reg . Controller )), ledger . NewKeyPart ( KeyPartKey , [] byte ( reg . Key )), }) } In order to map a Ledger key to a ledger path, the key can be converted to a corresponding path using pathfinder.KeyToPath . Depending on the version of the ledger pathfinder version, ledger path is either SHA256 or SHA3-256 (current version) hash of the canonical form of the ledger key. The canonical form of the ledger key is the concatenation of the individual key parts, taking into account the key part type ( source ) func ( k * Key ) CanonicalForm () [] byte { ret := \"\" for _ , kp := range k . KeyParts { ret += fmt . Sprintf ( \"/%d/%v\" , kp . Type , string ( kp . Value )) } return [] byte ( ret ) } Effectively, this means that for a given register, the ledger path is the SHA3-256 hash of the /0/<owner>/1/<controller>/2/<key> , where <controller> is typically either the <owner> field or an empty string, depending on the register. Common Registers There's a number of registers that have specific meaning and are often encountered in Flow. The following registers can be typically found in a Flow account: exists - does the account exist or not frozen - is the account frozen or not contract_names - list of the names of contracts deployed to an account public_key_count - number of public keys an account has public_key_<N> - location of the N-th public key of an account storage_used - amount of storage used by the account, in bytes storage_index - used to keep track of a number of registers in the owner account code.<name> - register where the name Cadence contract is stored Flow Contracts There are core Cadence contracts that are deployed on the Flow network as soon as it is bootstrapped. These contracts are essential to the normal operation of the Flow network. These contracts are: FlowToken FungibleToken FlowServiceAccount FlowEpoch FlowFees FlowStorageFees FlowClusterQC FlowDKG FlowIDTableStaking FlowStakingCollection LockedTokens StakingProxy These contracts are described in more detail here .","title":"Registers"},{"location":"knowledge_base/flow/flow_virtual_machine/registers/#registers","text":"The FVM interacts with the Flow execution state by running atomic operations against a ledger. What we call a ledger is a type of key/value storage. It holds an array of key-value pairs called registers. In Flow, the ledger implementation provides a number of functionalities and guarantees, including speed, memory-efficiency, crash resillience (via write-ahead logs and checkpoints), thread safety etc. It is also a stateful key/value storage, and every update to the ledger creates a new state. A limited number of recent states is kept, and updates can be applied to any one of those states. Each ledger register is referenced by an ID - the key and holds a value - binary data.","title":"Registers"},{"location":"knowledge_base/flow/flow_virtual_machine/registers/#register-id-to-ledger-path","text":"When referencing storage locations from Flow code, each register is uniquely identified by three components - owner , controller and key . Owner represents the account to which the register belongs to. Depending on the register, the controller field can be either empty or have the value of the owner field. The key field represents the specific storage location of the account. Definition of a register ID ( source ): type RegisterID struct { Owner string Controller string Key string } Register ID is converted to a ledger.Key by concatenating specific key parts ( source ). const ( KeyPartOwner = uint16 ( 0 ) KeyPartController = uint16 ( 1 ) KeyPartKey = uint16 ( 2 ) ) // ... func RegisterIDToKey ( reg flow . RegisterID ) ledger . Key { return ledger . NewKey ([] ledger . KeyPart { ledger . NewKeyPart ( KeyPartOwner , [] byte ( reg . Owner )), ledger . NewKeyPart ( KeyPartController , [] byte ( reg . Controller )), ledger . NewKeyPart ( KeyPartKey , [] byte ( reg . Key )), }) } In order to map a Ledger key to a ledger path, the key can be converted to a corresponding path using pathfinder.KeyToPath . Depending on the version of the ledger pathfinder version, ledger path is either SHA256 or SHA3-256 (current version) hash of the canonical form of the ledger key. The canonical form of the ledger key is the concatenation of the individual key parts, taking into account the key part type ( source ) func ( k * Key ) CanonicalForm () [] byte { ret := \"\" for _ , kp := range k . KeyParts { ret += fmt . Sprintf ( \"/%d/%v\" , kp . Type , string ( kp . Value )) } return [] byte ( ret ) } Effectively, this means that for a given register, the ledger path is the SHA3-256 hash of the /0/<owner>/1/<controller>/2/<key> , where <controller> is typically either the <owner> field or an empty string, depending on the register.","title":"Register ID to Ledger Path"},{"location":"knowledge_base/flow/flow_virtual_machine/registers/#common-registers","text":"There's a number of registers that have specific meaning and are often encountered in Flow. The following registers can be typically found in a Flow account: exists - does the account exist or not frozen - is the account frozen or not contract_names - list of the names of contracts deployed to an account public_key_count - number of public keys an account has public_key_<N> - location of the N-th public key of an account storage_used - amount of storage used by the account, in bytes storage_index - used to keep track of a number of registers in the owner account code.<name> - register where the name Cadence contract is stored","title":"Common Registers"},{"location":"knowledge_base/flow/flow_virtual_machine/registers/#flow-contracts","text":"There are core Cadence contracts that are deployed on the Flow network as soon as it is bootstrapped. These contracts are essential to the normal operation of the Flow network. These contracts are: FlowToken FungibleToken FlowServiceAccount FlowEpoch FlowFees FlowStorageFees FlowClusterQC FlowDKG FlowIDTableStaking FlowStakingCollection LockedTokens StakingProxy These contracts are described in more detail here .","title":"Flow Contracts"},{"location":"style/go/","text":"Go Code Style Guidelines Guidelines Copy Slices and Maps at Boundaries Slices and maps contain pointers to the underlying data so be wary of scenarios when they need to be copied. Receiving Slices and Maps Keep in mind that users can modify a map or slice you received as an argument if you store a reference to it. Bad Good func ( d * Driver ) SetTrips ( trips [] Trip ) { d . trips = trips } trips := ... d1 . SetTrips ( trips ) // Did you mean to modify d1.trips? trips [ 0 ] = ... func ( d * Driver ) SetTrips ( trips [] Trip ) { d . trips = make ([] Trip , len ( trips )) copy ( d . trips , trips ) } trips := ... d1 . SetTrips ( trips ) // We can now modify trips[0] without affecting d1.trips. trips [ 0 ] = ... Returning Slices and Maps Similarly, be wary of user modifications to maps or slices exposing internal state. Bad Good type Stats struct { mu sync . Mutex counters map [ string ] int } // Snapshot returns the current stats. func ( s * Stats ) Snapshot () map [ string ] int { s . mu . Lock () defer s . mu . Unlock () return s . counters } // snapshot is no longer protected by the mutex, so any // access to the snapshot is subject to data races. snapshot := stats . Snapshot () type Stats struct { mu sync . Mutex counters map [ string ] int } func ( s * Stats ) Snapshot () map [ string ] int { s . mu . Lock () defer s . mu . Unlock () result := make ( map [ string ] int , len ( s . counters )) for k , v := range s . counters { result [ k ] = v } return result } // Snapshot is now a copy. snapshot := stats . Snapshot () Start Enums at One The standard way of introducing enumerations in Go is to declare a custom type and a const group with iota . Since variables have a 0 default value, you should usually start your enums on a non-zero value. Bad Good type Operation int const ( Add Operation = iota Subtract Multiply ) // Add=0, Subtract=1, Multiply=2 type Operation int const ( Add Operation = iota + 1 Subtract Multiply ) // Add=1, Subtract=2, Multiply=3 There are cases where using the zero value makes sense, for example when the zero value case is the desirable default behavior. type LogOutput int const ( LogToStdout LogOutput = iota LogToFile LogToRemote ) // LogToStdout=0, LogToFile=1, LogToRemote=2 Error Types There are various options for declaring errors: errors.New for errors with simple static strings fmt.Errorf for formatted error strings Custom types that implement an Error() method Wrapped errors using \"pkg/errors\".Wrap When returning errors, consider the following to determine the best choice: Is this a simple error that needs no extra information? If so, errors.New should suffice. Do the clients need to detect and handle this error? If so, you should use a custom type, and implement the Error() method. Are you propagating an error returned by a downstream function? If so, check the section on error wrapping . Otherwise, fmt.Errorf is okay. If the client needs to detect a specific error case, a sentinel error should be created using errors.New . Bad Good // package foo func Open () error { return errors . New ( \"could not open\" ) } // package bar func use () { err := foo . Open () if err != nil { if err . Error () == \"could not open\" { // handle } else { panic ( \"unknown error\" ) } } } // package foo var ErrCouldNotOpen = errors . New ( \"could not open\" ) func Open () error { return ErrCouldNotOpen } // package bar err := foo . Open () if err != nil { if errors . Is ( err , foo . ErrCouldNotOpen ) { // handle } else { panic ( \"unknown error\" ) } } If you have an error that clients may need to detect, and you would like to add more information to it (e.g., it is not a static string), then you should use a custom type. Bad Good func open ( file string ) error { return fmt . Errorf ( \"file %q not found\" , file ) } func use () { err := open ( \"testfile.txt\" ) if err != nil { if strings . Contains ( err . Error (), \"not found\" ) { // handle } else { panic ( \"unknown error\" ) } } } type errNotFound struct { file string } func ( e errNotFound ) Error () string { return fmt . Sprintf ( \"file %q not found\" , e . file ) } func open ( file string ) error { return errNotFound { file : file } } func use () { err := open ( \"testfile.txt\" ) if err != nil { if _ , ok := err .( errNotFound ); ok { // handle } else { panic ( \"unknown error\" ) } } } Be careful with exporting custom error types directly since they become part of the public API of the package. It is preferable to expose matcher functions to check the error instead. // package foo type errNotFound struct { file string } func ( e errNotFound ) Error () string { return fmt . Sprintf ( \"file %q not found\" , e . file ) } func IsNotFoundError ( err error ) bool { _ , ok := err .( errNotFound ) return ok } func Open ( file string ) error { return errNotFound { file : file } } // package bar err := foo . Open ( \"foo\" ) if err != nil { if foo . IsNotFoundError ( err ) { // handle } else { panic ( \"unknown error\" ) } } Error Wrapping There are three main options for propagating errors if a call fails: Return the original error if there is no additional context to add and you want to maintain the original error type. Add context using \"pkg/errors\".Wrap so that the error message provides more context and \"pkg/errors\".Cause can be used to extract the original error. Use fmt.Errorf if the callers do not need to detect or handle that specific error case. It is recommended to add context where possible so that instead of a vague error such as \"connection refused\", you get more useful errors such as \"call service foo: connection refused\". See also Don't just check errors, handle them gracefully . Handle Type Assertion Failures The single return value form of a type assertion will panic on an incorrect type. Therefore, always use the \"comma ok\" idiom. Bad Good t := i .( string ) t , ok := i .( string ) if ! ok { // handle the error gracefully } Avoid Mutable Globals Avoid mutating global variables, instead opting for dependency injection. This applies to function pointers as well as other kinds of values. Bad Good // sign.go var _timeNow = time . Now func sign ( msg string ) string { now := _timeNow () return signWithTime ( msg , now ) } // sign.go type signer struct { now func () time . Time } func newSigner () * signer { return & signer { now : time . Now , } } func ( s * signer ) Sign ( msg string ) string { now := s . now () return signWithTime ( msg , now ) } // sign_test.go func TestSign ( t * testing . T ) { oldTimeNow := _timeNow _timeNow = func () time . Time { return someFixedTime } defer func () { _timeNow = oldTimeNow }() assert . Equal ( t , want , sign ( give )) } // sign_test.go func TestSigner ( t * testing . T ) { s := newSigner () s . now = func () time . Time { return someFixedTime } assert . Equal ( t , want , s . Sign ( give )) } Avoid init() Avoid init() where possible. When init() is unavoidable or desirable, code should attempt to: Be completely deterministic, regardless of program environment or invocation. Avoid depending on the ordering or side effects of other init() functions. While init() ordering is well-known, code can change, and thus relationships between init() functions can make code brittle and error-prone. Avoid accessing or manipulating global or environment state, such as machine information, environment variables, working directory, program arguments/inputs, etc. Avoid I/O, including both filesystem, network, and system calls. Code that cannot satisfy these requirements likely belongs as a helper to be called as part of main() (or elsewhere in a program's lifecycle), or be written as part of main() itself. In particular, libraries that are intended to be used by other programs should take special care to be completely deterministic and not perform \"init magic\". Bad Good type Foo struct { // ... } var _defaultFoo Foo func init () { _defaultFoo = Foo { // ... } } var _defaultFoo = Foo { // ... } // or, better, for testability: var _defaultFoo = defaultFoo () func defaultFoo () Foo { return Foo { // ... } } type Config struct { // ... } var _config Config func init () { // Bad: based on current directory cwd , _ := os . Getwd () // Bad: I/O raw , _ := ioutil . ReadFile ( path . Join ( cwd , \"config\" , \"config.yaml\" ), ) yaml . Unmarshal ( raw , & _config ) } type Config struct { // ... } func loadConfig () Config { cwd , err := os . Getwd () // handle err raw , err := ioutil . ReadFile ( path . Join ( cwd , \"config\" , \"config.yaml\" ), ) // handle err var config Config yaml . Unmarshal ( raw , & config ) return config } Considering the above, some situations in which init() may be preferable or necessary might include: Complex expressions that cannot be represented as single assignments. Pluggable hooks, such as database/sql dialects, encoding type registries, etc. Exit in Main Go programs use os.Exit or log.Fatal* to exit immediately. (Panicking is not a good way to exit programs, please don't panic.) Call one of os.Exit or log.Fatal* only in main() . All other functions should return errors to signal failure. Bad Good func main () { body := readFile ( path ) fmt . Println ( body ) } func readFile ( path string ) string { file , err := os . Open ( path ) if err != nil { log . Fatal ( err ) } payload , err := ioutil . ReadAll ( file ) if err != nil { log . Fatal ( err ) } return string ( payload ) } func main () { body , err := readFile ( path ) if err != nil { log . Fatal ( err ) } fmt . Println ( body ) } func readFile ( path string ) ( string , error ) { file , err := os . Open ( path ) if err != nil { return \"\" , err } payload , err := ioutil . ReadAll ( file ) if err != nil { return \"\" , err } return string ( payload ), nil } Rationale: Programs with multiple functions that exit present a few issues: Non-obvious control flow: Any function can exit the program, so it becomes difficult to reason about the control flow. Difficult to test: A function that exits the program will also exit the test calling it. This makes the function difficult to test and introduces risk of skipping other tests that have not yet been run by go test . Skipped cleanup: When a function exits the program, it skips function calls enqueued with defer statements. This adds risk of skipping important cleanup tasks. Exit Once If possible, prefer to call os.Exit or log.Fatal at most once in your main() . If there are multiple error scenarios that halt program execution, put that logic under a separate function and return errors from it. This has the effect of shortening your main() function and putting all key business logic into a separate, testable function. Bad Good package main func main () { args := os . Args [ 1 :] if len ( args ) != 1 { log . Fatal ( \"missing file\" ) } name := args [ 0 ] file , err := os . Open ( name ) if err != nil { log . Fatal ( err ) } defer file . Close () // If we call log.Fatal after this line, // file.Close will not be called. payload , err := ioutil . ReadAll ( file ) if err != nil { log . Fatal ( err ) } // ... } package main func main () { err := run () if err != nil { log . Fatal ( err ) } } func run () error { args := os . Args [ 1 :] if len ( args ) != 1 { return errors . New ( \"missing file\" ) } name := args [ 0 ] file , err := os . Open ( name ) if err != nil { return err } defer file . Close () payload , err := ioutil . ReadAll ( file ) if err != nil { return err } // ... } Performance Performance-specific guidelines apply only to the hot path(s) of the application. Prefer strconv over fmt When converting primitives to/from strings, strconv is faster than fmt . Bad Good for i := 0 ; i < b . N ; i ++ { s := fmt . Sprint ( rand . Int ()) } for i := 0 ; i < b . N ; i ++ { s := strconv . Itoa ( rand . Int ()) } BenchmarkFmtSprint-4 143 ns/op 2 allocs/op BenchmarkStrconv-4 64.2 ns/op 1 allocs/op Avoid string-to-byte conversion Do not create byte slices from a fixed string repeatedly. Instead, perform the conversion once and capture the result. Bad Good for i := 0 ; i < b . N ; i ++ { w . Write ([] byte ( \"Hello world\" )) } data := [] byte ( \"Hello world\" ) for i := 0 ; i < b . N ; i ++ { w . Write ( data ) } BenchmarkBad-4 50000000 22.2 ns/op BenchmarkGood-4 500000000 3.25 ns/op Prefer Specifying Container Capacity Specify container capacity where possible in order to allocate memory for the container up front. This minimizes subsequent allocations (by copying and resizing of the container) as elements are added. Specifying Map Capacity Hints Where possible, provide capacity hints when initializing maps with make() . make ( map [ T1 ] T2 , hint ) Providing a capacity hint to make() tries to right-size the map at initialization time, which reduces the need for growing the map and allocations as elements are added to the map. Note that, unlike slices, map capacity hints do not guarantee complete, preemptive allocation, but are used to approximate the number of hashmap buckets required. Consequently, allocations may still occur when adding elements to the map, even up to the specified capacity. Bad Good m := make ( map [ string ] os . FileInfo ) files , _ := ioutil . ReadDir ( \"./files\" ) for _ , f := range files { m [ f . Name ()] = f } files , _ := ioutil . ReadDir ( \"./files\" ) m := make ( map [ string ] os . FileInfo , len ( files )) for _ , f := range files { m [ f . Name ()] = f } `m` is created without a size hint; there may be more allocations at assignment time. `m` is created with a size hint; there may be fewer allocations at assignment time. Specifying Slice Capacity Where possible, provide capacity hints when initializing slices with make() , particularly when appending. make ([] T , length , capacity ) Unlike maps, slice capacity is not a hint: the compiler will allocate enough memory for the capacity of the slice as provided to make() , which means that subsequent append() operations will incur zero allocations (until the length of the slice matches the capacity, after which any appends will require a resize to hold additional elements). Bad Good for n := 0 ; n < b . N ; n ++ { data := make ([] int , 0 ) for k := 0 ; k < size ; k ++ { data = append ( data , k ) } } for n := 0 ; n < b . N ; n ++ { data := make ([] int , 0 , size ) for k := 0 ; k < size ; k ++ { data = append ( data , k ) } } BenchmarkBad-4 100000000 2.48s BenchmarkGood-4 100000000 0.21s Style Import Grouping In order to make imports orderly and clear, imported packages should be grouped in the following order from top to bottom: Standard library External dependencies External dependencies from our org Internal dependencies Within each import group, imports should be sorted alphabetically. Bad Good import ( \"errors\" \"fmt\" \"github.com/rs/zerolog\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" \"sync\" \"time\" ) import ( \"errors\" \"fmt\" \"sync\" \"time\" \"github.com/rs/zerolog\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" ) Package Names When naming packages, choose a name that is: All lower-case. No capitals or underscores. Does not need to be renamed using named imports at most call sites. Short and succinct. Remember that the name is identified in full at every call site. Not plural. For example, net/url , not net/urls . Not \"common\", \"util\", \"shared\", or \"lib\". These are bad, uninformative names. See also Package Names and Style guideline for Go packages . Function Names We follow the Go community's convention of using MixedCaps for function names . An exception is made for test functions, which may contain underscores for the purpose of grouping related test cases, e.g., TestMyFunction_WhatIsBeingTested . Function Grouping and Ordering Functions should be sorted in rough call order. Functions in a file should be grouped by receiver. Therefore, exported functions should appear first in a file, after struct , const , var definitions. A newXYZ() / NewXYZ() may appear after the type is defined, but before the rest of the methods on the receiver. Since functions are grouped by receiver, plain utility functions should appear towards the end of the file. Bad Good func ( s * something ) Cost () { return calcCost ( s . weights ) } type something struct { ... } func calcCost ( n [] int ) int { ... } func ( s * something ) Stop () { ... } func newSomething () * something { return & something {} } type something struct { ... } func newSomething () * something { return & something {} } func ( s * something ) Cost () { return calcCost ( s . weights ) } func ( s * something ) Stop () { ... } func calcCost ( n [] int ) int { ... } Reduce Nesting Code should reduce nesting where possible by handling error cases/special conditions first and returning early or continuing the loop. Reduce the amount of code that is nested multiple levels. Bad Good for _ , v := range data { if v . F1 == 1 { v = process ( v ) err := v . Call () if err == nil { v . Send () } else { return err } } else { log . Printf ( \"Invalid v: %v\" , v ) } } for _ , v := range data { if v . F1 != 1 { log . Printf ( \"Invalid v: %v\" , v ) continue } v = process ( v ) err := v . Call () if err != nil { return err } v . Send () } Unnecessary Else If a variable is set in both branches of an if, it can be replaced with a single if. Bad Good var a int if b { a = 100 } else { a = 10 } a := 10 if b { a = 100 } Top-level Variable Declarations At the top level, use the standard var keyword. Do not specify the type, unless it is not the same type as the expression. Bad Good var _s string = F () func F () string { return \"A\" } var _s = F () // Since F already states that it returns a string, we don't need to specify // the type again. func F () string { return \"A\" } Specify the type if the type of the expression does not match the desired type exactly. type myError struct {} func ( myError ) Error () string { return \"error\" } func F () myError { return myError {} } var _e error = F () // F returns an object of type myError, but we want error. Local Variable Declarations Short variable declarations ( := ) should be used if a variable is being set to some value explicitly. Bad Good var s = \"foo\" s := \"foo\" However, there are cases where the default value is clearer when the var keyword is used. Declaring Empty Slices , for example. Bad Good func f ( list [] int ) { filtered := [] int {} for _ , v := range list { if v > 10 { filtered = append ( filtered , v ) } } } func f ( list [] int ) { var filtered [] int for _ , v := range list { if v > 10 { filtered = append ( filtered , v ) } } } Reduce Scope of Variables Where possible, reduce scope of variables. Do not reduce the scope if it conflicts with Reduce Nesting . Bad Good data , err := ioutil . ReadFile ( name ) if err == nil { err = cfg . Decode ( data ) if err != nil { return err } fmt . Println ( cfg ) return nil } else { return err } data , err := ioutil . ReadFile ( name ) if err != nil { return err } err = cfg . Decode ( data ) if err != nil { return err } fmt . Println ( cfg ) return nil Avoid Naked Parameters Naked parameters in function calls can hurt readability. Add C-style comments( /* ... */ ) for parameter names when their meaning is not obvious. Bad Good // func printInfo(name string, isLocal, done bool) printInfo ( \"foo\" , true , true ) // func printInfo(name string, isLocal, done bool) printInfo ( \"foo\" , true /* isLocal */ , true /* done */ ) Better yet, replace naked bool types with custom types for more readable and type-safe code. This allows more than just two states (true/false) for that parameter in the future. type Region int const ( UnknownRegion Region = iota Local ) type Status int const ( StatusReady Status = iota + 1 StatusDone // Maybe we will have a StatusInProgress in the future. ) func printInfo ( name string , region Region , status Status ) Initializing Structs Use Field Names to Initialize Structs You should almost always specify field names when initializing structs. This is enforced by go vet . Bad Good k := User { \"John\" , \"Doe\" , true } k := User { FirstName : \"John\" , LastName : \"Doe\" , Admin : true , } Exception: Field names may be omitted in test tables when there are 3 or fewer fields. tests := [] struct { op Operation want string }{ { Add , \"add\" }, { Subtract , \"subtract\" }, } Omit Zero Value Fields in Structs When initializing structs with field names, omit fields that have zero values unless they provide meaningful context. Otherwise, let Go set these to zero values automatically. Bad Good user := User { FirstName : \"John\" , LastName : \"Doe\" , MiddleName : \"\" , Admin : false , } user := User { FirstName : \"John\" , LastName : \"Doe\" , } This helps reduce noise for readers by omitting values that are default in that context. Only meaningful values are specified. Include zero values where field names provide meaningful context. For example, test cases in test tables can benefit from names of fields even when they are zero-valued. tests := [] struct { give string want int }{ { give : \"0\" , want : 0 }, // ... } Use var for Zero Value Structs When all the fields of a struct are omitted in a declaration, use the var form to declare the struct. Bad Good user := User {} var user User This differentiates zero valued structs from those with non-zero fields similar to the distinction created for map initialization , and matches how we prefer to declare empty slices . Initializing Struct References Use &T{} instead of new(T) when initializing struct references so that it is consistent with the struct initialization. Bad Good sval := T { Name : \"foo\" } // inconsistent sptr := new ( T ) sptr . Name = \"bar\" sval := T { Name : \"foo\" } sptr := & T { Name : \"bar\" } Initializing Maps Prefer make(..) for empty maps, and maps populated programmatically. This makes map initialization visually distinct from declaration, and it makes it easy to add size hints later if available. Bad Good var ( // m1 is safe to read and write; // m2 will panic on writes. m1 = map [ T1 ] T2 {} m2 map [ T1 ] T2 ) var ( // m1 is safe to read and write; // m2 will panic on writes. m1 = make ( map [ T1 ] T2 ) m2 map [ T1 ] T2 ) Declaration and initialization are visually similar. Declaration and initialization are visually distinct. Where possible, provide capacity hints when initializing maps with make() . See Specifying Map Capacity Hints for more information. On the other hand, if the map holds a fixed list of elements, use map literals to initialize the map. Bad Good m := make ( map [ T1 ] T2 , 3 ) m [ k1 ] = v1 m [ k2 ] = v2 m [ k3 ] = v3 m := map [ T1 ] T2 { k1 : v1 , k2 : v2 , k3 : v3 , } The basic rule of thumb is to use map literals when adding a fixed set of elements at initialization time, otherwise use make (and specify a size hint if available). Patterns Functional Options Functional options is a pattern in which you declare an opaque Option type that records information in some internal struct. You accept a variadic number of these options and act upon the full information recorded by the options on the internal struct. Use this pattern for optional arguments in constructors and other public APIs that you foresee needing to expand, especially if you already have three or more arguments on those functions. Bad Good // package db func Open ( addr string , cache bool , logger * zap . Logger ) ( * Connection , error ) { // ... } // package db type Option interface { // ... } func WithCache ( c bool ) Option { // ... } func WithLogger ( log * zap . Logger ) Option { // ... } // Open creates a connection. func Open ( addr string , opts ... Option , ) ( * Connection , error ) { // ... } The cache and logger parameters must always be provided, even if the user wants to use the default. db . Open ( addr , db . DefaultCache , zap . NewNop ()) db . Open ( addr , db . DefaultCache , log ) db . Open ( addr , false /* cache */ , zap . NewNop ()) db . Open ( addr , false /* cache */ , log ) Options are provided only if needed. db . Open ( addr ) db . Open ( addr , db . WithLogger ( log )) db . Open ( addr , db . WithCache ( false )) db . Open ( addr , db . WithCache ( false ), db . WithLogger ( log ), ) Our suggested way of implementing this pattern is with an Option interface that holds an unexported method, recording options on an unexported options struct. type options struct { cache bool logger * zap . Logger } type Option interface { apply ( * options ) } type cacheOption bool func ( c cacheOption ) apply ( opts * options ) { opts . cache = bool ( c ) } func WithCache ( c bool ) Option { return cacheOption ( c ) } type loggerOption struct { Log * zap . Logger } func ( l loggerOption ) apply ( opts * options ) { opts . logger = l . Log } func WithLogger ( log * zap . Logger ) Option { return loggerOption { Log : log } } // Open creates a connection. func Open ( addr string , opts ... Option , ) ( * Connection , error ) { options := options { cache : defaultCache , logger : zap . NewNop (), } for _ , o := range opts { o . apply ( & options ) } // ... } Note that there's a method of implementing this pattern with closures, but we believe that the pattern above provides more flexibility for authors and is easier to debug and test for users. In particular, it allows options to be compared against each other in tests and mocks, versus closures where this is impossible. Further, it lets options implement other interfaces, including fmt.Stringer which allows for user-readable string representations of the options. See also, Self-referential functions and the design of options Functional options for friendly APIs","title":"Go"},{"location":"style/go/#go-code-style-guidelines","text":"","title":"Go Code Style Guidelines"},{"location":"style/go/#guidelines","text":"","title":"Guidelines"},{"location":"style/go/#copy-slices-and-maps-at-boundaries","text":"Slices and maps contain pointers to the underlying data so be wary of scenarios when they need to be copied.","title":"Copy Slices and Maps at Boundaries"},{"location":"style/go/#receiving-slices-and-maps","text":"Keep in mind that users can modify a map or slice you received as an argument if you store a reference to it. Bad Good func ( d * Driver ) SetTrips ( trips [] Trip ) { d . trips = trips } trips := ... d1 . SetTrips ( trips ) // Did you mean to modify d1.trips? trips [ 0 ] = ... func ( d * Driver ) SetTrips ( trips [] Trip ) { d . trips = make ([] Trip , len ( trips )) copy ( d . trips , trips ) } trips := ... d1 . SetTrips ( trips ) // We can now modify trips[0] without affecting d1.trips. trips [ 0 ] = ...","title":"Receiving Slices and Maps"},{"location":"style/go/#returning-slices-and-maps","text":"Similarly, be wary of user modifications to maps or slices exposing internal state. Bad Good type Stats struct { mu sync . Mutex counters map [ string ] int } // Snapshot returns the current stats. func ( s * Stats ) Snapshot () map [ string ] int { s . mu . Lock () defer s . mu . Unlock () return s . counters } // snapshot is no longer protected by the mutex, so any // access to the snapshot is subject to data races. snapshot := stats . Snapshot () type Stats struct { mu sync . Mutex counters map [ string ] int } func ( s * Stats ) Snapshot () map [ string ] int { s . mu . Lock () defer s . mu . Unlock () result := make ( map [ string ] int , len ( s . counters )) for k , v := range s . counters { result [ k ] = v } return result } // Snapshot is now a copy. snapshot := stats . Snapshot ()","title":"Returning Slices and Maps"},{"location":"style/go/#start-enums-at-one","text":"The standard way of introducing enumerations in Go is to declare a custom type and a const group with iota . Since variables have a 0 default value, you should usually start your enums on a non-zero value. Bad Good type Operation int const ( Add Operation = iota Subtract Multiply ) // Add=0, Subtract=1, Multiply=2 type Operation int const ( Add Operation = iota + 1 Subtract Multiply ) // Add=1, Subtract=2, Multiply=3 There are cases where using the zero value makes sense, for example when the zero value case is the desirable default behavior. type LogOutput int const ( LogToStdout LogOutput = iota LogToFile LogToRemote ) // LogToStdout=0, LogToFile=1, LogToRemote=2","title":"Start Enums at One"},{"location":"style/go/#error-types","text":"There are various options for declaring errors: errors.New for errors with simple static strings fmt.Errorf for formatted error strings Custom types that implement an Error() method Wrapped errors using \"pkg/errors\".Wrap When returning errors, consider the following to determine the best choice: Is this a simple error that needs no extra information? If so, errors.New should suffice. Do the clients need to detect and handle this error? If so, you should use a custom type, and implement the Error() method. Are you propagating an error returned by a downstream function? If so, check the section on error wrapping . Otherwise, fmt.Errorf is okay. If the client needs to detect a specific error case, a sentinel error should be created using errors.New . Bad Good // package foo func Open () error { return errors . New ( \"could not open\" ) } // package bar func use () { err := foo . Open () if err != nil { if err . Error () == \"could not open\" { // handle } else { panic ( \"unknown error\" ) } } } // package foo var ErrCouldNotOpen = errors . New ( \"could not open\" ) func Open () error { return ErrCouldNotOpen } // package bar err := foo . Open () if err != nil { if errors . Is ( err , foo . ErrCouldNotOpen ) { // handle } else { panic ( \"unknown error\" ) } } If you have an error that clients may need to detect, and you would like to add more information to it (e.g., it is not a static string), then you should use a custom type. Bad Good func open ( file string ) error { return fmt . Errorf ( \"file %q not found\" , file ) } func use () { err := open ( \"testfile.txt\" ) if err != nil { if strings . Contains ( err . Error (), \"not found\" ) { // handle } else { panic ( \"unknown error\" ) } } } type errNotFound struct { file string } func ( e errNotFound ) Error () string { return fmt . Sprintf ( \"file %q not found\" , e . file ) } func open ( file string ) error { return errNotFound { file : file } } func use () { err := open ( \"testfile.txt\" ) if err != nil { if _ , ok := err .( errNotFound ); ok { // handle } else { panic ( \"unknown error\" ) } } } Be careful with exporting custom error types directly since they become part of the public API of the package. It is preferable to expose matcher functions to check the error instead. // package foo type errNotFound struct { file string } func ( e errNotFound ) Error () string { return fmt . Sprintf ( \"file %q not found\" , e . file ) } func IsNotFoundError ( err error ) bool { _ , ok := err .( errNotFound ) return ok } func Open ( file string ) error { return errNotFound { file : file } } // package bar err := foo . Open ( \"foo\" ) if err != nil { if foo . IsNotFoundError ( err ) { // handle } else { panic ( \"unknown error\" ) } }","title":"Error Types"},{"location":"style/go/#error-wrapping","text":"There are three main options for propagating errors if a call fails: Return the original error if there is no additional context to add and you want to maintain the original error type. Add context using \"pkg/errors\".Wrap so that the error message provides more context and \"pkg/errors\".Cause can be used to extract the original error. Use fmt.Errorf if the callers do not need to detect or handle that specific error case. It is recommended to add context where possible so that instead of a vague error such as \"connection refused\", you get more useful errors such as \"call service foo: connection refused\". See also Don't just check errors, handle them gracefully .","title":"Error Wrapping"},{"location":"style/go/#handle-type-assertion-failures","text":"The single return value form of a type assertion will panic on an incorrect type. Therefore, always use the \"comma ok\" idiom. Bad Good t := i .( string ) t , ok := i .( string ) if ! ok { // handle the error gracefully }","title":"Handle Type Assertion Failures"},{"location":"style/go/#avoid-mutable-globals","text":"Avoid mutating global variables, instead opting for dependency injection. This applies to function pointers as well as other kinds of values. Bad Good // sign.go var _timeNow = time . Now func sign ( msg string ) string { now := _timeNow () return signWithTime ( msg , now ) } // sign.go type signer struct { now func () time . Time } func newSigner () * signer { return & signer { now : time . Now , } } func ( s * signer ) Sign ( msg string ) string { now := s . now () return signWithTime ( msg , now ) } // sign_test.go func TestSign ( t * testing . T ) { oldTimeNow := _timeNow _timeNow = func () time . Time { return someFixedTime } defer func () { _timeNow = oldTimeNow }() assert . Equal ( t , want , sign ( give )) } // sign_test.go func TestSigner ( t * testing . T ) { s := newSigner () s . now = func () time . Time { return someFixedTime } assert . Equal ( t , want , s . Sign ( give )) }","title":"Avoid Mutable Globals"},{"location":"style/go/#avoid-init","text":"Avoid init() where possible. When init() is unavoidable or desirable, code should attempt to: Be completely deterministic, regardless of program environment or invocation. Avoid depending on the ordering or side effects of other init() functions. While init() ordering is well-known, code can change, and thus relationships between init() functions can make code brittle and error-prone. Avoid accessing or manipulating global or environment state, such as machine information, environment variables, working directory, program arguments/inputs, etc. Avoid I/O, including both filesystem, network, and system calls. Code that cannot satisfy these requirements likely belongs as a helper to be called as part of main() (or elsewhere in a program's lifecycle), or be written as part of main() itself. In particular, libraries that are intended to be used by other programs should take special care to be completely deterministic and not perform \"init magic\". Bad Good type Foo struct { // ... } var _defaultFoo Foo func init () { _defaultFoo = Foo { // ... } } var _defaultFoo = Foo { // ... } // or, better, for testability: var _defaultFoo = defaultFoo () func defaultFoo () Foo { return Foo { // ... } } type Config struct { // ... } var _config Config func init () { // Bad: based on current directory cwd , _ := os . Getwd () // Bad: I/O raw , _ := ioutil . ReadFile ( path . Join ( cwd , \"config\" , \"config.yaml\" ), ) yaml . Unmarshal ( raw , & _config ) } type Config struct { // ... } func loadConfig () Config { cwd , err := os . Getwd () // handle err raw , err := ioutil . ReadFile ( path . Join ( cwd , \"config\" , \"config.yaml\" ), ) // handle err var config Config yaml . Unmarshal ( raw , & config ) return config } Considering the above, some situations in which init() may be preferable or necessary might include: Complex expressions that cannot be represented as single assignments. Pluggable hooks, such as database/sql dialects, encoding type registries, etc.","title":"Avoid init()"},{"location":"style/go/#exit-in-main","text":"Go programs use os.Exit or log.Fatal* to exit immediately. (Panicking is not a good way to exit programs, please don't panic.) Call one of os.Exit or log.Fatal* only in main() . All other functions should return errors to signal failure. Bad Good func main () { body := readFile ( path ) fmt . Println ( body ) } func readFile ( path string ) string { file , err := os . Open ( path ) if err != nil { log . Fatal ( err ) } payload , err := ioutil . ReadAll ( file ) if err != nil { log . Fatal ( err ) } return string ( payload ) } func main () { body , err := readFile ( path ) if err != nil { log . Fatal ( err ) } fmt . Println ( body ) } func readFile ( path string ) ( string , error ) { file , err := os . Open ( path ) if err != nil { return \"\" , err } payload , err := ioutil . ReadAll ( file ) if err != nil { return \"\" , err } return string ( payload ), nil } Rationale: Programs with multiple functions that exit present a few issues: Non-obvious control flow: Any function can exit the program, so it becomes difficult to reason about the control flow. Difficult to test: A function that exits the program will also exit the test calling it. This makes the function difficult to test and introduces risk of skipping other tests that have not yet been run by go test . Skipped cleanup: When a function exits the program, it skips function calls enqueued with defer statements. This adds risk of skipping important cleanup tasks.","title":"Exit in Main"},{"location":"style/go/#exit-once","text":"If possible, prefer to call os.Exit or log.Fatal at most once in your main() . If there are multiple error scenarios that halt program execution, put that logic under a separate function and return errors from it. This has the effect of shortening your main() function and putting all key business logic into a separate, testable function. Bad Good package main func main () { args := os . Args [ 1 :] if len ( args ) != 1 { log . Fatal ( \"missing file\" ) } name := args [ 0 ] file , err := os . Open ( name ) if err != nil { log . Fatal ( err ) } defer file . Close () // If we call log.Fatal after this line, // file.Close will not be called. payload , err := ioutil . ReadAll ( file ) if err != nil { log . Fatal ( err ) } // ... } package main func main () { err := run () if err != nil { log . Fatal ( err ) } } func run () error { args := os . Args [ 1 :] if len ( args ) != 1 { return errors . New ( \"missing file\" ) } name := args [ 0 ] file , err := os . Open ( name ) if err != nil { return err } defer file . Close () payload , err := ioutil . ReadAll ( file ) if err != nil { return err } // ... }","title":"Exit Once"},{"location":"style/go/#performance","text":"Performance-specific guidelines apply only to the hot path(s) of the application.","title":"Performance"},{"location":"style/go/#prefer-strconv-over-fmt","text":"When converting primitives to/from strings, strconv is faster than fmt . Bad Good for i := 0 ; i < b . N ; i ++ { s := fmt . Sprint ( rand . Int ()) } for i := 0 ; i < b . N ; i ++ { s := strconv . Itoa ( rand . Int ()) } BenchmarkFmtSprint-4 143 ns/op 2 allocs/op BenchmarkStrconv-4 64.2 ns/op 1 allocs/op","title":"Prefer strconv over fmt"},{"location":"style/go/#avoid-string-to-byte-conversion","text":"Do not create byte slices from a fixed string repeatedly. Instead, perform the conversion once and capture the result. Bad Good for i := 0 ; i < b . N ; i ++ { w . Write ([] byte ( \"Hello world\" )) } data := [] byte ( \"Hello world\" ) for i := 0 ; i < b . N ; i ++ { w . Write ( data ) } BenchmarkBad-4 50000000 22.2 ns/op BenchmarkGood-4 500000000 3.25 ns/op","title":"Avoid string-to-byte conversion"},{"location":"style/go/#prefer-specifying-container-capacity","text":"Specify container capacity where possible in order to allocate memory for the container up front. This minimizes subsequent allocations (by copying and resizing of the container) as elements are added.","title":"Prefer Specifying Container Capacity"},{"location":"style/go/#specifying-map-capacity-hints","text":"Where possible, provide capacity hints when initializing maps with make() . make ( map [ T1 ] T2 , hint ) Providing a capacity hint to make() tries to right-size the map at initialization time, which reduces the need for growing the map and allocations as elements are added to the map. Note that, unlike slices, map capacity hints do not guarantee complete, preemptive allocation, but are used to approximate the number of hashmap buckets required. Consequently, allocations may still occur when adding elements to the map, even up to the specified capacity. Bad Good m := make ( map [ string ] os . FileInfo ) files , _ := ioutil . ReadDir ( \"./files\" ) for _ , f := range files { m [ f . Name ()] = f } files , _ := ioutil . ReadDir ( \"./files\" ) m := make ( map [ string ] os . FileInfo , len ( files )) for _ , f := range files { m [ f . Name ()] = f } `m` is created without a size hint; there may be more allocations at assignment time. `m` is created with a size hint; there may be fewer allocations at assignment time.","title":"Specifying Map Capacity Hints"},{"location":"style/go/#specifying-slice-capacity","text":"Where possible, provide capacity hints when initializing slices with make() , particularly when appending. make ([] T , length , capacity ) Unlike maps, slice capacity is not a hint: the compiler will allocate enough memory for the capacity of the slice as provided to make() , which means that subsequent append() operations will incur zero allocations (until the length of the slice matches the capacity, after which any appends will require a resize to hold additional elements). Bad Good for n := 0 ; n < b . N ; n ++ { data := make ([] int , 0 ) for k := 0 ; k < size ; k ++ { data = append ( data , k ) } } for n := 0 ; n < b . N ; n ++ { data := make ([] int , 0 , size ) for k := 0 ; k < size ; k ++ { data = append ( data , k ) } } BenchmarkBad-4 100000000 2.48s BenchmarkGood-4 100000000 0.21s","title":"Specifying Slice Capacity"},{"location":"style/go/#style","text":"","title":"Style"},{"location":"style/go/#import-grouping","text":"In order to make imports orderly and clear, imported packages should be grouped in the following order from top to bottom: Standard library External dependencies External dependencies from our org Internal dependencies Within each import group, imports should be sorted alphabetically. Bad Good import ( \"errors\" \"fmt\" \"github.com/rs/zerolog\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" \"sync\" \"time\" ) import ( \"errors\" \"fmt\" \"sync\" \"time\" \"github.com/rs/zerolog\" \"github.com/onflow/flow-go/ledger\" \"github.com/onflow/flow-go/ledger/complete/mtrie/trie\" \"github.com/onflow/flow-go/model/flow\" \"github.com/optakt/flow-dps/models/dps\" )","title":"Import Grouping"},{"location":"style/go/#package-names","text":"When naming packages, choose a name that is: All lower-case. No capitals or underscores. Does not need to be renamed using named imports at most call sites. Short and succinct. Remember that the name is identified in full at every call site. Not plural. For example, net/url , not net/urls . Not \"common\", \"util\", \"shared\", or \"lib\". These are bad, uninformative names. See also Package Names and Style guideline for Go packages .","title":"Package Names"},{"location":"style/go/#function-names","text":"We follow the Go community's convention of using MixedCaps for function names . An exception is made for test functions, which may contain underscores for the purpose of grouping related test cases, e.g., TestMyFunction_WhatIsBeingTested .","title":"Function Names"},{"location":"style/go/#function-grouping-and-ordering","text":"Functions should be sorted in rough call order. Functions in a file should be grouped by receiver. Therefore, exported functions should appear first in a file, after struct , const , var definitions. A newXYZ() / NewXYZ() may appear after the type is defined, but before the rest of the methods on the receiver. Since functions are grouped by receiver, plain utility functions should appear towards the end of the file. Bad Good func ( s * something ) Cost () { return calcCost ( s . weights ) } type something struct { ... } func calcCost ( n [] int ) int { ... } func ( s * something ) Stop () { ... } func newSomething () * something { return & something {} } type something struct { ... } func newSomething () * something { return & something {} } func ( s * something ) Cost () { return calcCost ( s . weights ) } func ( s * something ) Stop () { ... } func calcCost ( n [] int ) int { ... }","title":"Function Grouping and Ordering"},{"location":"style/go/#reduce-nesting","text":"Code should reduce nesting where possible by handling error cases/special conditions first and returning early or continuing the loop. Reduce the amount of code that is nested multiple levels. Bad Good for _ , v := range data { if v . F1 == 1 { v = process ( v ) err := v . Call () if err == nil { v . Send () } else { return err } } else { log . Printf ( \"Invalid v: %v\" , v ) } } for _ , v := range data { if v . F1 != 1 { log . Printf ( \"Invalid v: %v\" , v ) continue } v = process ( v ) err := v . Call () if err != nil { return err } v . Send () }","title":"Reduce Nesting"},{"location":"style/go/#unnecessary-else","text":"If a variable is set in both branches of an if, it can be replaced with a single if. Bad Good var a int if b { a = 100 } else { a = 10 } a := 10 if b { a = 100 }","title":"Unnecessary Else"},{"location":"style/go/#top-level-variable-declarations","text":"At the top level, use the standard var keyword. Do not specify the type, unless it is not the same type as the expression. Bad Good var _s string = F () func F () string { return \"A\" } var _s = F () // Since F already states that it returns a string, we don't need to specify // the type again. func F () string { return \"A\" } Specify the type if the type of the expression does not match the desired type exactly. type myError struct {} func ( myError ) Error () string { return \"error\" } func F () myError { return myError {} } var _e error = F () // F returns an object of type myError, but we want error.","title":"Top-level Variable Declarations"},{"location":"style/go/#local-variable-declarations","text":"Short variable declarations ( := ) should be used if a variable is being set to some value explicitly. Bad Good var s = \"foo\" s := \"foo\" However, there are cases where the default value is clearer when the var keyword is used. Declaring Empty Slices , for example. Bad Good func f ( list [] int ) { filtered := [] int {} for _ , v := range list { if v > 10 { filtered = append ( filtered , v ) } } } func f ( list [] int ) { var filtered [] int for _ , v := range list { if v > 10 { filtered = append ( filtered , v ) } } }","title":"Local Variable Declarations"},{"location":"style/go/#reduce-scope-of-variables","text":"Where possible, reduce scope of variables. Do not reduce the scope if it conflicts with Reduce Nesting . Bad Good data , err := ioutil . ReadFile ( name ) if err == nil { err = cfg . Decode ( data ) if err != nil { return err } fmt . Println ( cfg ) return nil } else { return err } data , err := ioutil . ReadFile ( name ) if err != nil { return err } err = cfg . Decode ( data ) if err != nil { return err } fmt . Println ( cfg ) return nil","title":"Reduce Scope of Variables"},{"location":"style/go/#avoid-naked-parameters","text":"Naked parameters in function calls can hurt readability. Add C-style comments( /* ... */ ) for parameter names when their meaning is not obvious. Bad Good // func printInfo(name string, isLocal, done bool) printInfo ( \"foo\" , true , true ) // func printInfo(name string, isLocal, done bool) printInfo ( \"foo\" , true /* isLocal */ , true /* done */ ) Better yet, replace naked bool types with custom types for more readable and type-safe code. This allows more than just two states (true/false) for that parameter in the future. type Region int const ( UnknownRegion Region = iota Local ) type Status int const ( StatusReady Status = iota + 1 StatusDone // Maybe we will have a StatusInProgress in the future. ) func printInfo ( name string , region Region , status Status )","title":"Avoid Naked Parameters"},{"location":"style/go/#initializing-structs","text":"","title":"Initializing Structs"},{"location":"style/go/#use-field-names-to-initialize-structs","text":"You should almost always specify field names when initializing structs. This is enforced by go vet . Bad Good k := User { \"John\" , \"Doe\" , true } k := User { FirstName : \"John\" , LastName : \"Doe\" , Admin : true , } Exception: Field names may be omitted in test tables when there are 3 or fewer fields. tests := [] struct { op Operation want string }{ { Add , \"add\" }, { Subtract , \"subtract\" }, }","title":"Use Field Names to Initialize Structs"},{"location":"style/go/#omit-zero-value-fields-in-structs","text":"When initializing structs with field names, omit fields that have zero values unless they provide meaningful context. Otherwise, let Go set these to zero values automatically. Bad Good user := User { FirstName : \"John\" , LastName : \"Doe\" , MiddleName : \"\" , Admin : false , } user := User { FirstName : \"John\" , LastName : \"Doe\" , } This helps reduce noise for readers by omitting values that are default in that context. Only meaningful values are specified. Include zero values where field names provide meaningful context. For example, test cases in test tables can benefit from names of fields even when they are zero-valued. tests := [] struct { give string want int }{ { give : \"0\" , want : 0 }, // ... }","title":"Omit Zero Value Fields in Structs"},{"location":"style/go/#use-var-for-zero-value-structs","text":"When all the fields of a struct are omitted in a declaration, use the var form to declare the struct. Bad Good user := User {} var user User This differentiates zero valued structs from those with non-zero fields similar to the distinction created for map initialization , and matches how we prefer to declare empty slices .","title":"Use var for Zero Value Structs"},{"location":"style/go/#initializing-struct-references","text":"Use &T{} instead of new(T) when initializing struct references so that it is consistent with the struct initialization. Bad Good sval := T { Name : \"foo\" } // inconsistent sptr := new ( T ) sptr . Name = \"bar\" sval := T { Name : \"foo\" } sptr := & T { Name : \"bar\" }","title":"Initializing Struct References"},{"location":"style/go/#initializing-maps","text":"Prefer make(..) for empty maps, and maps populated programmatically. This makes map initialization visually distinct from declaration, and it makes it easy to add size hints later if available. Bad Good var ( // m1 is safe to read and write; // m2 will panic on writes. m1 = map [ T1 ] T2 {} m2 map [ T1 ] T2 ) var ( // m1 is safe to read and write; // m2 will panic on writes. m1 = make ( map [ T1 ] T2 ) m2 map [ T1 ] T2 ) Declaration and initialization are visually similar. Declaration and initialization are visually distinct. Where possible, provide capacity hints when initializing maps with make() . See Specifying Map Capacity Hints for more information. On the other hand, if the map holds a fixed list of elements, use map literals to initialize the map. Bad Good m := make ( map [ T1 ] T2 , 3 ) m [ k1 ] = v1 m [ k2 ] = v2 m [ k3 ] = v3 m := map [ T1 ] T2 { k1 : v1 , k2 : v2 , k3 : v3 , } The basic rule of thumb is to use map literals when adding a fixed set of elements at initialization time, otherwise use make (and specify a size hint if available).","title":"Initializing Maps"},{"location":"style/go/#patterns","text":"","title":"Patterns"},{"location":"style/go/#functional-options","text":"Functional options is a pattern in which you declare an opaque Option type that records information in some internal struct. You accept a variadic number of these options and act upon the full information recorded by the options on the internal struct. Use this pattern for optional arguments in constructors and other public APIs that you foresee needing to expand, especially if you already have three or more arguments on those functions. Bad Good // package db func Open ( addr string , cache bool , logger * zap . Logger ) ( * Connection , error ) { // ... } // package db type Option interface { // ... } func WithCache ( c bool ) Option { // ... } func WithLogger ( log * zap . Logger ) Option { // ... } // Open creates a connection. func Open ( addr string , opts ... Option , ) ( * Connection , error ) { // ... } The cache and logger parameters must always be provided, even if the user wants to use the default. db . Open ( addr , db . DefaultCache , zap . NewNop ()) db . Open ( addr , db . DefaultCache , log ) db . Open ( addr , false /* cache */ , zap . NewNop ()) db . Open ( addr , false /* cache */ , log ) Options are provided only if needed. db . Open ( addr ) db . Open ( addr , db . WithLogger ( log )) db . Open ( addr , db . WithCache ( false )) db . Open ( addr , db . WithCache ( false ), db . WithLogger ( log ), ) Our suggested way of implementing this pattern is with an Option interface that holds an unexported method, recording options on an unexported options struct. type options struct { cache bool logger * zap . Logger } type Option interface { apply ( * options ) } type cacheOption bool func ( c cacheOption ) apply ( opts * options ) { opts . cache = bool ( c ) } func WithCache ( c bool ) Option { return cacheOption ( c ) } type loggerOption struct { Log * zap . Logger } func ( l loggerOption ) apply ( opts * options ) { opts . logger = l . Log } func WithLogger ( log * zap . Logger ) Option { return loggerOption { Log : log } } // Open creates a connection. func Open ( addr string , opts ... Option , ) ( * Connection , error ) { options := options { cache : defaultCache , logger : zap . NewNop (), } for _ , o := range opts { o . apply ( & options ) } // ... } Note that there's a method of implementing this pattern with closures, but we believe that the pattern above provides more flexibility for authors and is easier to debug and test for users. In particular, it allows options to be compared against each other in tests and mocks, versus closures where this is impossible. Further, it lets options implement other interfaces, including fmt.Stringer which allows for user-readable string representations of the options. See also, Self-referential functions and the design of options Functional options for friendly APIs","title":"Functional Options"},{"location":"style/markdown/","text":"Markdown Style Guidelines General Section titles should follow the Chicago Title Capitalization standard. Lists should use the * character rather than the - character. Documents should start with a level one heading and should ideally be the same as the file name. For long files that are not served within this repository, it is best to have a table of contents at the end of the introduction of the level one heading section. Keep extra newlines between paragraphs and sections to make the source easier to read. Always specify the language for code blocks so that neither the syntax highlighter nor the text editor must guess. If no specific type makes sense, just use text . Use informative link titles. For example, instead of naming your links \"link\" or \"here\", wrap part of the sentence that is meant to be linked as a title. Do not use gendered pronouns when talking about users/consumers/whatever but always they/their instead. Do not use the future tense but use present simple for expressing general truths instead. Use active voice when there is no specific need to use passive. Abbreviations and acronyms should be spelled out the first time they appear in any technical document with the shortened form appearing in parentheses immediately after the term. The abbreviation or acronym can then be used throughout the document. Avoid ambiguous and abstract language (i.e. really, quite, very), imprecise or subjective terms (i.e. fast, slow, tall, small) and words that have no precise meaning (i.e. bit, thing, stuff). Avoid contractions (i.e. don't, you'll, etc.) as they are meant for informal contexts. Avoid generalized statements, because they are difficult to substantiate and too broad to be supported. Paragraphs that include multiple sentences should have the sentences on separate lines, so that updating one sentence results in a clear diff where one line changes. Specific to this Repository In this repository, Markdown is extended by some mkdocs plugins which allow you to include external files within a markdown files, and to insert admonitions. Admonitions are a way to include side content into a page without significantly interrupting the document flow. There are different types of admonitions, and they allow for the inclusion and nesting of arbitrary content. Including external files can be done by writing the name of a file present in the include directory surrounded by { / } and exclamation marks on an empty line. External Resources Technical Writing Standards Google Markdown Style Guide Mastering GitHub Markdown","title":"Markdown"},{"location":"style/markdown/#markdown-style-guidelines","text":"","title":"Markdown Style Guidelines"},{"location":"style/markdown/#general","text":"Section titles should follow the Chicago Title Capitalization standard. Lists should use the * character rather than the - character. Documents should start with a level one heading and should ideally be the same as the file name. For long files that are not served within this repository, it is best to have a table of contents at the end of the introduction of the level one heading section. Keep extra newlines between paragraphs and sections to make the source easier to read. Always specify the language for code blocks so that neither the syntax highlighter nor the text editor must guess. If no specific type makes sense, just use text . Use informative link titles. For example, instead of naming your links \"link\" or \"here\", wrap part of the sentence that is meant to be linked as a title. Do not use gendered pronouns when talking about users/consumers/whatever but always they/their instead. Do not use the future tense but use present simple for expressing general truths instead. Use active voice when there is no specific need to use passive. Abbreviations and acronyms should be spelled out the first time they appear in any technical document with the shortened form appearing in parentheses immediately after the term. The abbreviation or acronym can then be used throughout the document. Avoid ambiguous and abstract language (i.e. really, quite, very), imprecise or subjective terms (i.e. fast, slow, tall, small) and words that have no precise meaning (i.e. bit, thing, stuff). Avoid contractions (i.e. don't, you'll, etc.) as they are meant for informal contexts. Avoid generalized statements, because they are difficult to substantiate and too broad to be supported. Paragraphs that include multiple sentences should have the sentences on separate lines, so that updating one sentence results in a clear diff where one line changes.","title":"General"},{"location":"style/markdown/#specific-to-this-repository","text":"In this repository, Markdown is extended by some mkdocs plugins which allow you to include external files within a markdown files, and to insert admonitions. Admonitions are a way to include side content into a page without significantly interrupting the document flow. There are different types of admonitions, and they allow for the inclusion and nesting of arbitrary content. Including external files can be done by writing the name of a file present in the include directory surrounded by { / } and exclamation marks on an empty line.","title":"Specific to this Repository"},{"location":"style/markdown/#external-resources","text":"Technical Writing Standards Google Markdown Style Guide Mastering GitHub Markdown","title":"External Resources"}]}